{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaTeX Automation for NYCHA Waste Individual Action Plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code needed to create, populate, and write to LaTeX files numerous components of NYCHA's Individualized Waste-Management Action Plans (...or the same product under several similar titles). In addition, this notebook contains methods to compile two versions of the reports for each consolidation:\n",
    "1. Complete, formatted reports for final presentation\n",
    "2. Reports that have certain formatting elements removed for the purpose of presentation to development staff\n",
    "The definition and implementation of these compilation methods, along with necessary imports and setup comands, is located at the head of the document.\n",
    "\n",
    "To run these methods on your local machine, a few utilities and programs need to be installed, and their paths inserted at locations noted below. These utilities are:\n",
    "1. pdflatex -- Compiles LaTeX source code into a PDF. Installed as part of most LaTeX distributions.\n",
    "2. pdfjam -- Manipulates existing PDF files and pdflatex outputs. Often installed alongside LaTeX distributions, but may require custom installation.\n",
    "3. ghostscript -- Compresses PDFs to reduce file size. Installed via Homebrew or MacPorts (though other methods may be available).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the __following four code blocks.__ Take special care to ensure that all required packages import correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#System imports\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "\n",
    "#Data manipulation imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#File and text manipulation imports\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from pylatexenc.latexencode import unicode_to_latex\n",
    "from pylatexenc.latexencode import UnicodeToLatexEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Global Vars and Options\n",
    "os.chdir('/Users/kyleslugg/Documents/NYCHA/Production')\n",
    "pd.set_option('display.max_columns', None)\n",
    "fha_tds_list =['226', '283', '212', '226', '283', '212', '213', '274', '275', '260', '273', '284', '209']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Core Methods\n",
    "def get_standardized_names(drop_fha=True):\n",
    "    '''Reads multiple development and consolidation name lists, applies a fuzzy matching routine\n",
    "    to resolve discrepancies in those lists, and corrects known errors of that matching process \n",
    "    discovered during plan generation. Returns dictionaries containing data associated with\n",
    "    (1) consolidations and (2) developments, with TDS numbers as keys.\n",
    "    \n",
    "    Parameters:\n",
    "        drop_fha (bool): Boolean indicating whether FHA properties should be dropped from the\n",
    "        resulting dictionaries.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: a tuple \"(consolidations, developments)\" containing two dictionaries, which\n",
    "        ontain data associated with (1) consolidations and (2) developments. Keys are TDS numbers. \n",
    "    '''\n",
    "    \n",
    "    databook_names = pd.read_csv('DATA/name_tables/dev_data_book_name.csv')\n",
    "    databook_names['CONS_TDS'] = databook_names['CONS_TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    databook_names['TDS'] = databook_names['TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    staff_names = pd.read_csv('DATA/name_tables/staff_cons_name.csv')\n",
    "    staff_names['RC_Name'] = staff_names['RC Name']\n",
    "    \n",
    "    if drop_fha:\n",
    "        databook_names = databook_names.query(f'TDS not in {fha_tds_list}')\n",
    "    \n",
    "    consolidations = {}\n",
    "    developments = {}\n",
    "\n",
    "    for row in databook_names.itertuples():\n",
    "        consolidations[row.CONS_TDS] = {'name':row.CONS_NAME, 'alternates':[row.MANAGED_BY]}\n",
    "        developments[row.TDS] = {'name':row.DEV_NAME, 'name_alternates':[], 'cons_tds':row.CONS_TDS}\n",
    "        \n",
    "    def find_closest_fuzzy_match(name, comp_df, comp_col_name, return_col_name):\n",
    "        #Matches names in the two namesets\n",
    "        values = comp_df[comp_col_name].unique()\n",
    "        comp_df_copy = pd.DataFrame(data=values, index=[i for i in range(0,len(values))], columns=[comp_col_name])\n",
    "\n",
    "        '''\n",
    "        def strip_name(x):\n",
    "            string = str(x).lower()\n",
    "            string = string.replace('consolidated','')\n",
    "            string = string.replace('consolidation', '')\n",
    "            string = string.replace('houses', '')\n",
    "            return string\n",
    "\n",
    "        comp_df_copy['partial_ratio'] = comp_df_copy[comp_col_name].apply(lambda x: fuzz.partial_ratio(strip_name(name), strip_name(x)))\n",
    "        highest_match = comp_df_copy['partial_ratio'].max()\n",
    "\n",
    "        matches = comp_df_copy.loc[comp_df_copy['partial_ratio']==highest_match, 'CONS_NAME']\n",
    "\n",
    "        if matches.shape[0] == 1:\n",
    "            return matches.iloc[0]\n",
    "        else:\n",
    "            print(matches)\n",
    "            return 'ZZZ MULTIPLE MATCHES FOUND'\n",
    "\n",
    "        '''\n",
    "        return process.extractOne(str(name).lower(), values.tolist())[0]\n",
    "    \n",
    "    staff_names['NAME_MATCH'] = staff_names['RC Name'].apply(lambda x: find_closest_fuzzy_match(x, databook_names, 'CONS_NAME', 'CONS_NAME'))\n",
    "\n",
    "    match_corrections = {'Justice Sonia Sotomayor  Consolidated': 'SOTOMAYOR HOUSES CONSOLIDATED',\n",
    "                        'Murphy Consolidated': ''\n",
    "                        }\n",
    "\n",
    "    def make_corrections(row, index_col, data_col, dictionary):\n",
    "        #Applies manual corrections, using a dictionary with the structure {Name in Dataset: Standardized Name}\n",
    "        if str(row[index_col]).strip() in dictionary.keys():\n",
    "            return dictionary[row[index_col]]\n",
    "        else:\n",
    "            return row[data_col]\n",
    "\n",
    "    staff_names['AMENDED_MATCHES'] = staff_names.apply(lambda row: make_corrections(row, 'RC Name', 'NAME_MATCH', match_corrections), axis=1)\n",
    "\n",
    "    staff_names = staff_names.merge(databook_names[['CONS_NAME', 'CONS_TDS']], left_on='AMENDED_MATCHES', right_on='CONS_NAME', how='left')\n",
    "\n",
    "    for row in staff_names.itertuples():\n",
    "        try:\n",
    "            consolidations[row.CONS_TDS]['alternates'].append(row.RC_Name)\n",
    "        except:\n",
    "            print(f'TDS #{row.CONS_TDS} raised an exception.')\n",
    "    \n",
    "    #From vehicle data...\n",
    "    consolidation_corrections = {'Brooklyn Borough Management':'N/A',\n",
    "                            'LaGuardia Houses':'LA GUARDIA CONSOLIDATED',\n",
    "                            'Hylan':'BUSHWICK CONSOLIDATED',\n",
    "                            'Manhattan Property Management':'N/A',\n",
    "                            'NYCHA - Brooklyn Property Mgmt':'N/A',\n",
    "                            'Queens-Staten Island Borough Manag':'N/A',\n",
    "                            'Webster-Morrisania Houses': 'WEBSTER CONSOLIDATED',\n",
    "                            'NGO':'N/A',\n",
    "                            'Millbrook Houses':'MILL BROOK CONSOLIDATED',\n",
    "                            'Van Dyke Houses':'VAN DYKE I',\n",
    "                            'UPACA':'JACKIE ROBINSON CONSOLIDATED',\n",
    "                            'Department of Mixed Finance Asset':'N/A',\n",
    "                            'Ocean Hill-Saratoga Village':'OCEAN HILL CONSOLIDATED',\n",
    "                            'nan':'N/A',\n",
    "                            'L.E.S. II/Campos':'LOWER EAST SIDE CONSOLIDATED',\n",
    "                            'St. Marys Park/Moore': \"SAINT MARY'S PARK CONSOLIDATED\",\n",
    "                            'Seth Low/Glenmore Plaza':'SETH LOW CONSOLIDATED',\n",
    "                            'Woodson/Van Dyke II':'WOODSON',\n",
    "                            'Beach 41st Street/Oceanside':'BEACH 41ST STREET-BEACH CHANNEL DRIVE',\n",
    "                            'CONEY ISLAND' : 'SURFSIDE GARDENS CONSOLIDATED',\n",
    "                            'BLAND' : 'LATIMER GARDENS CONSOLIDATED',\n",
    "                            'GRAVESEND' : \"O'DWYER GARDENS CONSOLIDATED\",\n",
    "                            'LES 2' : 'LOWER EAST SIDE CONSOLIDATED',\n",
    "                            'OCEAN BAY' : 'BEACH 41ST STREET-BEACH CHANNEL DRIVE',\n",
    "                            'OCEANBAY' : 'BEACH 41ST STREET-BEACH CHANNEL DRIVE',\n",
    "                            \"ST. MARY'S\" : \"SAINT MARY'S PARK CONSOLIDATED\",\n",
    "                            \"ST. NICHOLAS\" : 'SAINT NICHOLAS',\n",
    "                            'UNION AVE. CON.' : 'UNION AVENUE CONSOLIDATED',\n",
    "                            'WILLIAM REID' : 'REID APARTMENTS CONSOLIDATED',\n",
    "                            'MURPHY CONSOLIDATED':'1010 EAST 178TH STREET',\n",
    "                            'Murphy Consolidated':'1010 EAST 178TH STREET'}\n",
    "    \n",
    "    consolidations['NaN'] = {'name':'N/A',\n",
    "                            'alternates':[]}\n",
    "    for key, value in consolidation_corrections.items():\n",
    "        for key_c, value_c in consolidations.items():\n",
    "            if value_c['name'] == value: \n",
    "                try:\n",
    "                    consolidations[key_c]['alternates'].append(key)\n",
    "                except:\n",
    "                    consolidations[key_c]['alternates'] = key\n",
    "    \n",
    "    for key, value in consolidations.items():\n",
    "        for key_dev, value_dev in developments.items():\n",
    "            if key == value_dev['cons_tds']:\n",
    "                try:\n",
    "                    value['developments'].append(key_dev)\n",
    "                except:\n",
    "                    value['developments'] = [key_dev]\n",
    "    \n",
    "    return(consolidations, developments)\n",
    "\n",
    "#LOAD OVERVIEW DATA\n",
    "def load_overview_data(drop_fha=True):\n",
    "    '''Loads overview data, from the DATA folder.\n",
    "    \n",
    "    Parameters:\n",
    "        drop_fha (bool): Determines whether FHA properties are dropped from the \n",
    "        relevant consolidations (in particular, Baisley Park -- TDS #091)\n",
    "    \n",
    "    Returns:\n",
    "        dataFrame: A Pandas dataframe containing overview data on each development'''\n",
    "    \n",
    "    overview_data = pd.read_csv('DATA/overview_table_data.csv')\n",
    "    overview_data['CONS_TDS'] = overview_data['CONS_TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    overview_data['TDS'] = overview_data['TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    if drop_fha:\n",
    "        overview_data = overview_data.query(f'TDS not in {fha_tds_list}')\n",
    "    \n",
    "    return overview_data\n",
    "\n",
    "#Character Substitutions for LaTeX -- set and define \"clean\" method\n",
    "def clean_text(text):\n",
    "    '''Replaces LaTeX-unfriendly characters in the provided block of text.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): A block of text to be cleaned.\n",
    "    \n",
    "    Returns:\n",
    "        str: A cleaned block of text ready to be inserted into a LaTeX document.'''\n",
    "    \n",
    "    substitutions = {'“':\"``\",\n",
    "                '”': \"''\",\n",
    "                '’':\"'\",\n",
    "                ' ':' ',\n",
    "                '–':'--',\n",
    "                ' ':' ',\n",
    "                '\\xa0':' ',\n",
    "                '&':r'\\&',\n",
    "                    ':':':',\n",
    "                    '#':'\\#'}\n",
    "    \n",
    "    for key, value in substitutions.items():\n",
    "        text = text.replace(key, value)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDS #nan raised an exception.\n"
     ]
    }
   ],
   "source": [
    "#Generate list of consolidations and developments\n",
    "consolidations, developments = get_standardized_names()\n",
    "\n",
    "#Count developments per consolidation\n",
    "counts ={}\n",
    "for key, value in developments.items():\n",
    "    if value['cons_tds'] not in counts.keys():\n",
    "        counts[value['cons_tds']] = {'developments':[key],\n",
    "                             'count':1}\n",
    "    else:\n",
    "        counts[value['cons_tds']]['developments'].append(key)\n",
    "        counts[value['cons_tds']]['count']+=1\n",
    "        \n",
    "\n",
    "count_list = [value['count'] for key, value in counts.items()]\n",
    "high_count_cons = [key for key, value in counts.items() if value['count']>=8]\n",
    "\n",
    "#Load core data and create authoritative list of consolidations\n",
    "overview_data = load_overview_data()\n",
    "cons_list = overview_data['CONS_TDS'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate reports using existing files\n",
    "Use the following methods to generate Individual Action Plans for some or all consolidations _without altering any report components generated using the methods below_. __First, run the following code block to define the compliation method.__\n",
    "\n",
    "Please note that reports will not compile if components generated below in this notebook have not been generated at least once. If that is the case, please run all blocks in the following sections before generating the IAPs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_latex_file(tds, counts=counts, spreadlist=None, no_sections=False):\n",
    "    '''When provided a TDS number and related information, generates the appropriate IAP as a PDF.\n",
    "    \n",
    "    Parameters:\n",
    "        tds (str): A three-character string of the consolidation TDS number for which a report is to be generated.\n",
    "        \n",
    "        counts (dict): A dictionary containing the number of developments in each consolidation. Defaults to counts.\n",
    "        \n",
    "        spreadlist (list): A list on consolidation TDS numbers for which reports are to be generated\n",
    "            as two-page spreads. Defaults to None.\n",
    "        \n",
    "        no_sections (bool): If True, reports are generated without section divider pages and the \n",
    "            \"Letter from Chair\" placeholder.\n",
    "    \n",
    "    Returns:\n",
    "        None'''\n",
    "    #SET UTILITY PATHS HERE\n",
    "    pdflatex_path = '/usr/local/texlive/2018/bin/x86_64-darwin/pdflatex'\n",
    "    ghostscript_path = '/usr/local/bin/gs'\n",
    "    pdfjam_path = '/usr/local/texlive/2018/texmf-dist/scripts/pdfjam/pdfjam'\n",
    "    if no_sections == False:\n",
    "        if counts[tds]['count'] <= 4:\n",
    "            with open('REPORT_TEMPLATE/report.tex', 'r') as file_handle:\n",
    "                text = file_handle.read()\n",
    "\n",
    "            new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "            with open(f'REPORTS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "                outfile.write(new_text)\n",
    "\n",
    "        else:\n",
    "            with open('REPORT_TEMPLATE/report_long.tex', 'r') as file_handle:\n",
    "                text = file_handle.read()\n",
    "\n",
    "            new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "            with open(f'REPORTS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "                outfile.write(new_text)\n",
    "\n",
    "        subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/LaTeX', f'REPORTS/LaTeX/{tds}_report.tex'])\n",
    "        subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/LaTeX', f'REPORTS/LaTeX/{tds}_report.tex'])\n",
    "\n",
    "        #Be sure to install ghostscript (to compress pdfs), or comment out next line. Available via homebrew.\n",
    "        subprocess.check_call([ghostscript_path, '-sDEVICE=pdfwrite', '-dCompatibilityLevel=1.5', '-dNOPAUSE', '-dQUIET', '-dBATCH', f'-sOutputFile=REPORTS/{tds}_report.pdf', f'REPORTS/LaTeX/{tds}_report.pdf'])\n",
    "        #if tds in spreadlist:\n",
    "            #handle = subprocess.Popen([pdfjam_path, '--nup 2x1', \"--openright 'true'\", f'REPORTS/{tds}_report.pdf', f'--outfile REPORTS/{tds}_spread.pdf', '--landscape', '--no-tidy'], stdout=subprocess.PIPE,stderr=subprocess.PIPE) \n",
    "            #err = handle.communicate() \n",
    "            #print(err)\n",
    "            #subprocess.check_call([pdfjam_path, '--nup 2x1', \"--openright 'true'\", \"--frame 'true'\", f'REPORTS/{tds}_report.pdf', f'--outfile {tds}_spread.pdf', '--landscape'])\n",
    "    else:\n",
    "        if counts[tds]['count'] <= 4:\n",
    "            with open('REPORT_TEMPLATE/report_nosec.tex', 'r') as file_handle:\n",
    "                text = file_handle.read()\n",
    "\n",
    "            new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "            with open(f'REPORTS/NO_SECTIONS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "                outfile.write(new_text)\n",
    "\n",
    "        else:\n",
    "            with open('REPORT_TEMPLATE/report_long_nosec.tex', 'r') as file_handle:\n",
    "                text = file_handle.read()\n",
    "\n",
    "            new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "            with open(f'REPORTS/NO_SECTIONS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "                outfile.write(new_text)\n",
    "\n",
    "        subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/NO_SECTIONS/LaTeX', f'REPORTS/NO_SECTIONS/LaTeX/{tds}_report.tex'])\n",
    "        subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/NO_SECTIONS/LaTeX', f'REPORTS/NO_SECTIONS/LaTeX/{tds}_report.tex'])\n",
    "\n",
    "        #Be sure to install ghostscript (to compress pdfs), or comment out next line. Available via homebrew.\n",
    "        subprocess.check_call([ghostscript_path, '-sDEVICE=pdfwrite', '-dCompatibilityLevel=1.5', '-dNOPAUSE', '-dQUIET', '-dBATCH', f'-sOutputFile=REPORTS/NO_SECTIONS/{tds}_report.pdf', f'REPORTS/NO_SECTIONS/LaTeX/{tds}_report.pdf'])\n",
    "        #if tds in spreadlist:\n",
    "            #handle = subprocess.Popen([pdfjam_path, '--nup 2x1', \"--openright 'true'\", f'REPORTS/{tds}_report.pdf', f'--outfile REPORTS/{tds}_spread.pdf', '--landscape', '--no-tidy'], stdout=subprocess.PIPE,stderr=subprocess.PIPE) \n",
    "            #err = handle.communicate() \n",
    "            #print(err)\n",
    "\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, reports will be generated for all consolidations. To generate reports only for select consolidation, un-comment the line \"consolidation_list = [...]\", and insert the desired consolidation TDS numbers into this list as strings (e.g., ['073', '056', '111'])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create necessary directories and copy working template to use in compilation\n",
    "if not os.path.exists('REPORTS'):\n",
    "    os.makedirs('REPORTS')\n",
    "\n",
    "if not os.path.exists('REPORTS/LaTeX'):\n",
    "    os.makedirs('REPORTS/LaTeX')\n",
    "\n",
    "os.system(\"cp REPORT_TEMPLATE/content.tex REPORTS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/preface.tex REPORTS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/content_long.tex REPORTS/LaTeX\")\n",
    "\n",
    "#Set list of consolidations for which reports should be compiled. \n",
    "#Recall that consolidations.keys() contains ALL CONSOLIDATIONS\n",
    "\n",
    "consolidation_list = consolidations.keys()\n",
    "#consolidation_list = ['073','127','067','003']\n",
    "\n",
    "\n",
    "error_list = []\n",
    "for tds in consolidation_list:\n",
    "    try:\n",
    "        compile_latex_file(tds, spreadlist=consolidation_list)\n",
    "    except:\n",
    "        print(f'{tds} raised exception: {sys.exc_info()[0]}')\n",
    "        error_list.append(tds)\n",
    "\n",
    "filelist = [f for f in os.listdir('REPORTS/LaTeX') if not f.endswith(\".tex\")]\n",
    "save_files = [f'{tds}_report.log' for tds in error_list]\n",
    "\n",
    "for f in filelist:\n",
    "    if f not in save_files:\n",
    "        os.remove(os.path.join('REPORTS/LaTeX', f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Reports (for site visits)\n",
    "\n",
    "Generate reports without section headings and unfinished pages using the following cell. Consolidation for which reports are to be generated are defined as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('REPORTS/NO_SECTIONS'):\n",
    "    os.makedirs('REPORTS/NO_SECTIONS')\n",
    "\n",
    "if not os.path.exists('REPORTS/NO_SECTIONS/LaTeX'):\n",
    "    os.makedirs('REPORTS/NO_SECTIONS/LaTeX')\n",
    "\n",
    "os.system(\"cp REPORT_TEMPLATE/content_nosec.tex REPORTS/NO_SECTIONS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/preface.tex REPORTS/NO_SECTIONS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/content_long_nosec.tex REPORTS/NO_SECTIONS/LaTeX\")\n",
    "\n",
    "\n",
    "consolidation_list = consolidations.keys()\n",
    "#consolidation_list = ['024', '028']\n",
    "error_list = []\n",
    "for tds in consolidation_list:\n",
    "    try:\n",
    "        compile_latex_file(tds, spreadlist=consolidation_list, no_sections=True)\n",
    "    except:\n",
    "        print(f'{tds} raised exception: {sys.exc_info()[0]}')\n",
    "        error_list.append(tds)\n",
    "\n",
    "filelist = [f for f in os.listdir('REPORTS/NO_SECTIONS/LaTeX') if not f.endswith(\".tex\")]\n",
    "save_files = [f'{tds}_report.log' for tds in error_list]\n",
    "\n",
    "for f in filelist:\n",
    "    if f not in save_files:\n",
    "        os.remove(os.path.join('REPORTS/NO_SECTIONS/LaTeX', f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Report Text, Tables, and Graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods generate various components of the individual action plans, either by converting source documents into a LaTeX-friendly form or by loading and analyzing data in CSV files to create tables. These methods are sorted into several section, depending on their function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and Processing Text Blocks\n",
    "These methods read and process Microsoft Word files such that they can be included in the reports' LaTeX source. \n",
    "__Please run the following block prior to running any particular conversion method.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lightly modified version of example found at http://etienned.github.io/posts/extract-text-from-word-docx-simply/\n",
    "\n",
    "try:\n",
    "    from xml.etree.cElementTree import XML\n",
    "except ImportError:\n",
    "    from xml.etree.ElementTree import XML\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Module that extract text from MS XML Word document (.docx).\n",
    "(Inspired by python-docx <https://github.com/mikemaccana/python-docx>)\n",
    "\"\"\"\n",
    "\n",
    "WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "PARA = WORD_NAMESPACE + 'p'\n",
    "TEXT = WORD_NAMESPACE + 't'\n",
    "\n",
    "\n",
    "def get_docx_text(path):\n",
    "    \"\"\"\n",
    "    Take the path of a docx file as argument, return the text in unicode.\n",
    "    \"\"\"\n",
    "    document = zipfile.ZipFile(path)\n",
    "    try:\n",
    "        xml_content = document.read('word/document.xml')\n",
    "    except:\n",
    "        xml_content = document.read('word/document2.xml')\n",
    "        \n",
    "    document.close()\n",
    "    tree = XML(xml_content)\n",
    "\n",
    "    paragraphs = []\n",
    "    for paragraph in tree.getiterator(PARA):\n",
    "        texts = [node.text\n",
    "                 for node in paragraph.getiterator(TEXT)\n",
    "                 if node.text]\n",
    "        if texts:\n",
    "            paragraphs.append(''.join(texts))\n",
    "\n",
    "    return '\\n\\n'.join(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preface -- What is an IAP?\n",
    "\n",
    "The following two cells 1) define the method used to process text for the IAP preface and 2) run that method for each consolidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preface_data():\n",
    "    \n",
    "    about_text = clean_text(get_docx_text('TEXT/preface_text/about_IAPs.docx'))\n",
    "    staff_names = pd.read_excel('DATA/Dev_Staff_Names.xlsx')\n",
    "    candidate_list = []\n",
    "\n",
    "    for key, value in consolidations.items():\n",
    "        candidate_list.append(str(value['name']).upper())\n",
    "        for item in value['alternates']:\n",
    "            candidate_list.append(item.upper())\n",
    "\n",
    "    def get_cons_name(name):\n",
    "        match = process.extractOne(str(name).upper(), candidate_list)[0]\n",
    "        #print(match)\n",
    "        for key, value in consolidations.items():\n",
    "            if (match.upper() == value['name'].upper()) or (match.upper() in [val.upper() for val in value['alternates']]):\n",
    "                return value['name']\n",
    "\n",
    "        return '!!!NOT FOUND'\n",
    "\n",
    "    def get_tds_from_name(x):\n",
    "        for key, value in consolidations.items():\n",
    "            if str(value['name']).upper().strip() == x.upper().strip():\n",
    "                return key\n",
    "\n",
    "        return 'N/A'\n",
    "\n",
    "    staff_names['CONS_MATCH'] = staff_names['CONS'].apply(lambda x: get_cons_name(x))\n",
    "    staff_names['CONS_TDS'] = staff_names['CONS_MATCH'].apply(lambda x: get_tds_from_name(x))\n",
    "    \n",
    "    return (staff_names, about_text)\n",
    "\n",
    "\n",
    "def make_preface_text(tds, preface_data, about_text):\n",
    "    cons_data = preface_data[preface_data['CONS_TDS'] == tds]\n",
    "    \n",
    "    latex_block = r'''\\chapter{\\textcolor{darkBlue}{Preface}}\n",
    "\n",
    "    \\section{Letter from the Chair}\\label{sec:Section1}\n",
    "    \\clearpage\n",
    "    {\\fontfamily{phv}\\selectfont\n",
    "    \\section{What is an Individual Action Plan?}'''+'\\n\\n'+about_text\n",
    "    \n",
    "    #Following is no longer included:\n",
    "    '''\n",
    "    Below is a list of %s Management Personnel as of August 2020:\n",
    "    \\begin{itemize}\n",
    "    \\item Operations VP: %s\n",
    "    \\item %s Borough Director: %s\n",
    "    \\item Regional Asset Manager: %s\n",
    "    \\item Property Manager: %s\n",
    "    \\item Superintendent: %s\n",
    "    \\end{itemize}\n",
    "    }'''\n",
    "    \n",
    "    '''\n",
    "    data = [cons name, ops vp, borough name, borough dir, RAM, PM, super]\n",
    "    preface_data = []\n",
    "    \n",
    "    preface_data.append(str(cons_data['CONS_MATCH'].iloc[0]).title())\n",
    "    \n",
    "    for col in ['OPS_VP', 'BORO', 'BORO_DIR', 'RAM', 'PM', 'PMS']:\n",
    "        preface_data.append(cons_data[col].iloc[0])\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    with open(f'TEXT/preface_text/{tds}_preface.tex', 'w') as file_handle:\n",
    "        file_handle.write(clean_text(latex_block % tuple(preface_data)))\n",
    "    '''\n",
    "    with open(f'TEXT/preface_text/{tds}_preface.tex', 'w') as file_handle:\n",
    "        file_handle.write(clean_text(latex_block))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preface_data = load_preface_data()\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_preface_text(tds, *preface_data)\n",
    "    except:\n",
    "        print(f'{tds} raised an exception')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overview_text(cons_tds):\n",
    "    '''Loads and cleans Overview text (found in ...[cons_tds]_Overview.docx) for each consolidation.\n",
    "    \n",
    "    Parameters:\n",
    "        cons_tds (str): The relevant consolidation TDS number as a three-character string.\n",
    "    \n",
    "    Returns:\n",
    "        None'''\n",
    "    \n",
    "    u = UnicodeToLatexEncoder(non_ascii_only = True, unknown_char_policy = (lambda x: ' '))\n",
    "    header = re.compile(r'''(([\\w\\-\\'\\’]*\\s)*(Overview))\\s*(:{0,2})\\s*''')\n",
    "    linebreaks = re.compile(r'[\\n]+')\n",
    "    \n",
    "    overview_text = get_docx_text(f'TEXT/overview_text/{cons_tds}_Overview.docx')\n",
    "    overview_text = clean_text(overview_text)\n",
    "    \n",
    "    if len(header.findall(overview_text)) == 0:\n",
    "            overview_text = overview_text\n",
    "    else:\n",
    "        try:\n",
    "            overview_text = overview_text.replace(header.findall(overview_text)[0],'')\n",
    "            \n",
    "        except:\n",
    "            overview_text = overview_text.replace(header.findall(overview_text)[0][0],'')\n",
    "    \n",
    "    if overview_text[0] == ':':\n",
    "        overview_text = overview_text[1:]\n",
    "    \n",
    "    overview_text = re.sub(linebreaks, r\"\\\\par \\\\vspace{.7\\\\baselineskip}\", overview_text.strip())\n",
    "    with open(f'TEXT/overview_text/{cons_tds}_overview.tex', 'w') as file_handle:\n",
    "        #file_handle.write(u.unicode_to_latex(overview_text))\n",
    "        file_handle.write(overview_text)\n",
    "    \n",
    "    #return overview_text\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_overview_text(tds)\n",
    "    except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    #except:\n",
    "     #   print(f\"{tds} raised error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_analysis_text(cons_tds):\n",
    "    '''Loads and cleans Analysis text (found in ...[cons_tds]_Analysis.docx) for each consolidation.\n",
    "    \n",
    "    Parameters:\n",
    "        cons_tds (str): The relevant consolidation TDS number as a three-character string.\n",
    "    \n",
    "    Returns:\n",
    "        None'''\n",
    "    \n",
    "    analysis_text = get_docx_text(f'TEXT/analysis_text/{cons_tds}_Analysis.docx')\n",
    "\n",
    "    header = re.compile(r'''(([\\w\\-\\'\\’\\(\\)]*\\s)*(Analysis)):{0,1}\\s*''')\n",
    "    \n",
    "    analysis_text = clean_text(analysis_text)\n",
    "\n",
    "    section_headings = {'Inspection and Collection Requirement':['Inspection and Collection Requirements',\n",
    "                                                                 'Inspection and Collection Requirement',\n",
    "                                                                 'Collection and Inspection Requirements',\n",
    "                                                                'Collection and Inspection Requirement'],\n",
    "                        'Removal or Storage Requirement':['Removal or Storage Requirements',\n",
    "                                                          'Removal or Storage Requirement',\n",
    "                                                          'Removal and Storage Requirements',\n",
    "                                                         'Removal and Storage Requirement',\n",
    "                                                         'Storage or Removal Requirement',\n",
    "                                                          'Storage and Removal Requirements',\n",
    "                                                         'Storage and Removal Requirement',\n",
    "                                                         'Removal or Storage Requirement '],\n",
    "                       'Additional Context':['Additional Context']}\n",
    "    \n",
    "    for heading, variants in section_headings.items():\n",
    "        for variant in variants:\n",
    "            if variant in analysis_text:\n",
    "                analysis_text = analysis_text.replace(variant, r'\\textbf{%s}' % (heading))\n",
    "                break\n",
    "\n",
    "    #if len(header.findall(analysis_text)) == 0:\n",
    "     #   pass\n",
    "    #else:\n",
    "    try:\n",
    "        analysis_text = analysis_text.replace(header.findall(analysis_text)[0][0],'')\n",
    "\n",
    "    except:\n",
    "        analysis_text = analysis_text.replace(header.findall(analysis_text)[0],'')\n",
    "        \n",
    "    if analysis_text[0] == ':':\n",
    "        analysis_text = analysis_text[1:]\n",
    "\n",
    "    latex_block = analysis_text\n",
    "\n",
    "    with open(f'TEXT/analysis_text/{cons_tds}_analysis.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_analysis_text(tds)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    #except:\n",
    "     #   print(f\"{tds} raised error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Maps\n",
    "\n",
    "The two following cells import context maps for each consolidation and split the images in two, to be placed on opposing pages.\n",
    "\n",
    "__PLEASE NOTE: This operation takes a relatively long time, and does not need to be repeated frequently. Skip as necessary to maximize efficiency.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set asset map path\n",
    "#asset_map_path = f\"MAPS/asset_maps/{cons_tds}_asset_map.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split context map into two pages\n",
    "def process_context_map(cons_tds):\n",
    "    '''Divides the context map for the supplied consolidation in half, exporting each as a separate file.\n",
    "    \n",
    "    Parameters:\n",
    "        cons_tds (str): The relevant consolidation TDS number as a three-character string.\n",
    "    \n",
    "    Returns:\n",
    "        None'''\n",
    "    \n",
    "    image = Image.open(f'MAPS/context_maps/{cons_tds}_context_map.png')\n",
    "    width, height = image.size\n",
    "\n",
    "    bb1 = (0,0,width/2,height)\n",
    "    bb2 = (width/2, 0, width, height)\n",
    "\n",
    "    img_1 = image.crop(bb1)\n",
    "    img_2 = image.crop(bb2)\n",
    "\n",
    "    img_1.save(f'MAPS/context_maps/{cons_tds}_context_1.png', format=\"PNG\")\n",
    "    img_2.save(f'MAPS/context_maps/{cons_tds}_context_2.png', format=\"PNG\")\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        process_context_map(tds)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Tables\n",
    "\n",
    "The following subsections produce each of the tables found in the IAPs. In general, each section contains two methods: one to load and process the relevant data, and another that inserts those data into a table.\n",
    "\n",
    "Tables are, with some exceptions, built using LaTeX's tabular environment. The first block of text establishes the number of columns in the table; the width of each column, if specified; and how text is positioned in each column. Next, each row (including the header row) is created as a raw string, with placeholders (%s) for each piece of data to be inserted; these data are later supplied as a tuple of strings. Once the table is completely formed, the tabular (or similar) environment is ended with the command \\end{tabular}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Overview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overview_table(cons_tds, overview_data=overview_data):\n",
    "    '''Creates overview table, located in the introduction section.\n",
    "    \n",
    "    Parameters:\n",
    "        cons_tds (str): The relevant consolidation TDS number as a three-character string.\n",
    "        \n",
    "        overview_data (dataFrame): A Pandas dataFrame containing core data on developments\n",
    "    \n",
    "    Returns:\n",
    "        None'''\n",
    "    \n",
    "    cons_data = overview_data.loc[overview_data['CONS_TDS']== cons_tds]\n",
    "    \n",
    "    overview_table = ''\n",
    "\n",
    "    overview_frame = r'''\n",
    "    \\resizebox{\\textwidth}{!}{\n",
    "    \\begin{tabular}{l|c|c|c|c|c|c|}\n",
    "    \\cline{2-7}\n",
    "                                                                           & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} TDS \\#} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Stairhalls \\#} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Units}  & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Households} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Official Population} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Average Family Size} \\\\ \\hline\n",
    "\n",
    "    '''\n",
    "\n",
    "    development_template = r'''\\multicolumn{1}{|l|}{\\cellcolor{ccteallight}%s}        & %s                                                   & %s                            & %s                                                   & %s                                                           & %s                                                                & %s                                                                \\\\ \\hline'''\n",
    "\n",
    "\n",
    "    overview_table += overview_frame\n",
    "\n",
    "    for row in cons_data.itertuples():\n",
    "        dev_name = clean_text(row.DEV_NAME.title())\n",
    "        dev_tds = row.TDS\n",
    "        stairhalls = int(row.STAIRHALLS)\n",
    "        units = f\"{int(row.TOTAL_APTS):,d}\" #Adds thousands comma sep.\n",
    "        total_hhs = row.TOTAL_HH\n",
    "        official_population = row.TOTAL_POP\n",
    "        avg_family_size = row.AVG_FAMILY_SIZE\n",
    "\n",
    "        overview_table += development_template % (dev_name, dev_tds, stairhalls, units, total_hhs, official_population, avg_family_size)\n",
    "\n",
    "    overview_table += r'''\n",
    "    \\end{tabular}\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    with open(f'TABLES/overview_table/{cons_tds}_overview_table.tex', 'w') as file_handle:\n",
    "        file_handle.write(overview_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_data = load_overview_data()\n",
    "for tds in consolidations.keys():\n",
    "    make_overview_table(tds, overview_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typology Table\n",
    "\n",
    "To associate developments with typologies, the following routines 1) load two sources of typology data and other data related to developments' physical characteristics, 2) matches detailed typologies to icons used in the Connected Communities plan, and 3) produces a table showing development names, typologies, and typology icons.\n",
    "\n",
    "Note the structure of the tabular environment here: text-icon blocks are split into one or two lines, depending on the size of the consolidation. Each line is then embedded in an outer one-by-two table, so that the entire assembly is inserted into the document as a unit. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_typology_data(drop_fha=True):\n",
    "    #Cleaning and Shaping Data\n",
    "    typ_1 = pd.read_csv('DATA/typologies_1.csv')\n",
    "    typ_2 = pd.read_csv('DATA/typologies_2.csv')\n",
    "\n",
    "    typ_1.columns = ['CONS_NAME', 'DEV_NAME', 'TDS', 'TYPOLOGY']\n",
    "    typ_2.columns = ['CONS_NAME', 'CONS_TDS', 'DEV_NAME', 'TDS', 'METHOD', \n",
    "                     'CONSTRUCTION_DATE', 'BLDG_AGE', 'STORIES', 'BLDG_COVERAGE_SQFT', 'OPEN_SPACE_RATIO', 'SCATTERED_SITE_FLAG']\n",
    "\n",
    "    def make_dates(date_col):\n",
    "        date = str(date_col).split('/')\n",
    "        try:\n",
    "            if int(date[2]) > 18:\n",
    "                return datetime.date(int(f'19{date[2]}'), int(date[0]), int(date[1]))\n",
    "            else:\n",
    "                return datetime.date(int(f'20{date[2]}'), int(date[0]), int(date[1]))\n",
    "        except IndexError:\n",
    "            return datetime.date(1900,1,1)\n",
    "\n",
    "    typ_2['CONSTRUCTION_DATE'] = typ_2['CONSTRUCTION_DATE'].apply(lambda x: make_dates(x))\n",
    "    typ_2['SCATTERED_SITE_FLAG'] = typ_2['SCATTERED_SITE_FLAG'].apply(lambda x: x == 'YES')\n",
    "    typ_2.loc[typ_2['SCATTERED_SITE_FLAG']=='YES','SCATTERED_SITE_FLAG'] = 1\n",
    "\n",
    "    typology = typ_1.merge(typ_2[['CONS_TDS', 'TDS', 'METHOD',\n",
    "                                 'CONSTRUCTION_DATE', 'BLDG_AGE', \n",
    "                                 'STORIES', 'BLDG_COVERAGE_SQFT', \n",
    "                                 'OPEN_SPACE_RATIO', 'SCATTERED_SITE_FLAG']], how='left', on='TDS')\n",
    "\n",
    "    typology['CONS_TDS'] = typology['CONS_TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    typology['PREWAR'] = typology['CONSTRUCTION_DATE'].apply(lambda x: x < datetime.date(1945,1,1))\n",
    "    \n",
    "    if drop_fha:\n",
    "        typology = typology.query(f'TDS not in {fha_tds_list}')\n",
    "    #Adding Typology Icons\n",
    "\n",
    "    typ_icons = [r'\\rootpath/IMAGES/typology_earlytower.png', r'\\rootpath/IMAGES/typology_towerpark.png', r'\\rootpath/IMAGES/typology_prewar.png', r'\\rootpath/IMAGES/typology_scatteredsite.png']\n",
    "    typ_dict = {}\n",
    "    [typ_dict.setdefault(key, '') for key in typology['TYPOLOGY'].unique().tolist()]\n",
    "\n",
    "    typ_dict['1 - High-rise in the park'] = typ_icons[1]\n",
    "    typ_dict['2 - Mid-rise in the park'] = typ_icons[1]\n",
    "    typ_dict['3 - Low-rise in the park'] = typ_icons[0]\n",
    "    typ_dict['4 - Context Towers'] = typ_icons[3]\n",
    "    typ_dict['5 - Context Mid-rises'] = typ_icons[2]\n",
    "    typ_dict['6 - Walkups & Brownstones'] = typ_icons[2]\n",
    "\n",
    "    typ_header = re.compile(r'\\d\\s-\\s')\n",
    "\n",
    "    typology['TYP_NAME'] = typology['TYPOLOGY'].apply(lambda x: typ_header.sub('', str(x)))\n",
    "    typology['IMAGE_PATH'] = typology['TYPOLOGY'].apply(lambda x: typ_dict[x])\n",
    "    \n",
    "    return typology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_typology_table_block(cons_tds, typ_data):\n",
    "    cons_data = typ_data[typ_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "    \n",
    "    if num_devs < 5:\n",
    "        block_1 = cons_data\n",
    "    \n",
    "    elif num_devs >=5 and num_devs < 7:\n",
    "        block_1 = cons_data.iloc[0:3]\n",
    "        block_2 = cons_data.iloc[3:]\n",
    "    \n",
    "    else:\n",
    "        block_1 = cons_data.iloc[0:4]\n",
    "        block_2 = cons_data.iloc[4:]\n",
    "    \n",
    "    len_1 = block_1.shape[0]\n",
    "    \n",
    "    try:\n",
    "        len_2 = block_2.shape[0]\n",
    "    except:\n",
    "        len_2 = 0\n",
    "    \n",
    "    headers = {1:r\"\\begin{tabular}{m{1.5in} m{2in}}\"+'\\n',\n",
    "              2:r\"\\begin{tabular}{m{1.25in} m{2in} m{.1in} m{1.25in} m{2in}}\"+'\\n',\n",
    "              3:r\"\\begin{tabular}{m{1.25in} m{1.5in} m{.2in} m{1.25in} m{1.5in} m{.2in} m{1.25in} m{1.5in}}\"+'\\n',\n",
    "              4:r\"\\begin{tabular}{m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in}}\"+'\\n'}\n",
    "         \n",
    "    lines = {1:r'''\\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            2:r'''\\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            3:r'''\\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            4:r'''\\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}& & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}'''+'\\n'+r'\\end{tabular}'}\n",
    "    \n",
    "    \n",
    "    data_1 = []\n",
    "    data_2 = []\n",
    "    \n",
    "    for row in block_1.itertuples():\n",
    "        data_1.append(clean_text(str(row.DEV_NAME).title()))\n",
    "        data_1.append(str(row.TYP_NAME).replace('&', '\\&'))\n",
    "        data_1.append(row.IMAGE_PATH)\n",
    "    \n",
    "    if len_2 > 0:\n",
    "        for row in block_2.itertuples():\n",
    "            data_2.append(clean_text(str(row.DEV_NAME.title())))\n",
    "            data_2.append(str(row.TYP_NAME).replace('&', '\\&'))\n",
    "            data_2.append(row.IMAGE_PATH)\n",
    "    \n",
    "    # Assembling Nested Tables\n",
    "    latex_block = ''\n",
    "    if num_devs >= 2:\n",
    "        latex_block += r'''\\begin{table}[H]\n",
    "        \\resizebox{.9\\textwidth}{!}{\n",
    "        \\begin{tabular}{c}\n",
    "        '''\n",
    "    else:\n",
    "        latex_block += r'''\\begin{table}[H]\n",
    "        \\begin{tabular}{c}\n",
    "        '''\n",
    "    \n",
    "    latex_block += headers[len_1]\n",
    "    latex_block += lines[len_1] % tuple(data_1)\n",
    "    \n",
    "    if len_2 > 0:\n",
    "        latex_block += r'''\\\\\n",
    "        '''\n",
    "        latex_block += headers[len_2]\n",
    "        latex_block += lines[len_2] % tuple(data_2)\n",
    "    \n",
    "    if num_devs >= 2:\n",
    "        latex_block += r'''\\end{tabular}}\n",
    "        \\end{table}'''\n",
    "    else:\n",
    "        latex_block += r'''\\end{tabular}\n",
    "        \\end{table}'''\n",
    "    \n",
    "    with open(f'TABLES/typology_table/{cons_tds}_typology.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note: upon generating tables, execptions can be expected for TDS numbers 210 and 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n",
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 raised an exception.\n",
      "128 raised an exception.\n",
      "NaN raised an exception.\n"
     ]
    }
   ],
   "source": [
    "typology = load_typology_data()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_typology_table_block(tds, typology)\n",
    "    except:\n",
    "        print(f'{tds} raised an exception.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waste Services and Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_container_counts = pd.read_csv('DATA/BULK_CONTAINER_COUNTS.csv')\n",
    "bulk_container_counts.drop(columns=bulk_container_counts.columns[3:], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wsa_data():\n",
    "    wsa_data = pd.read_csv('DATA/WASTE_SERVICES_ASSETS.csv')\n",
    "    wsa_data['TDS'] = wsa_data['DEV_TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    wsa_data['DEV_TDS'] = wsa_data['DEV_TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    wsa_data['INT_COMP_DATE'] = pd.to_datetime(wsa_data['INT_COMP_INSTALL_DATE'], errors='ignore')\n",
    "    \n",
    "    waste_collection_days = pd.read_csv('DATA/WASTE_COLLECTION_SCHEDULE.csv')\n",
    "    waste_collection_days['DEV_TDS'] = waste_collection_days['DEV_TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    day_abbreviations = {'Mon':'M',\n",
    "                        'Tue':'T',\n",
    "                        'Wed':'W',\n",
    "                        'Thu':'Th',\n",
    "                        'Fri':'F',\n",
    "                        'Sat':'Sa',\n",
    "                        'Sun':'Su'}\n",
    "    \n",
    "    def convert_days(x):\n",
    "        new_x = ''\n",
    "        try:\n",
    "            if ',' in str(x):\n",
    "                for part in str(x).split(','):\n",
    "                    new_x += day_abbreviations[part.strip()]\n",
    "\n",
    "            else:\n",
    "                new_x = day_abbreviations[str(x).strip()]\n",
    "\n",
    "            return new_x\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for col in ['FREQ_REFUS','FREQ_RECYC','FREQ_BULK']:\n",
    "        waste_collection_days[col] = waste_collection_days[col].apply(lambda x: convert_days(x))\n",
    "    \n",
    "    \n",
    "    def get_date(x):\n",
    "        try:\n",
    "            return x.strftime('%Y')\n",
    "        except:\n",
    "            return ' '\n",
    "\n",
    "    extcomp_data = pd.read_csv('DATA/EXT_COMPACTORS.csv')\n",
    "    extcomp_data['TDS'] = extcomp_data['LOCATION'].apply(lambda x: x.split('.')[0])\n",
    "    extcomp_data['INSTALLDATE'] = pd.to_datetime(extcomp_data['INSTALLDATE'])\n",
    "    extcomp_data.head()\n",
    "    groupby = extcomp_data.groupby('TDS').agg({'ASSETNUM':'count', 'INSTALLDATE': max}).reset_index()\n",
    "    groupby['EXT_COMP_YEAR'] = groupby['INSTALLDATE'].apply(lambda x: get_date(x))\n",
    "    groupby['TDS'] = groupby['TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    \n",
    "    #Adding number of bulk containers\n",
    "    bulk_container_counts = pd.read_csv('DATA/BULK_CONTAINER_COUNTS.csv')\n",
    "    candidate_list = []\n",
    "    for key, value in developments.items():\n",
    "        candidate_list.append(str(value['name']).upper())\n",
    "        for item in value['name_alternates']:\n",
    "            candidate_list.append(item.upper())\n",
    "\n",
    "    def get_dev_name(name):\n",
    "        match = process.extractOne(str(name).upper(), candidate_list)[0]\n",
    "        #print(match)\n",
    "        for key, value in developments.items():\n",
    "            if (match.upper() == value['name'].upper()) or (match.upper() in [val.upper() for val in value['name_alternates']]):\n",
    "                return value['name']\n",
    "\n",
    "        return '!!!NOT FOUND'\n",
    "\n",
    "    bulk_container_counts['DEV_NAME'] = bulk_container_counts['DEV_NAME'].apply(lambda x: get_dev_name(x))\n",
    "    wsa_data = wsa_data.merge(bulk_container_counts, on='DEV_NAME', how='left')\n",
    "\n",
    "    def get_count(x):\n",
    "        try:\n",
    "            return (str(int(x)))\n",
    "        except:\n",
    "            return '0'\n",
    "\n",
    "    wsa_data['COUNT'] = wsa_data['COUNT'].apply(lambda x: get_count(x))\n",
    "    \n",
    "    wsa_data = wsa_data.merge(waste_collection_days, on='DEV_TDS', how='left')\n",
    "    wsa_data = wsa_data.merge(groupby, on='TDS', how='left')\n",
    "    \n",
    "    #wsa_data[wsa_data['EXT_COMP'] != 0]\n",
    "\n",
    "    return wsa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_waste_services_table(cons_tds, wsa_data, counts_dict=counts):\n",
    "    dev_list = counts_dict[cons_tds]['developments']\n",
    "    if cons_tds == '091':\n",
    "        cons_data = wsa_data.query(f\"TDS in {dev_list}\").iloc[1:]\n",
    "    else:\n",
    "        cons_data = wsa_data.query(f\"TDS in {dev_list}\")\n",
    "    num_devs = counts_dict[cons_tds]['count']\n",
    "    \n",
    "    bulk_pickup_site = ''\n",
    "    \n",
    "    for dev in cons_data.itertuples():\n",
    "        if pd.isna(dev.BULK_HAULER):\n",
    "            pass\n",
    "        else:\n",
    "            bulk_pickup_site = str(dev.DEV_NAME).title()\n",
    "    \n",
    "    def make_waste_services_block(num_cols, block_data, bulk_pickup_site = bulk_pickup_site):\n",
    "        col_format = r'X|'\n",
    "        header = r'\\begin{tabularx}{\\textwidth}{V{1.5in}|'+col_format*(num_cols)+r'''}\n",
    "    \\cline{2-%s}\n",
    "                                                                                       '''% (num_cols)+r'& \\cellcolor{ccorange}{\\color[HTML]{FFFFFF} %s}'*num_cols+r' \\\\ \\hline'+'\\n'\n",
    "        hh_waste_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}Household Waste (DSNY)}               '+r'& %s'*num_cols+r'\\\\ \\hline'+'\\n'\n",
    "        bulk_waste_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}Bulk Waste}                  '+r'& %s'*num_cols+r' \\\\ \\hline'+'\\n'\n",
    "        norm_recycling_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                   '+r'& DSNY Curb Setout; collected %s'*num_cols + r'\\\\ \\hline'+'\\n'\n",
    "        special_recycling_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                   '+r'& %s'*num_cols +r'\\\\ \\hline' + '\\n'\n",
    "        \n",
    "        latex_block = r''''''\n",
    "        latex_block += header % tuple(block_data['DEV_NAME'].apply(lambda x: clean_text(str(x).title())).tolist())\n",
    "        \n",
    "        \n",
    "        hh_waste_data = []\n",
    "        bulk_waste_data = []\n",
    "        ewaste_data = []\n",
    "        textiles_data = []\n",
    "    \n",
    "        if bulk_pickup_site == '':\n",
    "            for dev in block_data.itertuples():\n",
    "                if dev.CURBSIDE == 1:\n",
    "                    hh_waste_data.append(f'Curbside Pickup {dev.FREQ_REFUS}')\n",
    "                elif dev.SHARE == 1:\n",
    "                    hh_waste_data.append(f'Transfer to {clean_text(str(dev.SHARE_SITE).title())}')\n",
    "                else:\n",
    "                    if True:\n",
    "                        if (dev.EXT_COMP_BE == 1) and (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactor in {int(dev.COMPACTOR_YARDS)} waste yard; collected as requested by staff')\n",
    "                        elif (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yard; collected as requested by staff')\n",
    "                        else:\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yards; collected as requested by staff')\n",
    "                    else:\n",
    "                        if (dev.EXT_COMP_BE == 1) and (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactor in {int(dev.COMPACTOR_YARDS)} waste yard; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                        elif (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yard; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                        else:\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yards; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                    \n",
    "                if pd.isna(dev.BULK_HAULER):\n",
    "                    if int(dev.BULK_SITES) == 0:\n",
    "                        bulk_waste_data.append(f\"Transferred for Pickup\")\n",
    "                    elif int(dev.BULK_SITES) == 1:\n",
    "                        bulk_waste_data.append(f\"One Bulk Waste Holding Site; Transferred for Pickup\")\n",
    "                    else:\n",
    "                        bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Waste Holding Sites; Transferred for Pickup\")\n",
    "                else:\n",
    "                    if int(dev.BULK_SITES) == 1:\n",
    "                        bulk_waste_data.append(f\"One Bulk Waste Holding; Picked up by {dev.BULK_HAULER}\")\n",
    "                    elif int(dev.BULK_SITES) > 1:\n",
    "                        bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Waste Holding Sites; Picked up by {dev.BULK_HAULER}\")\n",
    "                    else:\n",
    "                        bulk_waste_data.append(f\"Picked up by {dev.BULK_HAULER}\")\n",
    "\n",
    "                if dev.ECYCLE == 1:\n",
    "                    ewaste_data.append('Previously available through ECycle')\n",
    "                else:\n",
    "                    ewaste_data.append('N/A')\n",
    "\n",
    "                if dev.REFASHION == 1:\n",
    "                    textiles_data.append('Previously available through Refashion')\n",
    "                else:\n",
    "                    textiles_data.append('N/A')\n",
    "        else:\n",
    "            for dev in block_data.itertuples():\n",
    "                if dev.CURBSIDE == 1:\n",
    "                    hh_waste_data.append(f'Curbside Pickup {dev.FREQ_REFUS}')\n",
    "                elif dev.SHARE == 1:\n",
    "                    hh_waste_data.append(f'Transfer to {clean_text(str(dev.SHARE_SITE).title())}')\n",
    "                else:\n",
    "                    if True: \n",
    "                        if (dev.EXT_COMP_BE == 1) and (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactor in {int(dev.COMPACTOR_YARDS)} waste yard; collected as requested by staff')\n",
    "                        elif (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yard; collected as requested by staff')\n",
    "                        else:\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yards; collected as requested by staff')\n",
    "                    else:\n",
    "                        if (dev.EXT_COMP_BE == 1) and (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactor in {int(dev.COMPACTOR_YARDS)} waste yard; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                        elif (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yard; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                        else:\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yards; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                    \n",
    "                if pd.isna(dev.BULK_HAULER):\n",
    "                    if int(dev.BULK_SITES) == 0:\n",
    "                        bulk_waste_data.append(f\"Transferred to {bulk_pickup_site} for Pickup\")\n",
    "                    elif int(dev.BULK_SITES) == 1:\n",
    "                        bulk_waste_data.append(f\"One Bulk Waste Holding Site; Transferred to {bulk_pickup_site} for Pickup\")\n",
    "                    else:\n",
    "                        bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Waste Holding Sites; Transferred to {bulk_pickup_site} for Pickup\")\n",
    "                else:\n",
    "                    if int(dev.BULK_SITES) == 1:\n",
    "                        bulk_waste_data.append(f\"One Bulk Waste Holding Site; Picked up by {dev.BULK_HAULER}\")\n",
    "                    elif int(dev.BULK_SITES) > 1:\n",
    "                        bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Waste Holding Sites; Picked up by {dev.BULK_HAULER}\")\n",
    "                    else:\n",
    "                        bulk_waste_data.append(f\"Picked up by {dev.BULK_HAULER}\")\n",
    "\n",
    "                if dev.ECYCLE == 1:\n",
    "                    ewaste_data.append('Previously available through ECycle')\n",
    "                else:\n",
    "                    ewaste_data.append('N/A')\n",
    "\n",
    "                if dev.REFASHION == 1:\n",
    "                    textiles_data.append('Previously available through Refashion')\n",
    "                else:\n",
    "                    textiles_data.append('N/A')\n",
    "        \n",
    "        latex_block += hh_waste_line % tuple(hh_waste_data)\n",
    "        latex_block += bulk_waste_line % tuple(bulk_waste_data)\n",
    "        latex_block += norm_recycling_line % tuple(['Recycling: Paper and Cardboard']+block_data['FREQ_RECYC'].tolist())\n",
    "        latex_block += norm_recycling_line % tuple(['Recycling: Metal, Glass and Plastic']+block_data['FREQ_RECYC'].tolist())\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: Mattresses']+['N/A' for i in range(0, num_cols)])\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: E-Waste']+ewaste_data)\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: Textiles']+textiles_data)\n",
    "        latex_block += r'\\end{tabularx}'\n",
    "        \n",
    "        return latex_block\n",
    "    \n",
    "    if num_devs <= 4:\n",
    "        num_cols = num_devs\n",
    "        block_data = cons_data\n",
    "        #print(cons_data)\n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols, block_data))\n",
    "        \n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)\n",
    "        block_data_1 = cons_data.iloc[0:num_cols_1]\n",
    "        block_data_2 = cons_data.iloc[num_cols_1:]\n",
    "        \n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services_1.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols_1, block_data_1))\n",
    "            \n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services_2.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols_2, block_data_2))\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_wsa_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-258fd3d72038>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwsa_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wsa_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsolidations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmake_waste_services_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwsa_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_wsa_data' is not defined"
     ]
    }
   ],
   "source": [
    "wsa_data = load_wsa_data()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_waste_services_table(tds, wsa_data)\n",
    "    except:\n",
    "        print(f'{tds} raised exception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_waste_assets_table(cons_tds, wsa_data, counts_dict=counts):\n",
    "    dev_list = counts_dict[cons_tds]['developments']\n",
    "    if cons_tds == '091':\n",
    "        cons_data = wsa_data.query(f\"TDS in {dev_list}\").iloc[1:]\n",
    "    else:\n",
    "        cons_data = wsa_data.query(f\"TDS in {dev_list}\")\n",
    "    num_devs = counts_dict[cons_tds]['count']\n",
    "    \n",
    "#    header = r'''\n",
    "#    \\begin{tabular}{V{.25\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.25\\columnwidth}|V{.15\\columnwidth}|}\n",
    "#\\cline{2-5}\n",
    "#                                                                                              & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Internal Compactors} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} External Compactors} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Other External Assets}   & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Recycling Bins\\tnote{1}} \\\\ \\hline'''+'\\n'\n",
    "#    line_format = r'\\multicolumn{1}{|V{.25\\columnwidth}|}{\\cellcolor{ccorange}{\\color[HTML]{FFFFFF} %s}}        & %s                                                & %s                                                                  & %s & %s                                                            \\\\ \\hline'+'\\n'\n",
    "    \n",
    "    header = r'''\n",
    "    \\begin{tabular}{V{.15\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|}\n",
    "\\cline{2-6}\n",
    "                                                                                              & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Internal Compactors} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} External Compactors}  & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Bulk Containers} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Cardboard Balers} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Mattress Containers} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Recycling Bins\\tnote{1}} \\\\ \\hline'''+'\\n'\n",
    "    line_format = r'\\multicolumn{1}{|V{.15\\columnwidth}|}{\\cellcolor{ccorange}{\\color[HTML]{FFFFFF} %s}}        & %s    & %s                                               & %s           & %s      & %s                                                             & %s                                                             \\\\ \\hline'+'\\n'\n",
    "    \n",
    "    \n",
    "    latex_block = r''''''\n",
    "    latex_block += header\n",
    "    \n",
    "    for dev in cons_data.itertuples():\n",
    "        line_data = []\n",
    "        line_data.append(clean_text(str(dev.DEV_NAME).title()))\n",
    "        \n",
    "        if (dev.INT_COMP == 0):\n",
    "            int_comp_string = '0'\n",
    "        elif pd.isna(dev.INT_COMP_DATE):\n",
    "            int_comp_string = str(int(dev.INT_COMP))\n",
    "        else:\n",
    "            int_comp_string = f'{str(int(dev.INT_COMP))}; last replaced {str(dev.INT_COMP_DATE.year)}'\n",
    "        \n",
    "        line_data.append(int_comp_string)\n",
    "        \n",
    "        if pd.isna(dev.EXT_COMP_YEAR):\n",
    "            line_data.append(str(int(dev.EXT_COMP_BE)))\n",
    "        else:\n",
    "            line_data.append(f\"{int(dev.EXT_COMP_BE)}; last replaced {dev.EXT_COMP_YEAR}\")\n",
    "        \n",
    "        #if (dev.BULK_CRUSHERS == 0) and (dev.BALERS == 0)... REDO THIS ONCE DATA ARE COMPLETE\n",
    "        #line_data.append('PLACEHOLDER UNTIL DATA ARE COMPLETE')\n",
    "        try:\n",
    "            line_data.append(str(int(dev.COUNT)))\n",
    "        except:\n",
    "            line_data.append(str(dev.COUNT))\n",
    "        \n",
    "        line_data.append(str(int(dev.BALERS)))\n",
    "        \n",
    "        line_data.append(str(int(0)))\n",
    "        \n",
    "        line_data.append(str(int(dev.RECYCLING_BINS)))\n",
    "        \n",
    "        latex_block += line_format % tuple(line_data)\n",
    "    \n",
    "    latex_block += r'\\end{tabular}'\n",
    "    \n",
    "    with open(f'TABLES/waste_assets/{cons_tds}_waste_assets.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN raised exception\n"
     ]
    }
   ],
   "source": [
    "wsa_data = load_wsa_data()\n",
    "for tds in consolidations.keys():  \n",
    "    try:\n",
    "        make_waste_assets_table(tds, wsa_data)\n",
    "    except:\n",
    "        print(f'{tds} raised exception')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidation Assets\n",
    "\n",
    "Similarly to the typologies table above, this section makes use of nested tables to ensure a consistent layout. In this case, the name and number of each vehicle type is positioned above an icon of that vehicle in a 1-by-two table; these tables are then arranged side-by-side in a single-row table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vehicle_data():\n",
    "    vehicle_data = pd.read_excel('DATA/vehicle_inventory.xlsx')\n",
    "    vehicle_data['CONS'] = vehicle_data['WORK LOCATION'].apply(lambda x: str(x).replace('NYCHA-',''))\n",
    "\n",
    "    candidate_list = []\n",
    "    for key, value in consolidations.items():\n",
    "        candidate_list.append(str(value['name']).upper())\n",
    "        for item in value['alternates']:\n",
    "            candidate_list.append(item.upper())\n",
    "\n",
    "    def get_cons_name(name):\n",
    "        match = process.extractOne(str(name).upper(), candidate_list)[0]\n",
    "        #print(match)\n",
    "        for key, value in consolidations.items():\n",
    "            if (match.upper() == value['name'].upper()) or (match.upper() in [val.upper() for val in value['alternates']]):\n",
    "                return value['name']\n",
    "\n",
    "        return '!!!NOT FOUND'\n",
    "\n",
    "\n",
    "    def get_tds_from_name(x):\n",
    "        for key, value in consolidations.items():\n",
    "            if str(value['name']).upper().strip() == x.upper().strip():\n",
    "                return key\n",
    "\n",
    "        return 'N/A'\n",
    "    \n",
    "    \n",
    "    def get_vehicle_type(x):\n",
    "        \n",
    "        if type(x) != str:\n",
    "            return 'N/A'\n",
    "        \n",
    "        van_keys = ['VAN', 'SPRINTER', 'ECONOLINE', 'TRANSIT']\n",
    "        truck_keys = ['PICK-UP', 'F250', 'F450', 'SIERRA', 'RANGER', 'Pick-Up']\n",
    "        for key in van_keys:\n",
    "            if key in x:\n",
    "                return 'VAN'\n",
    "\n",
    "        for key in truck_keys:\n",
    "            if key in x:\n",
    "                return 'TRUCK'\n",
    "\n",
    "        return 'OTHER'\n",
    "    \n",
    "    vehicle_data['CONS_MATCH'] = vehicle_data['CONS'].apply(lambda x: get_cons_name(str(x)))\n",
    "\n",
    "    vehicle_data['TDS'] = vehicle_data['CONS_MATCH'].apply(lambda x: get_tds_from_name(str(x)))\n",
    "    #vehicle_data['CONS_TDS'] = vehicle_data['TDS'].apply(lambda x: developments[x]['cons_tds'] if x in developments.keys() else np.NaN)\n",
    "    vehicle_data['TYPE'] = vehicle_data['DESCRIPTION'].apply(lambda x: get_vehicle_type(x))\n",
    "    \n",
    "    return vehicle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_horticultural_data():\n",
    "    equip_table = pd.read_csv('DATA/hort_equipment.csv')\n",
    "    equip_types = pd.read_csv('DATA/hort_equipment_types.csv')\n",
    "    equip_table.rename(columns={'MODL':'MODEL'}, inplace=True)\n",
    "    equip_table = equip_table.merge(equip_types, on=['MAKE', 'MODEL'], how='left')\n",
    "    \n",
    "    candidate_list = []\n",
    "    for key, value in consolidations.items():\n",
    "        candidate_list.append(str(value['name']).upper())\n",
    "        for item in value['alternates']:\n",
    "            candidate_list.append(item.upper())\n",
    "\n",
    "    def get_cons_name(name):\n",
    "        match = process.extractOne(str(name).upper(), candidate_list)[0]\n",
    "        #print(match)\n",
    "        for key, value in consolidations.items():\n",
    "            if (match.upper() == value['name'].upper()) or (match.upper() in [val.upper() for val in value['alternates']]):\n",
    "                return value['name']\n",
    "\n",
    "        return '!!!NOT FOUND'\n",
    "\n",
    "\n",
    "    def get_tds_from_name(x):\n",
    "        for key, value in consolidations.items():\n",
    "            if str(value['name']).upper().strip() == x.upper().strip():\n",
    "                return key\n",
    "\n",
    "        return 'N/A'\n",
    "    \n",
    "    def clean_consolidations(x):\n",
    "        correction_dict = {'': 'N/A',\n",
    "                            'Loaner': 'N/A',\n",
    "                            'ATLANTIC TERMINAL': 'WYCKOFF GARDENS CONSOLIDATED',\n",
    "                            'Assignment':'N/A',\n",
    "                            'BAYCHESTER':'N/A',\n",
    "                            'FLEET (GSD)' : 'N/A',\n",
    "                            'FLEET LOANER' : 'N/A',\n",
    "                            'LOANER' : 'N/A',\n",
    "                            'Loaner' : 'N/A',\n",
    "                            'MAR' : 'N/A',\n",
    "                            'MRST' : 'N/A',\n",
    "                            'MURPHY CONSOL.' : '1010 EAST 178TH STREET',\n",
    "                            'MURPHY CONSOLIDATED' : '1010 EAST 178TH STREET',\n",
    "                            'MURPHY' : '1010 EAST 178TH STREET',\n",
    "                            \"TBD\" : 'N/A',\n",
    "                            'loaner':'N/A'}\n",
    "        if str(x).strip() in correction_dict.keys():\n",
    "            return correction_dict[str(x).strip()]\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    equip_table['CONS'] = equip_table['LOCATION'].apply(lambda x: get_cons_name(clean_consolidations(x)))\n",
    "    equip_table['CONS_TDS'] = equip_table['CONS'].apply(lambda x: get_tds_from_name(x))\n",
    "    \n",
    "    return equip_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_consolidation_assets_table(cons_tds, vehicle_data, hort_data):\n",
    "    cons_vehicle_data = vehicle_data[vehicle_data['TDS'] == cons_tds]\n",
    "    cons_hort_data = hort_data[hort_data['CONS_TDS'] == cons_tds]\n",
    "    \n",
    "    block_template = r'''\\begin{tabular}{m{.25\\columnwidth}m{.25\\columnwidth}m{.25\\columnwidth}m{.25\\columnwidth}}\n",
    "    {\\color{ccorange} %s Trucks} & {\\color{ccorange} %s Skid Steers} & {\\color{ccorange} %s Tractors} & {\\color{ccorange} %s Sweepers} \\\\\n",
    "    \\includegraphics[width=.15\\columnwidth]{\\rootpath/IMAGES/truck.png}  & \\includegraphics[width=.15\\columnwidth]{\\rootpath/IMAGES/bobcat.png} & \\includegraphics[width=.15\\columnwidth]{\\rootpath/IMAGES/tractor.png} & \\includegraphics[width=.15\\columnwidth]{\\rootpath/IMAGES/road-sweeper.png}                         \n",
    "    \\end{tabular}'''\n",
    "    \n",
    "    num_trucks = cons_vehicle_data[cons_vehicle_data['TYPE'] == 'TRUCK'].shape[0]\n",
    "    num_vans = cons_vehicle_data[cons_vehicle_data['TYPE'] == 'VAN'].shape[0]\n",
    "    num_skidsteers = cons_hort_data[cons_hort_data['TYPE'] == 'SKIDSTEER'].shape[0]\n",
    "    num_mowers = cons_hort_data[cons_hort_data['TYPE'] == 'MOWER'].shape[0]\n",
    "    num_tractors = cons_hort_data.query(f\"TYPE in {['TRACTOR', 'TOOLCAT']}\").shape[0]\n",
    "    num_sweepers = cons_hort_data.query(f\"TYPE in {['SWEEPER', 'VAC']}\").shape[0]\n",
    "    \n",
    "    block_data = [str(num_trucks), str(num_skidsteers), str(num_tractors), str(num_sweepers)]\n",
    "    \n",
    "    with open(f'TABLES/consolidation_assets/{cons_tds}_consolidation_assets.tex', 'w') as file_handle:\n",
    "        file_handle.write(block_template % tuple(block_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_data = load_vehicle_data()\n",
    "hort_data = load_horticultural_data()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_consolidation_assets_table(tds, vehicle_data, hort_data)\n",
    "    except:\n",
    "        print(f'{tds} raised exception')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waste Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_waste_cols(overview_data):\n",
    "    \n",
    "    try:\n",
    "        diversion_rates = pd.read_csv('DIVERSION_RATES.csv')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    conversion_factors = {'units_to_tons_day': 0.0025,\n",
    "                         'cy_per_ton': {'trash': 21.05,\n",
    "                                        'trash_actual': 0,\n",
    "                                       'MGP': 18.02,\n",
    "                                       'cardboard': 26.67,\n",
    "                                       'paper': 6.19,\n",
    "                                       'organics': 4.32,\n",
    "                                       'ewaste': 5.65,\n",
    "                                       'textiles': 13.33},\n",
    "                         'gallons_per_cy': 201.974,\n",
    "                         'gallons_per_64gal': 64,\n",
    "                         'gallons_per_40lb_bag': 44,\n",
    "                         'cy_per_44gal_bag':0.174,\n",
    "                         'cy_per_cardboard_bale':0.193}\n",
    "\n",
    "    waste_percentages = {'trash': .26,\n",
    "                         'trash_actual':.894,\n",
    "                        'MGP': .19,\n",
    "                        'cardboard': .07,\n",
    "                        'paper': .07,\n",
    "                        'organics':.32,\n",
    "                        'ewaste': .01,\n",
    "                        'textiles': .08}\n",
    "\n",
    "    capture_rates = {'trash_primary': .75,\n",
    "                    'trash_secondary': .25,\n",
    "                    'mgp': .30,\n",
    "                    'cardboard': .50,\n",
    "                    'paper': .20}\n",
    "    \n",
    "    def get_capture_rate(TDS, CONS_TDS):\n",
    "        try:\n",
    "            trash_primary = diversion_rates.loc[diversion_rates['TDS']==TDS]['TRASH_PRIMARY']\n",
    "            trash_secondary = diversion_rates.loc[diversion_rates['TDS']==TDS]['TRASH_SECONDARY']\n",
    "        except:\n",
    "            try:\n",
    "                trash_primary = diversion_rates.loc[diversion_rates['CONS_TDS']==CONS_TDS]['TRASH_PRIMARY']\n",
    "                trash_secondary = diversion_rates.loc[diversion_rates['CONS_TDS']==CONS_TDS]['TRASH_SECONDARY']\n",
    "            except:\n",
    "                trash_primary = .75\n",
    "                trash_secondary = .25\n",
    "        \n",
    "        return [trash_primary, trash_secondary]\n",
    "    \n",
    "    overview_data['WASTE_TONS_DAY'] = overview_data['CURRENT_APTS'].apply(lambda x: x * conversion_factors['units_to_tons_day'])\n",
    "    overview_data['TRASH_PRIMARY'] = overview_data.apply(lambda row: get_capture_rate(row['TDS'], row['CONS_TDS'])[0], axis=1)\n",
    "    overview_data['TRASH_SECONDARY'] = overview_data.apply(lambda row: get_capture_rate(row['TDS'], row['CONS_TDS'])[1], axis=1)\n",
    "    \n",
    "    for key, value in waste_percentages.items():\n",
    "        overview_data[f'{key.upper()}_CY'] = overview_data['WASTE_TONS_DAY'].apply(lambda x: x * value * conversion_factors['cy_per_ton'][key])\n",
    "        overview_data[f'{key.upper()}_TONS'] = overview_data['WASTE_TONS_DAY'].apply(lambda x: x * value)\n",
    "    \n",
    "    overview_data['TRASH_ACTUAL_CY'] = (overview_data['TRASH_CY']+\n",
    "                                           overview_data['MGP_CY']+\n",
    "                                           overview_data['CARDBOARD_CY']+\n",
    "                                           overview_data['PAPER_CY']+\n",
    "                                           overview_data['ORGANICS_CY']+\n",
    "                                           overview_data['EWASTE_CY']+\n",
    "                                           overview_data['TEXTILES_CY'])-(overview_data['MGP_CY']*capture_rates['mgp']+\n",
    "                                                                         overview_data['CARDBOARD_CY']*capture_rates['cardboard']+\n",
    "                                                                         overview_data['PAPER_CY']*capture_rates['paper'])\n",
    "\n",
    "    overview_data['TRASH_CHUTE_CY'] = overview_data['TRASH_ACTUAL_CY']*overview_data['TRASH_PRIMARY']\n",
    "    overview_data['TRASH_CHUTE_TONS'] = overview_data['TRASH_ACTUAL_TONS']*overview_data['TRASH_PRIMARY']\n",
    "    overview_data['TRASH_CHUTE_SAUSAGE'] = ((overview_data['TRASH_CHUTE_CY'])/conversion_factors['cy_per_ton']['trash'])*(2000/40)\n",
    "    overview_data['TRASH_DROP_CY'] = overview_data['TRASH_ACTUAL_CY']*overview_data['TRASH_SECONDARY']\n",
    "    overview_data['TRASH_DROP_TONS'] = overview_data['TRASH_ACTUAL_TONS']*overview_data['TRASH_SECONDARY']\n",
    "    overview_data['TRASH_DROP_BINS'] = overview_data['TRASH_DROP_CY']*conversion_factors['gallons_per_cy']/64\n",
    "    overview_data['CAPTURED_MGP_TONS_WEEK'] = overview_data['MGP_TONS']*capture_rates['mgp']*7\n",
    "    overview_data['CAPTURED_CARDBOARD_TONS_WEEK'] = overview_data['CARDBOARD_TONS']*capture_rates['cardboard']*7\n",
    "    overview_data['CAPTURED_PAPER_TONS_WEEK'] = overview_data['PAPER_TONS']*capture_rates['paper']*7\n",
    "    overview_data['MGP_BAGS_WEEK'] = overview_data['MGP_CY']*capture_rates['mgp']*7/conversion_factors['cy_per_44gal_bag']\n",
    "    overview_data['PAPER_BAGS_WEEK'] = overview_data['PAPER_CY']*capture_rates['paper']*7/conversion_factors['cy_per_44gal_bag']\n",
    "    overview_data['CARDBOARD_BALES_WEEK'] = overview_data['CARDBOARD_CY']*capture_rates['cardboard']*7/conversion_factors['cy_per_cardboard_bale']\n",
    "    \n",
    "    actual_tonnage = pd.read_csv('DATA/WASTE_TONNAGE_2017.csv').dropna()\n",
    "    overview_data = overview_data.merge(actual_tonnage, on='DEV_NAME', how='left')\n",
    "    \n",
    "    return overview_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_waste_distribution_table(cons_tds, overview_data, wsa_data):\n",
    "    cons_data = overview_data[overview_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "    \n",
    "    if num_devs == 1:\n",
    "        num_cols = num_devs\n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)+1\n",
    "    else:\n",
    "        num_cols = num_devs+1\n",
    "    \n",
    "    if num_devs != 1:\n",
    "        cons_data.loc['Total']= cons_data.sum(numeric_only=True, axis=0)\n",
    "        cons_data.loc['Total','DEV_NAME'] = 'Total'\n",
    "    \n",
    "    def make_waste_distribution_table_block(cons_data, num_cols):\n",
    "        dev_col_format = r'X|'\n",
    "\n",
    "        opening = r'''\n",
    "        \\begin{tabularx}{\\textwidth}{V{1.25in}|%s}\n",
    "        \\cline{2-%s}\n",
    "        ''' % (dev_col_format*num_cols, (num_cols+1))\n",
    "\n",
    "        top_row = r'''\n",
    "                                                                       '''+(r\"& \\multicolumn{1}{V{1.25in}|}{\\cellcolor{ccorange}{\\color[HTML]{FFFFFF}%s}}\"*(num_cols))+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        standard_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s}                 \"+(r\"& %s                                    \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        captured_row = r\"\\multicolumn{1}{|Y{1.25in}|}{\\cellcolor{ccorangelight}Captured / Week (tons)\\tnote{4}}                        \"+(r\"& %s                                    \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        #chute_row = r\"\\multicolumn{1}{|Y{1.25in}|}{\\cellcolor{ccorangelight}Trash Chutes\\tnote{3}}                 \"+(r\"& %s tons [%s 40 lbs. sausage bags]      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "        chute_row = r\"\\multicolumn{1}{|Y{1.25in}|}{\\cellcolor{ccorangelight}Trash Chutes\\tnote{3}}                 \"+(r\"& %s sausage bags      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        #dropsite_row = r\"\\multicolumn{1}{|Y{1.25in}|}{\\cellcolor{ccorangelight}Drop Sites\\tnote{4}}                 \"+(r\"& %s tons [%s 64-gal. bins]      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "        dropsite_row = r\"\\multicolumn{1}{|Y{1.25in}|}{\\cellcolor{ccorangelight}Drop Sites\\tnote{4}}                 \"+(r\"& %s 64-gal. bins      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        OET_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s / Day (CY)}              \"+(r\"& %s                                    \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        #recycling_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s \\tnote{5}}                 \"+(r\"& %s tons [%s 44-gal. bags]                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "        recycling_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s \\tnote{6}}                 \"+(r\"& %s 44-gal. bags                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        #cardboard_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s \\tnote{5}}                 \"+(r\"& %s tons [%s bales]                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "        cardboard_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s \\tnote{6}}                 \"+(r\"& %s bales                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "\n",
    "        def make_trash_text(row, text_var, cy_col, other_col):\n",
    "            text_var.append(round(row[cy_col],1))\n",
    "            text_var.append(round(row[other_col], 1))\n",
    "            pass\n",
    "\n",
    "        latex_block = r'\\textbf{Projected Daily Trash Volumes}'\n",
    "        latex_block += opening\n",
    "        latex_block += top_row % tuple(cons_data['DEV_NAME'].apply(lambda x: clean_text(str(x).title())).tolist())\n",
    "        latex_block += standard_row % tuple([r\"Waste Generated / Day (Tons)\\tnote{1}\"]+[round(item, 1) for item in cons_data['WASTE_TONS_DAY'].tolist()])\n",
    "        latex_block += standard_row % tuple([r\"Trash / Day (tons)\\tnote{2}\"]+cons_data['TRASH_ACTUAL_TONS'].apply(lambda x: str(round(x,1))).tolist())\n",
    "\n",
    "        trash_chute_text = []\n",
    "        dropsite_text = []\n",
    "\n",
    "        #cons_data.apply(lambda row: make_trash_text(row, trash_chute_text, 'TRASH_CHUTE_TONS', 'TRASH_CHUTE_SAUSAGE'), axis=1)\n",
    "        #cons_data.apply(lambda row: make_trash_text(row, dropsite_text, 'TRASH_DROP_TONS', 'TRASH_DROP_BINS'), axis=1)\n",
    "\n",
    "        #latex_block += chute_row % tuple(trash_chute_text)\n",
    "        #latex_block += dropsite_row % tuple(dropsite_text)\n",
    "        latex_block += chute_row % tuple([round(item, 1) for item in cons_data['TRASH_CHUTE_SAUSAGE'].tolist()])\n",
    "        latex_block += dropsite_row % tuple([round(item, 1) for item in cons_data['TRASH_DROP_BINS'].tolist()])\n",
    "        latex_block += standard_row % tuple([r\"Est. Drop Sites \\tnote{5}\"]+cons_data['BLDGS'].apply(lambda x: str(int(x))).tolist())\n",
    "\n",
    "        \n",
    "        latex_block += r\"\\end{tabularx}\\bigskip\"\n",
    "        \n",
    "        latex_block += r'\\textbf{Projected Weekly Recycling Volumes}'\n",
    "        latex_block += opening\n",
    "        latex_block += top_row % tuple(cons_data['DEV_NAME'].apply(lambda x: clean_text(str(x).title())).tolist())\n",
    "\n",
    "        mgp_text = []\n",
    "        cardboard_text= []\n",
    "        paper_text = []\n",
    "\n",
    "        #cons_data.apply(lambda row: make_trash_text(row, mgp_text, 'CAPTURED_MGP_TONS_WEEK', 'MGP_BAGS_WEEK'), axis=1)\n",
    "\n",
    "        #cons_data.apply(lambda row: make_trash_text(row, cardboard_text, 'CAPTURED_CARDBOARD_TONS_WEEK', 'CARDBOARD_BALES_WEEK'), axis=1)\n",
    "\n",
    "        #cons_data.apply(lambda row: make_trash_text(row, paper_text, 'CAPTURED_PAPER_TONS_WEEK', 'PAPER_BAGS_WEEK'), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        latex_block += recycling_row % tuple([r\"Metal, Glass, Plastic Captured / Week (tons)\"]+[round(item, 1) for item in cons_data['MGP_BAGS_WEEK'].tolist()])\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_MGP_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        latex_block += cardboard_row % tuple([r\"Cardboard Captured / Week (tons)\"]+[round(item, 1) for item in cons_data['CARDBOARD_BALES_WEEK'].tolist()])\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_CARDBOARD_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        latex_block += recycling_row % tuple([r\"Paper Captured / Week (tons)\"]+[round(item, 1) for item in cons_data['PAPER_BAGS_WEEK'].tolist()])\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_PAPER_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "\n",
    "        #latex_block += OET_row % tuple(['Organics']+cons_data['ORGANICS_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        #latex_block += OET_row % tuple(['E-Waste']+cons_data['EWASTE_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        #latex_block += OET_row % tuple(['Textiles']+cons_data['TEXTILES_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "\n",
    "        latex_block += r\"\\end{tabularx}\"\n",
    "\n",
    "        return latex_block\n",
    "    \n",
    "    if num_devs<= 4:\n",
    "        latex_block = make_waste_distribution_table_block(cons_data, num_cols)\n",
    "\n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block)\n",
    "    \n",
    "    else:\n",
    "        latex_block_1 = make_waste_distribution_table_block(cons_data.iloc[0:num_cols_1], num_cols_1)\n",
    "        latex_block_2 = make_waste_distribution_table_block(cons_data.iloc[num_cols_1:], num_cols_2)\n",
    "        \n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table_1.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block_1)\n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table_2.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block_2)\n",
    "            \n",
    "    '''\n",
    "\n",
    "    text_block = r''''''\n",
    "\n",
    "    text_line_multi = r\"{%s}: This development has %s apartment units and %s stairhalls.\\\\\"\n",
    "\n",
    "    text_line_singular = r\"{%s}: This development has %s apartment units and one stairhall.\\\\\"\n",
    "\n",
    "    for row in cons_data.itertuples():\n",
    "\n",
    "        if int(row.STAIRHALLS) == 1:\n",
    "            text_block += text_line_singular % (clean_text(row.DEV_NAME.title()), int(row.CURRENT_APTS))\n",
    "        else:\n",
    "            text_block += text_line_multi % (clean_text(row.DEV_NAME.title()), int(row.CURRENT_APTS), int(row.STAIRHALLS))\n",
    "\n",
    "\n",
    "    with open(f'TEXT/waste_distribution_bottom/{cons_tds}_wd_bottom.tex', 'w') as file_handle:\n",
    "        file_handle.write(text_block)\n",
    "    \n",
    "    '''\n",
    "    with open(f'TEXT/waste_distribution_bottom/{cons_tds}_wd_bottom.tex', 'w') as file_handle:\n",
    "        file_handle.write('')  \n",
    "        \n",
    "    top_block_template = r'''\n",
    "    Quantifying how much waste is generated at each consolidation will inform how well current assets and services serve current needs, and what additional elements are necessary for each consolidation to operate as efficiently as possible.\n",
    "    \n",
    "    %s has %s 30-cubic yard exterior compactors. %s'''\n",
    "    \n",
    "    #wsa_data = load_wsa_data()\n",
    "    wsa_data = wsa_data.query(f\"TDS in {counts[cons_tds]['developments']}\")\n",
    "    extcomp_total = int(wsa_data['EXT_COMP_BE'].sum())\n",
    "    #print(cons_data['TRASH_ACTUAL_CY'])\n",
    "    #print(cons_data['TRASH_ACTUAL_CY'].sum())\n",
    "    #days_to_fill = extcomp_total*(cons_data['TRASH_ACTUAL_CY'].sum())/(30)\n",
    "    weight_df = cons_data[['TDS','DEV_NAME','TONS_PER_CONTAINER']].dropna()\n",
    "    \n",
    "    if weight_df.shape[0] > 0:\n",
    "        weight_at_collection = round(weight_df['TONS_PER_CONTAINER'].mean(),1)\n",
    "        weight_text = f'On average, the exterior compactors at this consolidation contain {weight_at_collection} tons of waste at the time of collection. DSNY prefers compactors to contain more than 7 tons and up to 12 tons at collection. The closer to 12 tons, the more efficient collection is for both DSNY and the consolidation.'\n",
    "    else:\n",
    "        weight_at_collection = None\n",
    "        weight_text = 'The average weight of DSNY collections at this consolidation are unknown.'\n",
    "    \n",
    "    top_block_data = []\n",
    "    top_block_data.append(clean_text(str(consolidations[cons_tds]['name']).title()))\n",
    "    \n",
    "    if extcomp_total > 0:\n",
    "        top_block_data.append(f'({str(extcomp_total)})')\n",
    "    else:\n",
    "        top_block_data.append('no')\n",
    "        \n",
    "    top_block_data.append(weight_text)\n",
    "    \n",
    "    with open(f'TEXT/waste_distribution_top/{cons_tds}_wd_top.tex', 'w') as file_handle:\n",
    "        file_handle.write(top_block_template % tuple(top_block_data))\n",
    "    \n",
    "    #print(top_block_data)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_waste_cols(load_overview_data()).to_csv('OVERVIEW_WITH_WASTE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised by NaN\n"
     ]
    }
   ],
   "source": [
    "overview_data = add_waste_cols(load_overview_data())\n",
    "wsa_data = load_wsa_data()\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_waste_distribution_table(tds, overview_data, wsa_data)\n",
    "    except:\n",
    "        print(f'Exception raised by {tds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Capital Improvements Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_asset_data():\n",
    "    asset_data = {'fwd': ['In-Sink Food Grinders', pd.read_csv('DATA/capital_fwd.csv')],\n",
    "                  'ehd': ['Enlarged Hopper Doors', pd.read_csv('DATA/capital_ehd.csv')],\n",
    "                  'int_compactor':['Interior Compactor Replacement', pd.read_csv('DATA/capital_intcom.csv')],\n",
    "                  'wasteyard':['Waste Yard Redesign', pd.read_csv('DATA/capital_wasteyard.csv')]}\n",
    "\n",
    "    for value in asset_data.values():\n",
    "        value[1].columns = [item.strip() for item in value[1].columns]\n",
    "\n",
    "    asset_data['wasteyard'][1]['ESTIMATE'] = asset_data['wasteyard'][1]['TOT_EST']\n",
    "    asset_data['wasteyard'][1]['COST'] = np.nan\n",
    "    \n",
    "    def year_to_string(year):\n",
    "        if pd.isna(year):\n",
    "            return 'N/A'\n",
    "        else:\n",
    "            if int(year) <= 2022:\n",
    "                return str(int(year))\n",
    "            elif (int(year) > 2022) & (int(year) <= 2025):\n",
    "                return '2023-2025'\n",
    "            elif (int(year)>2025) and (int(year)<=2030):\n",
    "                return '2026-2030'\n",
    "            else:\n",
    "                return 'After 2030'\n",
    "    \n",
    "    asset_data['fwd'][1]['_YEAR'] = asset_data['fwd'][1]['EST_YEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['ehd'][1]['_YEAR'] = asset_data['ehd'][1]['CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['int_compactor'][1]['_YEAR'] = asset_data['int_compactor'][1]['CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['wasteyard'][1]['_YEAR'] = asset_data['wasteyard'][1]['CONS_CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    return asset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_capital_table(cons_tds, asset_data, overview_data=overview_data):\n",
    "    cons_data = overview_data[overview_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "\n",
    "    def make_capital_table_block(block_data, num_devs):\n",
    "        dev_col_format = r'X|'\n",
    "        header = r'''\n",
    "        \\begin{tabularx}{\\textwidth}{r|%s}\n",
    "        \\cline{2-%s}\n",
    "        ''' % ((dev_col_format*num_devs), num_devs)\n",
    "\n",
    "        top_row = r\"\\multicolumn{1}{l|}{}                                                        \"+r\"& \\cellcolor{ccorange}{\\color[HTML]{FFFFFF}%s} \"*num_devs+r\"\\\\ \\hline\"+\"\\n\"\n",
    "\n",
    "        project_block = r\"\\multicolumn{1}{|V{.2\\columnwidth}|}{\\cellcolor{ccorangelight}%s}          \"+(r\"&                                                                  \"*num_devs)+r\"\\\\\"+r'''\n",
    "        \\multicolumn{1}{|r|}{\\cellcolor{ccorangelight}\\textit{Status}}                '''+(r\"& %s                                                         \"*num_devs)+r'''\\\\\n",
    "        \\multicolumn{1}{|r|}{\\cellcolor{ccorangelight}\\textit{%s}}                  '''+(\"& %s                                                     \"*num_devs)+r\"\\\\ \\hline\"+\"\\n\"\n",
    "\n",
    "        devs = block_data['DEV_NAME'].apply(lambda x: str(x).upper()).tolist()\n",
    "        devs_title = block_data['DEV_NAME'].apply(lambda x: clean_text(str(x).title())).tolist()\n",
    "        latex_block = ''\n",
    "        latex_block += header\n",
    "        latex_block += top_row % tuple(block_data['DEV_NAME'].apply(lambda x: clean_text(str(x).title())).tolist())\n",
    "\n",
    "        for asset in asset_data.keys():\n",
    "            asset_df = asset_data[asset][1]\n",
    "            #print(asset_data[asset][0])\n",
    "            #print(devs)\n",
    "            #print(asset_df['DEVELOPMENT'].tolist())\n",
    "            if any((dev in asset_df['DEVELOPMENT'].tolist()) for dev in devs):\n",
    "                status_list = []\n",
    "                year_list = []\n",
    "\n",
    "                for dev in devs:\n",
    "                    if dev in asset_df['DEVELOPMENT'].tolist():\n",
    "                        #print(dev)\n",
    "                        if pd.isna(asset_df.loc[asset_df['DEVELOPMENT']== dev,'STATUS'].iloc[0]):\n",
    "                            status_list.append('Not Yet Scheduled')\n",
    "                        else:\n",
    "                            status_list.append(str(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'STATUS'].iloc[0]).title())\n",
    "\n",
    "                        #print(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'STATUS'])\n",
    "                        #print(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'COST'])\n",
    "                        #try:\n",
    "                        year_list.append(str(asset_df.loc[asset_df['DEVELOPMENT']== dev,'_YEAR'].iloc[0]))\n",
    "                    #except:\n",
    "                            #year_list.append('TBD')\n",
    "\n",
    "                    else:\n",
    "                        status_list.append('N/A')\n",
    "                        year_list.append(' ')\n",
    "\n",
    "                asset_block = project_block % tuple([asset_data[asset][0]]+status_list+['Year Planned']+year_list)\n",
    "\n",
    "                latex_block += asset_block\n",
    "\n",
    "        latex_block += r\"\\end{tabularx}\"\n",
    "\n",
    "        return latex_block\n",
    "    \n",
    "    \n",
    "    if num_devs <= 4:\n",
    "        num_cols = num_devs\n",
    "        block_data = cons_data\n",
    "        \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data, num_cols))\n",
    "        \n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)\n",
    "        block_data_1 = cons_data.iloc[0:num_cols_1]\n",
    "        block_data_2 = cons_data.iloc[num_cols_1:]\n",
    "        \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects_1.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data_1, num_cols_1))\n",
    "            \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects_2.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data_2, num_cols_2))\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_data= load_asset_data()\n",
    "overview_data = load_overview_data()\n",
    "for tds in consolidations.keys(): \n",
    "    make_capital_table(tds, asset_data, overview_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Staff Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staff_data(name_dict):\n",
    "    #Read budgeted staff and formula allocation\n",
    "    dev_staff = pd.read_csv('DATA/staff_for_table.csv')\n",
    "    dev_staff.fillna(0,inplace=True)\n",
    "    \n",
    "    def find_cons_tds(name, name_dict):\n",
    "        corrections = {'East River CONSOLIDATED':'009',\n",
    "                      'FOREST CONSOLIDATION': '059',\n",
    "                      'WILSON CONSOLIDATED': '112',\n",
    "                      'WOODSON CONSOLIDATED': '182',\n",
    "                      'BEACH 41ST STREET CONSOLIDATION':'165',\n",
    "                      'TAYLOR-WYTHE CONSOLIDATED': '234'}\n",
    "        for key, value in name_dict.items():\n",
    "            if (name == value['name']) | (name in value['alternates']):\n",
    "                return key\n",
    "            else:\n",
    "                try:\n",
    "                    return corrections[name]\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "    dev_staff['CONS_TDS'] = dev_staff['Consolidation'].apply(lambda x: find_cons_tds(x, name_dict))\n",
    "    dev_staff['CONS_NAME'] = dev_staff['CONS_TDS'].apply(lambda x: name_dict[x]['name'] if x is not None else 'NO NAME FOUND')\n",
    "    #Note: Staff list missing for Armstrong, Ft. Washington, and Williams Plaza, as well as scatter-site third-party-managed consolidations\n",
    "    dev_staff.to_csv('DATA/_TROUBLESHOOTING_DEV_STAFF.csv')\n",
    "    #Read budgeted staff and actuals\n",
    "    actuals_data = pd.read_csv('DATA/Staffing_Analysis/DEVHC.csv')\n",
    "    actuals_data.fillna(0, inplace=True)\n",
    "    actuals_data = actuals_data[actuals_data['RC Name'].apply(lambda x: \"total\" not in str(x).lower()) & actuals_data['Department'].apply(lambda x: \"total\" not in str(x).lower())]\n",
    "    actuals_data['CONS_TDS'] = actuals_data['RC Name'].apply(lambda x: find_cons_tds(x, name_dict))\n",
    "    actuals_data['CONS_NAME'] = actuals_data['CONS_TDS'].apply(lambda x: name_dict[x]['name'] if x is not None else 'NO NAME FOUND')\n",
    "\n",
    "    def convert_neg(x):\n",
    "        try:\n",
    "            return int(x)\n",
    "        except:\n",
    "            return int('-'+str(x).replace('(','').replace(')',''))\n",
    "\n",
    "    actuals_data['VARIANCE'] = actuals_data['Unnamed: 5'].apply(lambda x: convert_neg(x))\n",
    "    actuals_data['ACT'] = actuals_data['13']\n",
    "    \n",
    "    table_frame = pd.read_csv('DATA/Table_Keys.csv')\n",
    "    actuals_keys = pd.read_csv('DATA/Staffing_Analysis/DEVHC_CODES.csv')\n",
    "    \n",
    "    actuals_data = actuals_data.merge(actuals_keys, how='left', left_on='CST_NAME', right_on='TITLE_NAME')\n",
    "    for column in ['Current Modified', 'ACT', 'VARIANCE']:\n",
    "        actuals_data[column] = actuals_data[column].astype(int)\n",
    "        \n",
    "    actuals_data.to_csv('DATA/_TROUBLESHOOTING_ACTUALS.csv')\n",
    "    \n",
    "    return (dev_staff, actuals_data, table_frame, actuals_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_staff_table(cons_tds, dev_staff, actuals_data, table_frame, actuals_keys):\n",
    "    #Fetching staff data for consolidation\n",
    "    cons_data = dev_staff.loc[dev_staff['CONS_TDS'] == cons_tds]\n",
    "    if cons_data.shape[0] == 0:\n",
    "        with open(f'TABLES/staff_table/{cons_tds}_staff_table.tex', 'w') as file_handle:\n",
    "            file_handle.write('')\n",
    "        return(f'Consolidation {cons_tds} not found in staffing data.')\n",
    "    #print(cons_tds)\n",
    "    #print(dev_staff)\n",
    "    \n",
    "    # Isolate and process actuals data for consolidation\n",
    "    try:\n",
    "        cons_actuals = actuals_data[actuals_data['CONS_TDS'] == cons_tds]\n",
    "    except:\n",
    "        print(f'{cons_tds} not found in actuals.')\n",
    "        return np.NaN\n",
    "    \n",
    "    cons_actuals = cons_actuals[['CONS_NAME', 'CONS_TDS', 'Current Modified', 'ACT', \n",
    "                                 'CODE_KEY', 'CODE_NAME']].groupby(by='CODE_KEY', as_index=False).agg({'CONS_NAME': 'first',\n",
    "                                                                                                       'CONS_TDS': 'first',\n",
    "                                                                                                     'Current Modified':sum,\n",
    "                                                                                                     'ACT':sum,\n",
    "                                                                                                     'CODE_NAME':'first'})\n",
    "    cons_actuals\n",
    "    cons_actuals.loc['Total']= cons_actuals.sum(numeric_only=True, axis=0)\n",
    "    cons_actuals.loc['Total','CODE_KEY'] = 11\n",
    "    cons_actuals.loc['Total','CODE_NAME'] = 'TOT'\n",
    "    \n",
    "    for row in cons_actuals.itertuples():\n",
    "        cons_data[f'{row.CODE_NAME}_ACT'] = row.ACT\n",
    "    #print(cons_data)\n",
    "    #Setting up table and transposing data\n",
    "    cons_table_frame = table_frame\n",
    "    cons_table_frame['Formula'] = cons_table_frame['FORMULA_KEY'].iloc[:-1].apply(lambda key: cons_data[key].iloc[0])\n",
    "    cons_table_frame['Budgeted'] = cons_table_frame['BUDG_KEY'].apply(lambda key: cons_data[key].iloc[0])\n",
    "    cons_table_frame['Actual'] = cons_table_frame['ACTUALS_KEY'].iloc[:-2].apply(lambda key: cons_data[key].iloc[0] if key in cons_data.columns else 0)\n",
    "\n",
    "    \n",
    "    #Simplifying table\n",
    "    cons_table = cons_table_frame[['CHART_LINE', 'Formula', 'Budgeted', 'Actual']]\n",
    "    #print(cons_table)\n",
    "    \n",
    "    #Defining LaTeX table format\n",
    "    \n",
    "    def make_staff_table_block(staff_data):\n",
    "    \n",
    "        table_template = r'''\n",
    "        \\begin{tabular}{l|c|c|c|}\n",
    "        \\cline{2-4}\n",
    "                                                                                     & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Formula Allocation \\tnote{1}} & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Budgeted} & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Actual Staff (June 2020)} \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Employees}                      & %s                                                      & %s                                                                & %s                                                        \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Property Manager}               & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Asst. Property Manager}         & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Secretaries}                    & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Housing Assistants}             & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Superintendent}                 & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Assistant Superintendent}       & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Supervisor of Caretakers (SOC)} & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Supervisor of Grounds (SOG)}    & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Maintenance Workers}            & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers X}                   & %s                                                      & %s                                                                &                                                       \\\\ \\cline{1-3}\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers J\\tnote{2}}                   &                                                       & %s                                                                &                                                         \\\\ \\cline{1-1} \\cline{3-3}\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers G}                   & \\multirow{-2}{*}{%s}                                                      & %s                                     & \\multirow{-3}{*}{%s \\tnote{3}}                           \\\\ \\hline\n",
    "        \\end{tabular}\n",
    "        \n",
    "        '''\n",
    "\n",
    "        values = []\n",
    "\n",
    "        def extract_data_through_mw(row):\n",
    "            [values.append(item) for item in [str(int(row['Formula'])), \n",
    "                                              str(int(row['Budgeted'])), \n",
    "                                              str(int(row['Actual']))]]\n",
    "            pass\n",
    "\n",
    "        #Processing through Maintenance Worker\n",
    "        staff_data.iloc[0:-3].apply(lambda row: extract_data_through_mw(row), axis=1)\n",
    "\n",
    "        #Processing Caretakers\n",
    "        values.append(str(int(staff_data.iloc[-3, 1])))\n",
    "        values.append(str(int(staff_data.iloc[-3, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-2, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-2, 1])))\n",
    "        values.append(str(int(staff_data.iloc[-1, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-3, 3])))\n",
    "\n",
    "        return table_template % tuple(values)\n",
    "    \n",
    "    #Make and export LaTeX code\n",
    "    with open(f'TABLES/staff_table/{cons_tds}_staff_table.tex', 'w') as file_handle:\n",
    "        file_handle.write(make_staff_table_block(cons_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staff_description_text():\n",
    "    text = get_docx_text('TEXT/WM_Role_Descriptions.docx')\n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "staff_data = load_staff_data(consolidations)\n",
    "#staff_descriptions = load_staff_description_text()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    make_staff_table(tds, *staff_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Analysis Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process File Names\n",
    "def process_analysis_graphic_paths():\n",
    "    \n",
    "    def clean_paths(path_list):\n",
    "        clean_path_list = []\n",
    "        for path in path_list:\n",
    "            if (' ' in path.split('/')[-1]) or ('&' in path.split('/')[-1]):\n",
    "                os.rename(path, path.replace(' ','-').replace('&','-'))\n",
    "                clean_path_list.append(path)\n",
    "            else:\n",
    "                clean_path_list.append(path)\n",
    "            \n",
    "        return clean_path_list\n",
    "            \n",
    "    cons_bar_charts_raw = list(glob.glob('WORK_ORDER_ANALYSIS/Consolidation_BarCharts/png/*'))\n",
    "    cons_bar_charts = clean_paths(cons_bar_charts_raw)\n",
    "    \n",
    "    tds_nums = [path.split('/')[-1].split('_')[0].zfill(3) for path in cons_bar_charts]\n",
    "\n",
    "    cons_chart_paths = {}\n",
    "    for pair in list(zip(tds_nums, cons_bar_charts)):\n",
    "        cons_chart_paths[pair[0]] = pair[1]\n",
    "    \n",
    "    dev_chart_paths = {}\n",
    "\n",
    "    dev_bar_charts_raw = glob.glob('WORK_ORDER_ANALYSIS/Development_BarCharts/png/*')\n",
    "    dev_bar_charts = clean_paths(dev_bar_charts_raw)\n",
    "    \n",
    "    dev_tds_nums = [path.split('/')[-1].split('_')[0].zfill(3) for path in dev_bar_charts]\n",
    "\n",
    "    for pair in list(zip(dev_tds_nums, dev_bar_charts)):\n",
    "        dev_chart_paths[pair[0]] = {'Development_BarCharts': pair[1]}\n",
    "\n",
    "    for directory in ['Dev_Interior_Comp_Repair_BarCharts', 'Dev_Exterior_Comp_Repair_BarCharts']:\n",
    "        paths_raw = glob.glob(f'WORK_ORDER_ANALYSIS/{directory}/png/*')\n",
    "        paths = clean_paths(paths_raw)\n",
    "        tds_nums = [path.split('/')[-1].split('_')[3].zfill(3) for path in paths]\n",
    "\n",
    "        for pair in list(zip(tds_nums, paths)):\n",
    "            try:\n",
    "                dev_chart_paths[pair[0]][directory] = pair[1]\n",
    "            except:\n",
    "                dev_chart_paths[pair[0]] = {directory: pair[1]}\n",
    "                \n",
    "    return (cons_chart_paths, dev_chart_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_layout(tds, cons_chart_paths, dev_chart_paths, cons_dict):\n",
    "    analysis_image_layout = r''''''\n",
    "    \n",
    "    cons_devs = cons_dict[tds]['developments']\n",
    "    \n",
    "    dev_bar_paths = []\n",
    "    dev_int_paths = []\n",
    "    dev_ext_paths = []\n",
    "    \n",
    "    for dev in cons_devs:\n",
    "        try:\n",
    "            dev_bar_paths.append(dev_chart_paths[dev]['Development_BarCharts'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            dev_int_paths.append(dev_chart_paths[dev]['Dev_Interior_Comp_Repair_BarCharts'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            dev_ext_paths.append(dev_chart_paths[dev]['Dev_Exterior_Comp_Repair_BarCharts'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #Adding consolidation and development bar charts\n",
    "    bar_charts_heading = r'''\\begin{center}\n",
    "                                \\tablehead{\\hspace{1cm}\\\\}\n",
    "                                \\tabletail{\\hspace{1cm}\\\\}\n",
    "                                \\begin{supertabular}{p{0.5\\textwidth}p{0.5\\textwidth}}\n",
    "                                \\shrinkheight{1in}\n",
    "                                \\multicolumn{2}{p{\\textwidth}}{The following bar charts show how frequently various types of maintenance issue -- including compactor-related problems, pest problems, and plumbing issues -- occur in compactor locations consolidation-wide as well as at major developments.} \\\\\n",
    "                                \\multicolumn{2}{c}{\\includegraphics[width=0.6\\textwidth]{\\rootpath/'''+cons_chart_paths[tds]+r'''}} \\\\\n",
    "                                '''\n",
    "    \n",
    "    if len(dev_bar_paths) > 1:\n",
    "        analysis_image_layout += bar_charts_heading\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(dev_bar_paths):\n",
    "            if (len(dev_bar_paths)-i) >= 2:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_bar_paths[i]+r'''} & \\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_bar_paths[i+1]+r'''} \\\\\n",
    "                                        '''\n",
    "            else:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_bar_paths[i]+r'''} &  \\hspace{1cm} \\\\\n",
    "                                        '''\n",
    "            \n",
    "            i += 2\n",
    "        analysis_image_layout += r'\\end{supertabular}'+'\\n'+r'\\end{center}'+'\\n'\n",
    "        \n",
    "    elif len(dev_bar_paths) == 1:\n",
    "        analysis_image_layout += bar_charts_heading\n",
    "        analysis_image_layout += r'''\\multicolumn{2}{c}{\\includegraphics[width=0.6\\textwidth]{\\rootpath/'''+dev_bar_paths[0]+r'''}} \\\\\n",
    "                                    \\end{supertabular}\n",
    "                                    \\end{center}\n",
    "                                    '''\n",
    "    else:\n",
    "        analysis_image_layout += bar_charts_heading.replace(' as well as at major developments','').replace('bar charts show','bar chart shows')\n",
    "        analysis_image_layout += r'\\end{supertabular}'+'\\n'+r'\\end{center}'+'\\n'\n",
    "        \n",
    "    \n",
    "    #Adding interior compactor section, including tables\n",
    "    int_comp_heading = r'''\n",
    "                        \\begin{center}\n",
    "                        \\tablehead{\\hspace{1cm}\\\\}\n",
    "                        \\tabletail{\\hspace{1cm}\\\\}\n",
    "                        \\begin{supertabular}{p{0.5\\textwidth}p{0.5\\textwidth}}\n",
    "                        \\multicolumn{2}{p{\\textwidth}}{The following figures highlight repairs conducted in interior compactor locations at each major development, as well as within up to five buildings at each development.} \\\\\n",
    "                        '''\n",
    "    if len(dev_int_paths) > 1:\n",
    "        analysis_image_layout += int_comp_heading\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(dev_int_paths):\n",
    "            if (len(dev_int_paths)-i) >= 2:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_int_paths[i]+r'''} & \\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_int_paths[i+1]+r'''} \\\\\n",
    "                                        '''\n",
    "            else:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_int_paths[i]+r'''} &  \\hspace{1cm} \\\\\n",
    "                                        '''\n",
    "            i += 2\n",
    "            \n",
    "        analysis_image_layout += r'\\multicolumn{2}{c}{\\input{\\rootpath/WORK_ORDER_ANALYSIS/Dev_Interior_Comp_Repair_Tables/\\tds_repair_table}} \\\\'+'\\n'\n",
    "        analysis_image_layout += r'\\end{supertabular}'+'\\n'+r'\\end{center}'+'\\n'\n",
    "        \n",
    "    elif len(dev_int_paths) == 1:\n",
    "        analysis_image_layout += int_comp_heading\n",
    "        analysis_image_layout += r'''\\multicolumn{2}{c}{\\includegraphics[width=0.6\\textwidth]{\\rootpath/'''+dev_int_paths[0]+r'''}} \\\\\n",
    "                                    \\multicolumn{2}{c}{\\input{\\rootpath/WORK_ORDER_ANALYSIS/Dev_Interior_Comp_Repair_Tables/\\tds_repair_table}} \\\\\n",
    "                                    \\end{supertabular}\n",
    "                                    \\end{center}\n",
    "                                    '''\n",
    "    else:\n",
    "        analysis_image_layout += int_comp_heading.replace('at each major development, as well as within','in').replace('figures highlight','tables highlight')\n",
    "        analysis_image_layout += r'\\multicolumn{2}{c}{\\input{\\rootpath/WORK_ORDER_ANALYSIS/Dev_Interior_Comp_Repair_Tables/\\tds_repair_table}} \\\\'+'\\n'\n",
    "        analysis_image_layout += r'''\\end{supertabular}\n",
    "                                    \\end{center}\n",
    "                                    '''\n",
    "    \n",
    "    #Adding exterior compactor charts\n",
    "    ext_comp_heading = r'''\n",
    "                        \\begin{center}\n",
    "                        \\tablehead{\\hspace{1cm}\\\\}\n",
    "                        \\tabletail{\\hspace{1cm}\\\\}\n",
    "                        \\begin{supertabular}{p{0.5\\textwidth}p{0.5\\textwidth}}\n",
    "                        \\multicolumn{2}{p{\\textwidth}}{The following charts examine repairs made at exterior compactor locations at major developments.} \\\\\n",
    "                        '''\n",
    "    if len(dev_ext_paths) > 1:\n",
    "        analysis_image_layout += ext_comp_heading\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(dev_ext_paths):\n",
    "            if (len(dev_ext_paths)-i) >= 2:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_ext_paths[i]+r'''} & \\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_ext_paths[i+1]+r'''} \\\\\n",
    "                                        '''\n",
    "            else:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_ext_paths[i]+r'''} &  \\hspace{1cm} \\\\\n",
    "                                        '''\n",
    "            \n",
    "            i += 2\n",
    "        analysis_image_layout += r'\\end{supertabular}'+'\\n'+r'\\end{center}'+'\\n'\n",
    "        \n",
    "    elif len(dev_ext_paths) == 1:\n",
    "        analysis_image_layout += ext_comp_heading.replace('charts examine', 'chart examines').replace(' at major developments','')\n",
    "        analysis_image_layout += r'''\\multicolumn{2}{c}{\\includegraphics[width=0.6\\textwidth]{\\rootpath/'''+dev_ext_paths[0]+r'''}} \\\\\n",
    "                                    \\end{supertabular}\n",
    "                                    \\end{center}\n",
    "                                    '''\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    with open(f'WORK_ORDER_ANALYSIS/image_layouts/{tds}_layout.tex', 'w') as file_handle:\n",
    "        file_handle.write(analysis_image_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 raised exception\n",
      "128 raised exception\n",
      "NaN raised exception\n"
     ]
    }
   ],
   "source": [
    "cons_chart_paths, dev_chart_paths = process_analysis_graphic_paths()\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_image_layout(tds, cons_chart_paths, dev_chart_paths, consolidations)\n",
    "    except:\n",
    "        print(f'{tds} raised exception')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Site Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_site_plans():\n",
    "    plan_format = re.compile(r'[A-z0-9\\s]+_[0-9]{3}_[A-z\\s]+.pdf')\n",
    "    site_plan_pdf_candidates = glob.glob('APPENDICES/site_plans/*/*/*.*', recursive=True)\n",
    "    site_plan_pdfs = [path for path in site_plan_pdf_candidates if bool(plan_format.match(path.split('/')[-1]))]\n",
    "\n",
    "    for f in site_plan_pdf_candidates:\n",
    "        if f not in site_plan_pdfs:\n",
    "            os.remove(f)\n",
    "        else:\n",
    "            tds = f.split('/')[-1].split('_')[1]\n",
    "            try:\n",
    "                file = convert_from_path(f, dpi=300, single_file=True)[0]\n",
    "                path = f'APPENDICES/site_plans/{tds}.png'\n",
    "                height = file.height\n",
    "                width = file.width\n",
    "                if file.width > file.height:\n",
    "                    file = file.rotate(90, expand=True)#.resize((height, width))\n",
    "                if file.height > 3300:\n",
    "                    scale_ratio = 3300/float(file.height)\n",
    "                    new_height = 3300\n",
    "                    new_width = int(float(file.width)*scale_ratio)\n",
    "                    file = file.resize((new_width, new_height))\n",
    "                file.save(path, 'PNG')\n",
    "                #print(f\"saved {tds}\")\n",
    "            except:\n",
    "                print(f'{tds} raised exception')\n",
    "                \n",
    "    site_plans = glob.glob('APPENDICES/site_plans/*.png')\n",
    "    dev_list = [path.split('/')[-1].split('.')[0] for path in site_plans]\n",
    "    for cons, content in consolidations.items():\n",
    "        img_list = []\n",
    "        pdf_filename = f'APPENDICES/site_plans/{cons}.pdf'\n",
    "        for dev in content['developments']:\n",
    "            if dev in dev_list:\n",
    "                img_list.append(Image.open(f'APPENDICES/site_plans/{dev}.png'))\n",
    "        if len(img_list)==1:\n",
    "            img_list[0].save(pdf_filename, 'PDF', resolution=300.0)\n",
    "        elif len(img_list)>1:\n",
    "            img_list[0].save(pdf_filename, 'PDF', resolution=300.0, save_all=True, append_images=img_list[1:])\n",
    "        else:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN raised exception\n"
     ]
    }
   ],
   "source": [
    "site_plans = glob.glob('APPENDICES/site_plans/*.png')\n",
    "dev_list = [path.split('/')[-1].split('.')[0] for path in site_plans]\n",
    "for cons, content in consolidations.items():\n",
    "    img_list = []\n",
    "    pdf_filename = f'APPENDICES/site_plans/{cons}.pdf'\n",
    "    try:\n",
    "        for dev in content['developments']:\n",
    "            if dev in dev_list:\n",
    "                img_list.append(Image.open(f'APPENDICES/site_plans/{dev}.png'))\n",
    "    except:\n",
    "        print(f'{cons} raised exception')\n",
    "    if len(img_list)==1:\n",
    "        img_list[0].save(pdf_filename, 'PDF', resolution=300.0)\n",
    "    elif len(img_list)>1:\n",
    "        img_list[0].save(pdf_filename, 'PDF', resolution=300.0, save_all=True, append_images=img_list[1:])\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Floorplans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_plan_paths = glob.glob('APPENDICES/floorplans/*')\n",
    "\n",
    "candidate_list = []\n",
    "for key, value in developments.items():\n",
    "    candidate_list.append(str(value['name']).upper())\n",
    "    for item in value['name_alternates']:\n",
    "        candidate_list.append(item.upper())\n",
    "\n",
    "def get_dev_name(name):\n",
    "    match = process.extractOne(str(name).upper(), candidate_list)[0]\n",
    "    #print(match)\n",
    "    for key, value in developments.items():\n",
    "        if (match.upper() == value['name'].upper()) or (match.upper() in [val.upper() for val in value['name_alternates']]):\n",
    "            return value['name']\n",
    "\n",
    "    return '!!!NOT FOUND'\n",
    "\n",
    "\n",
    "def get_tds_from_name(x):\n",
    "    for key, value in developments.items():\n",
    "        if x == value['name']:\n",
    "            return key\n",
    "        \n",
    "    return 'N/A'\n",
    "\n",
    "floor_plan_names = [get_dev_name(path.split('/')[-1].replace('.pdf','')) for path in floor_plan_paths]\n",
    "floor_plan_tds = [get_tds_from_name(name) for name in floor_plan_names]\n",
    "\n",
    "floor_plans = pd.DataFrame(data=list(zip(floor_plan_tds, floor_plan_names, floor_plan_paths)), columns=['TDS', 'NAME', 'PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "floor_plans.to_csv('APPENDICES/floorplans/floor_plans_for_screening.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making and Compiling LaTeX Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdfjam --nup 2x1 --openright 'true' --frame 'true' REPORTS/tds_report.pdf --outfile tds_spread.pdf --landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 raised exception: <class 'subprocess.CalledProcessError'>\n",
      "210 raised exception: <class 'subprocess.CalledProcessError'>\n",
      "128 raised exception: <class 'subprocess.CalledProcessError'>\n",
      "NaN raised exception: <class 'KeyError'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
