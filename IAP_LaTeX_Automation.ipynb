{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaTeX Automation for NYCHA Waste Individual Action Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import subprocess\n",
    "import shutil\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Global Vars and Options\n",
    "os.chdir('/Users/kyleslugg/Documents/NYCHA/Production')\n",
    "cons_tds = '073'\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standardized_names():\n",
    "    databook_names = pd.read_csv('DATA/name_tables/dev_data_book_name.csv')\n",
    "    databook_names['CONS_TDS'] = databook_names['CONS_TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    databook_names['TDS'] = databook_names['TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "\n",
    "    staff_names = pd.read_csv('DATA/name_tables/staff_cons_name.csv')\n",
    "    staff_names['RC_Name'] = staff_names['RC Name']\n",
    "    \n",
    "    consolidations = {}\n",
    "    developments = {}\n",
    "\n",
    "    for row in databook_names.itertuples():\n",
    "        consolidations[row.CONS_TDS] = {'name':row.CONS_NAME, 'alternates':[row.MANAGED_BY]}\n",
    "        developments[row.TDS] = {'name':row.DEV_NAME, 'name_alternates':[], 'cons_tds':row.CONS_TDS}\n",
    "        \n",
    "    def find_closest_fuzzy_match(name, comp_df, comp_col_name, return_col_name):\n",
    "        values = comp_df[comp_col_name].unique()\n",
    "        comp_df_copy = pd.DataFrame(data=values, index=[i for i in range(0,len(values))], columns=[comp_col_name])\n",
    "\n",
    "        '''\n",
    "        def strip_name(x):\n",
    "            string = str(x).lower()\n",
    "            string = string.replace('consolidated','')\n",
    "            string = string.replace('consolidation', '')\n",
    "            string = string.replace('houses', '')\n",
    "            return string\n",
    "\n",
    "        comp_df_copy['partial_ratio'] = comp_df_copy[comp_col_name].apply(lambda x: fuzz.partial_ratio(strip_name(name), strip_name(x)))\n",
    "        highest_match = comp_df_copy['partial_ratio'].max()\n",
    "\n",
    "        matches = comp_df_copy.loc[comp_df_copy['partial_ratio']==highest_match, 'CONS_NAME']\n",
    "\n",
    "        if matches.shape[0] == 1:\n",
    "            return matches.iloc[0]\n",
    "        else:\n",
    "            print(matches)\n",
    "            return 'ZZZ MULTIPLE MATCHES FOUND'\n",
    "\n",
    "        '''\n",
    "        return process.extractOne(str(name).lower(), values.tolist())[0]\n",
    "    \n",
    "    staff_names['NAME_MATCH'] = staff_names['RC Name'].apply(lambda x: find_closest_fuzzy_match(x, databook_names, 'CONS_NAME', 'CONS_NAME'))\n",
    "\n",
    "    match_corrections = {'Justice Sonia Sotomayor  Consolidated': 'SOTOMAYOR HOUSES CONSOLIDATED',\n",
    "                        'Murphy Consolidated': 'ZZZUNKNOWN'\n",
    "                        }\n",
    "\n",
    "    def make_corrections(row, index_col, data_col, dictionary):\n",
    "        if str(row[index_col]).strip() in dictionary.keys():\n",
    "            return dictionary[row[index_col]]\n",
    "        else:\n",
    "            return row[data_col]\n",
    "\n",
    "    staff_names['AMENDED_MATCHES'] = staff_names.apply(lambda row: make_corrections(row, 'RC Name', 'NAME_MATCH', match_corrections), axis=1)\n",
    "\n",
    "    staff_names = staff_names.merge(databook_names[['CONS_NAME', 'CONS_TDS']], left_on='AMENDED_MATCHES', right_on='CONS_NAME', how='left')\n",
    "\n",
    "    for row in staff_names.itertuples():\n",
    "        try:\n",
    "            consolidations[row.CONS_TDS]['alternates'].append(row.RC_Name)\n",
    "        except:\n",
    "            print(f'TDS #{row.CONS_TDS} raised an exception.')\n",
    "    \n",
    "    return(consolidations, developments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDS #nan raised an exception.\n"
     ]
    }
   ],
   "source": [
    "consolidations, developments = get_standardized_names()\n",
    "\n",
    "counts ={}\n",
    "for key, value in developments.items():\n",
    "    if value['cons_tds'] not in counts.keys():\n",
    "        counts[value['cons_tds']] = {'developments':[key],\n",
    "                             'count':1}\n",
    "    else:\n",
    "        counts[value['cons_tds']]['developments'].append(key)\n",
    "        counts[value['cons_tds']]['count']+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['167', '359', '091', '530', '127']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_list = [value['count'] for key, value in counts.items()]\n",
    "high_count_cons = [key for key, value in counts.items() if value['count']>=8]\n",
    "high_count_cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_overview_data():#Load Data\n",
    "    overview_data = pd.read_csv('DATA/overview_table_data.csv')\n",
    "    overview_data['CONS_TDS'] = overview_data['CONS_TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    overview_data['TDS'] = overview_data['TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    \n",
    "    return overview_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_data = load_overview_data()\n",
    "cons_list = overview_data['CONS_TDS'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and Process Text Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO COME:\n",
    "- Waste Services and Assets\n",
    "- Waste Distribution (top text -- bottom accounted for below)\n",
    "- What Is an IAP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lightly modified version of example found at http://etienned.github.io/posts/extract-text-from-word-docx-simply/\n",
    "\n",
    "try:\n",
    "    from xml.etree.cElementTree import XML\n",
    "except ImportError:\n",
    "    from xml.etree.ElementTree import XML\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Module that extract text from MS XML Word document (.docx).\n",
    "(Inspired by python-docx <https://github.com/mikemaccana/python-docx>)\n",
    "\"\"\"\n",
    "\n",
    "WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "PARA = WORD_NAMESPACE + 'p'\n",
    "TEXT = WORD_NAMESPACE + 't'\n",
    "\n",
    "\n",
    "def get_docx_text(path):\n",
    "    \"\"\"\n",
    "    Take the path of a docx file as argument, return the text in unicode.\n",
    "    \"\"\"\n",
    "    document = zipfile.ZipFile(path)\n",
    "    try:\n",
    "        xml_content = document.read('word/document.xml')\n",
    "    except:\n",
    "        xml_content = document.read('word/document2.xml')\n",
    "        \n",
    "    document.close()\n",
    "    tree = XML(xml_content)\n",
    "\n",
    "    paragraphs = []\n",
    "    for paragraph in tree.getiterator(PARA):\n",
    "        texts = [node.text\n",
    "                 for node in paragraph.getiterator(TEXT)\n",
    "                 if node.text]\n",
    "        if texts:\n",
    "            paragraphs.append(''.join(texts))\n",
    "\n",
    "    return '\\n\\n'.join(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character Substitutions for LaTeX -- set and define \"clean\" method\n",
    "def clean_text(text):\n",
    "    substitutions = {'“':\"``\",\n",
    "                '”': \"''\",\n",
    "                '’':\"'\",\n",
    "                ' ':' ',\n",
    "                '–':'--',\n",
    "                ' ':' ',\n",
    "                '\\xa0':' '}\n",
    "    \n",
    "    for key, value in substitutions.items():\n",
    "        text = text.replace(key, value)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overview_text(cons_tds):\n",
    "    header = re.compile(r'((\\w*\\s)*(Overview)):?')\n",
    "    \n",
    "    overview_text = get_docx_text(f'TEXT/overview_text/{cons_tds}_Overview.docx')\n",
    "    if len(header.findall(overview_text)) == 0:\n",
    "            overview_text = overview_text\n",
    "    else:\n",
    "        try:\n",
    "            overview_text = overview_text.replace(header.findall(overview_text)[0]+'\\n','')\n",
    "        except:\n",
    "            overview_text = overview_text.replace(header.findall(overview_text)[0][0]+'\\n','')\n",
    "    \n",
    "    overview_text = clean_text(overview_text)\n",
    "    \n",
    "    with open(f'TEXT/overview_text/{cons_tds}_overview.tex', 'w') as file_handle:\n",
    "        file_handle.write(overview_text)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "073 raised error\n",
      "031 raised error\n",
      "210 raised error\n",
      "016 raised error\n",
      "128 raised error\n"
     ]
    }
   ],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_overview_text(tds)\n",
    "    #except KeyError:\n",
    "        #pass\n",
    "    #except FileNotFoundError:\n",
    "        #pass\n",
    "    except:\n",
    "        print(f\"{tds} raised error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_analysis_text(cons_tds):\n",
    "    analysis_text = get_docx_text(f'TEXT/analysis_text/{cons_tds}_Analysis.docx')\n",
    "\n",
    "    header = re.compile(r'([\\w\\s]*:)')\n",
    "    \n",
    "    analysis_text = clean_text(analysis_text)\n",
    "\n",
    "    section_headings = {'Inspection and Collection Requirement':['Inspection and Collection Requirements',\n",
    "                                                                 'Inspection and Collection Requirement',\n",
    "                                                                 'Collection and Inspection Requirements',\n",
    "                                                                'Collection and Inspection Requirement'],\n",
    "                        'Removal or Storage Requirement':['Removal or Storage Requirements',\n",
    "                                                          'Removal or Storage Requirement',\n",
    "                                                          'Removal and Storage Requirements',\n",
    "                                                         'Removal and Storage Requirement',\n",
    "                                                         'Storage or Removal Requirement',\n",
    "                                                          'Storage and Removal Requirements',\n",
    "                                                         'Storage and Removal Requirement',\n",
    "                                                         'Removal or Storage Requirement '],\n",
    "                       'Additional Context':['Additional Context']}\n",
    "    \n",
    "    for heading, variants in section_headings.items():\n",
    "        for variant in variants:\n",
    "            if variant in analysis_text:\n",
    "                analysis_text = analysis_text.replace(variant, r'\\textbf{%s}' % (heading))\n",
    "                break\n",
    "\n",
    "    if len(header.findall(analysis_text)) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        analysis_text = analysis_text.replace(header.findall(analysis_text)[0]+'\\n','')\n",
    "\n",
    "    latex_block = analysis_text\n",
    "\n",
    "    with open(f'TEXT/analysis_text/{cons_tds}_analysis.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 raised error\n",
      "097 raised error\n",
      "359 raised error\n",
      "073 raised error\n",
      "153 raised error\n",
      "210 raised error\n",
      "091 raised error\n",
      "165 raised error\n",
      "337 raised error\n",
      "056 raised error\n",
      "065 raised error\n",
      "252 raised error\n",
      "016 raised error\n",
      "530 raised error\n",
      "086 raised error\n",
      "113 raised error\n",
      "075 raised error\n",
      "134 raised error\n",
      "170 raised error\n",
      "351 raised error\n",
      "127 raised error\n",
      "041 raised error\n",
      "169 raised error\n",
      "044 raised error\n",
      "341 raised error\n",
      "010 raised error\n",
      "081 raised error\n",
      "021 raised error\n",
      "083 raised error\n",
      "112 raised error\n",
      "093 raised error\n",
      "035 raised error\n",
      "005 raised error\n",
      "055 raised error\n",
      "377 raised error\n",
      "128 raised error\n"
     ]
    }
   ],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_analysis_text(tds)\n",
    "    #except FileNotFoundError:\n",
    "        #pass\n",
    "    except:\n",
    "        print(f\"{tds} raised error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''By understanding how much waste is generated at each consolidation, planners and managers\n",
    "can better determine how well current assets and services serve current needs, and what additional \n",
    "elements are necessary in order for each consolidation to operate as efficiently as possible. \n",
    "\n",
    "%s has (%s) 30-CY external compactors, amounting to %s ft2 of storage space for garbage. \n",
    "Given the rate at which waste is produced at NYCHA properties, these containers will fill\n",
    "up in about (3) days at the Sumner Consolidation. The average weight of the containers at\n",
    "capacity should be about (3) tons.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and Prepare Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set asset map path\n",
    "asset_map_path = f\"MAPS/asset_maps/{cons_tds}_asset_map.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split context map into two pages\n",
    "def process_context_map(cons_tds):\n",
    "    image = Image.open(f'MAPS/context_maps/{cons_tds}_context_map.png')\n",
    "    width, height = image.size\n",
    "\n",
    "    bb1 = (0,0,width/2,height)\n",
    "    bb2 = (width/2, 0, width, height)\n",
    "\n",
    "    img_1 = image.crop(bb1)\n",
    "    img_2 = image.crop(bb2)\n",
    "\n",
    "    img_1.save(f'MAPS/context_maps/{cons_tds}_context_1.png', format=\"PNG\")\n",
    "    img_2.save(f'MAPS/context_maps/{cons_tds}_context_2.png', format=\"PNG\")\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        process_context_map(tds)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO COME:\n",
    "- Consolidation Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Overview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overview_table(cons_tds, overview_data=overview_data):\n",
    "    \n",
    "    cons_data = overview_data.loc[overview_data['CONS_TDS']== cons_tds]\n",
    "    \n",
    "    overview_table = ''\n",
    "\n",
    "    overview_frame = r'''\n",
    "    \\resizebox{\\textwidth}{!}{\n",
    "    \\begin{tabular}{l|c|c|c|c|}\n",
    "    \\cline{2-5}\n",
    "                                                                           & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} TDS \\#} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Total Households} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Official Population} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Average Family Size} \\\\ \\hline\n",
    "\n",
    "    '''\n",
    "\n",
    "    development_template = r'''\\multicolumn{1}{|l|}{\\cellcolor{ccteallight}%s}        & %s                                                   & %s                                                           & %s                                                                & %s                                                                \\\\ \\hline'''\n",
    "\n",
    "\n",
    "    overview_table += overview_frame\n",
    "\n",
    "    for row in cons_data.itertuples():\n",
    "        dev_name = row.DEV_NAME.title()\n",
    "        dev_tds = row.TDS\n",
    "        total_hhs = row.TOTAL_HH\n",
    "        official_population = row.TOTAL_POP\n",
    "        avg_family_size = row.AVG_FAMILY_SIZE\n",
    "\n",
    "        overview_table += development_template % (dev_name, dev_tds, total_hhs, official_population, avg_family_size)\n",
    "\n",
    "    overview_table += r'''\n",
    "    \\end{tabular}\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    with open(f'TABLES/overview_table/{cons_tds}_overview_table.tex', 'w') as file_handle:\n",
    "        file_handle.write(overview_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tds in consolidations.keys():\n",
    "    make_overview_table(tds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typology Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_typology_data():\n",
    "    #Cleaning and Shaping Data\n",
    "    typ_1 = pd.read_csv('DATA/typologies_1.csv')\n",
    "    typ_2 = pd.read_csv('DATA/typologies_2.csv')\n",
    "\n",
    "    typ_1.columns = ['CONS_NAME', 'DEV_NAME', 'TDS', 'TYPOLOGY']\n",
    "    typ_2.columns = ['CONS_NAME', 'CONS_TDS', 'DEV_NAME', 'TDS', 'METHOD', \n",
    "                     'CONSTRUCTION_DATE', 'BLDG_AGE', 'STORIES', 'BLDG_COVERAGE_SQFT', 'OPEN_SPACE_RATIO', 'SCATTERED_SITE_FLAG']\n",
    "\n",
    "    def make_dates(date_col):\n",
    "        date = str(date_col).split('/')\n",
    "        try:\n",
    "            if int(date[2]) > 18:\n",
    "                return datetime.date(int(f'19{date[2]}'), int(date[0]), int(date[1]))\n",
    "            else:\n",
    "                return datetime.date(int(f'20{date[2]}'), int(date[0]), int(date[1]))\n",
    "        except IndexError:\n",
    "            return datetime.date(1900,1,1)\n",
    "\n",
    "    typ_2['CONSTRUCTION_DATE'] = typ_2['CONSTRUCTION_DATE'].apply(lambda x: make_dates(x))\n",
    "    typ_2['SCATTERED_SITE_FLAG'] = typ_2['SCATTERED_SITE_FLAG'].apply(lambda x: x == 'YES')\n",
    "    typ_2.loc[typ_2['SCATTERED_SITE_FLAG']=='YES','SCATTERED_SITE_FLAG'] = 1\n",
    "\n",
    "    typology = typ_1.merge(typ_2[['CONS_TDS', 'TDS', 'METHOD',\n",
    "                                 'CONSTRUCTION_DATE', 'BLDG_AGE', \n",
    "                                 'STORIES', 'BLDG_COVERAGE_SQFT', \n",
    "                                 'OPEN_SPACE_RATIO', 'SCATTERED_SITE_FLAG']], how='left', on='TDS')\n",
    "\n",
    "    typology['CONS_TDS'] = typology['CONS_TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    typology['PREWAR'] = typology['CONSTRUCTION_DATE'].apply(lambda x: x < datetime.date(1945,1,1))\n",
    "\n",
    "    #Adding Typology Icons\n",
    "\n",
    "    typ_icons = [r'\\rootpath/IMAGES/typology_earlytower.png', r'\\rootpath/IMAGES/typology_towerpark.png', r'\\rootpath/IMAGES/typology_prewar.png', r'\\rootpath/IMAGES/typology_scatteredsite.png']\n",
    "    typ_dict = {}\n",
    "    [typ_dict.setdefault(key, '') for key in typology['TYPOLOGY'].unique().tolist()]\n",
    "\n",
    "    typ_dict['1 - High-rise in the park'] = typ_icons[1]\n",
    "    typ_dict['2 - Mid-rise in the park'] = typ_icons[1]\n",
    "    typ_dict['3 - Low-rise in the park'] = typ_icons[0]\n",
    "    typ_dict['4 - Context Towers'] = typ_icons[3]\n",
    "    typ_dict['5 - Context Mid-rises'] = typ_icons[2]\n",
    "    typ_dict['6 - Walkups & Brownstones'] = typ_icons[2]\n",
    "\n",
    "    typ_header = re.compile(r'\\d\\s-\\s')\n",
    "\n",
    "    typology['TYP_NAME'] = typology['TYPOLOGY'].apply(lambda x: typ_header.sub('', str(x)))\n",
    "    typology['IMAGE_PATH'] = typology['TYPOLOGY'].apply(lambda x: typ_dict[x])\n",
    "    \n",
    "    return typology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "typology = load_typology_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_typology_table_block(cons_tds, typ_data):\n",
    "    cons_data = typ_data[typ_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "    \n",
    "    if num_devs < 5:\n",
    "        block_1 = cons_data\n",
    "    \n",
    "    elif num_devs >=5 and num_devs < 7:\n",
    "        block_1 = cons_data.iloc[0:3]\n",
    "        block_2 = cons_data.iloc[3:]\n",
    "    \n",
    "    else:\n",
    "        block_1 = cons_data.iloc[0:4]\n",
    "        block_2 = cons_data.iloc[4:]\n",
    "    \n",
    "    len_1 = block_1.shape[0]\n",
    "    \n",
    "    try:\n",
    "        len_2 = block_2.shape[0]\n",
    "    except:\n",
    "        len_2 = 0\n",
    "    \n",
    "    headers = {1:r\"\\begin{tabular}{m{1.5in} m{2in}\"+'\\n',\n",
    "              2:r\"\\begin{tabular}{m{1.25in} m{2in} m{.1in} m{1.25in} m{2in}}\"+'\\n',\n",
    "              3:r\"\\begin{tabular}{m{1.25in} m{1.5in} m{.2in} m{1.25in} m{1.5in} m{.2in} m{1.25in} m{1.5in}}\"+'\\n',\n",
    "              4:r\"\\begin{tabular}{m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in}}\"+'\\n'}\n",
    "         \n",
    "    lines = {1:r'''\\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            2:r'''\\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            3:r'''\\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            4:r'''\\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}& & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}'''+'\\n'+r'\\end{tabular}'}\n",
    "    \n",
    "    \n",
    "    data_1 = []\n",
    "    data_2 = []\n",
    "    \n",
    "    for row in block_1.itertuples():\n",
    "        data_1.append(str(row.DEV_NAME).title())\n",
    "        data_1.append(str(row.TYP_NAME).replace('&', '\\&'))\n",
    "        data_1.append(row.IMAGE_PATH)\n",
    "    \n",
    "    if len_2 > 0:\n",
    "        for row in block_2.itertuples():\n",
    "            data_2.append(str(row.DEV_NAME.title()))\n",
    "            data_2.append(str(row.TYP_NAME).replace('&', '\\&'))\n",
    "            data_2.append(row.IMAGE_PATH)\n",
    "    \n",
    "    # Assembling Nested Tables\n",
    "    latex_block = ''\n",
    "    latex_block += r'''\\begin{table}[H]\n",
    "    \\resizebox{\\textwidth}{!}{\n",
    "    \\begin{tabular}{c}\n",
    "    '''\n",
    "    \n",
    "    latex_block += headers[len_1]\n",
    "    latex_block += lines[len_1] % tuple(data_1)\n",
    "    \n",
    "    if len_2 > 0:\n",
    "        latex_block += r'''\\\\\n",
    "        '''\n",
    "        latex_block += headers[len_2]\n",
    "        latex_block += lines[len_2] % tuple(data_2)\n",
    "    \n",
    "    latex_block += r'''\\end{tabular}}\n",
    "    \\end{table}'''\n",
    "    \n",
    "    with open(f'TABLES/typology_table/{cons_tds}_typology.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 raised an exception.\n",
      "091 raised an exception.\n",
      "128 raised an exception.\n"
     ]
    }
   ],
   "source": [
    "typology = load_typology_data()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_typology_table_block(tds, typology)\n",
    "    except:\n",
    "        print(f'{tds} raised an exception.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_cons_tds = {'091':'Baisley Park. Isolate important developments.',\n",
    "                   '359':'Skip for now.'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waste Services and Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wsa_data():\n",
    "    wsa_data = pd.read_csv('DATA/WASTE_SERVICES_ASSETS.csv')\n",
    "    wsa_data['TDS'] = wsa_data['DEV_TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    wsa_data['INT_COMP_DATE'] = pd.to_datetime(wsa_data['INT_COMP_INSTALL_DATE'], errors='ignore')\n",
    "    \n",
    "    return wsa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_waste_services_table(cons_tds, wsa_data, counts_dict=counts):\n",
    "    dev_list = counts_dict[cons_tds]['developments']\n",
    "    cons_data = wsa_data.query(f\"TDS in {dev_list}\")\n",
    "    num_devs = counts_dict[cons_tds]['count']\n",
    "    \n",
    "    def make_waste_services_block(num_cols, block_data):\n",
    "    \n",
    "        col_format = r'X|'\n",
    "        header = r'\\begin{tabularx}{\\textwidth}{V{1.5in}|'+col_format*(num_cols)+r'''}\n",
    "    \\cline{2-4}\n",
    "                                                                                       '''+r'& \\cellcolor{ccorange}{\\color[HTML]{FFFFFF} %s}'*num_cols+r' \\\\ \\hline'+'\\n'\n",
    "        hh_waste_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}Household Waste (DSNY)}               '+r'& %s'*num_cols+r'\\\\ \\hline'+'\\n'\n",
    "        bulk_waste_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}Bulk Waste}                  '+r'& %s'*num_cols+r' \\\\ \\hline'+'\\n'\n",
    "        norm_recycling_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                   '+r'& DSNY Curb Setout'*num_cols + r'\\\\ \\hline'+'\\n'\n",
    "        special_recycling_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                   '+r'& %s'*num_cols +r'\\\\ \\hline' + '\\n'\n",
    "        \n",
    "        latex_block = r''''''\n",
    "        latex_block += header % tuple(block_data['DEV_NAME'].apply(lambda x: str(x).title()).tolist())\n",
    "        \n",
    "        \n",
    "        hh_waste_data = []\n",
    "        bulk_waste_data = []\n",
    "        ewaste_data = []\n",
    "        textiles_data = []\n",
    "        \n",
    "        for dev in block_data.itertuples():\n",
    "            if dev.CURBSIDE == 1:\n",
    "                hh_waste_data.append('Curbside Pickup')\n",
    "            elif dev.SHARE == 1:\n",
    "                hh_waste_data.append(f'Transfer to {str(dev.SHARE_SITE).title()}')\n",
    "            else:\n",
    "                if (dev.EXT_COMP_BE == 1) and (dev.COMPACTOR_YARDS == 1):\n",
    "                    hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactor in {int(dev.COMPACTOR_YARDS)} waste yard')\n",
    "                elif (dev.COMPACTOR_YARDS == 1):\n",
    "                    hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yard')\n",
    "                else:\n",
    "                    hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yards')\n",
    "                \n",
    "            if pd.isna(dev.BULK_HAULER):\n",
    "                if int(dev.BULK_SITES) == 0:\n",
    "                    bulk_waste_data.append(\"Transferred for Pickup\")\n",
    "                elif int(dev.BULK_SITES) == 1:\n",
    "                    bulk_waste_data.append(\"One Bulk Drop Site; Transferred for Pickup\")\n",
    "                else:\n",
    "                    bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Drop Sites; Transferred for Pickup\")\n",
    "            else:\n",
    "                if int(dev.BULK_SITES) == 1:\n",
    "                    bulk_waste_data.append(f\"One Bulk Drop Site; Picked up by {dev.BULK_HAULER}\")\n",
    "                elif int(dev.BULK_SITES) > 1:\n",
    "                    bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Drop Sites; Picked up by {dev.BULK_HAULER}\")\n",
    "                else:\n",
    "                    bulk_waste_data.append(f\"Picked up by {dev.BULK_HAULER}\")\n",
    "            \n",
    "            if dev.ECYCLE == 1:\n",
    "                ewaste_data.append('Previously available through ECycle')\n",
    "            else:\n",
    "                ewaste_data.append('N/A')\n",
    "            \n",
    "            if dev.REFASHION == 1:\n",
    "                textiles_data.append('Previously available through Refashion')\n",
    "            else:\n",
    "                textiles_data.append('N/A')\n",
    "        \n",
    "        latex_block += hh_waste_line % tuple(hh_waste_data)\n",
    "        latex_block += bulk_waste_line % tuple(bulk_waste_data)\n",
    "        latex_block += norm_recycling_line % 'Recycling: Paper and Cardboard'\n",
    "        latex_block += norm_recycling_line % 'Recycling: Metal, Glass, and Plastic'\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: Mattresses']+['N/A' for i in range(0, num_cols)])\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: E-Waste']+ewaste_data)\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: Textiles']+textiles_data)\n",
    "        latex_block += r'\\end{tabularx}'\n",
    "        \n",
    "        return latex_block\n",
    "    \n",
    "    if num_devs <= 4:\n",
    "        num_cols = num_devs\n",
    "        block_data = cons_data\n",
    "        \n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols, block_data))\n",
    "        \n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)\n",
    "        block_data_1 = cons_data.iloc[0:num_cols_1]\n",
    "        block_data_2 = cons_data.iloc[num_cols_1:]\n",
    "        \n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services_1.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols_1, block_data_1))\n",
    "            \n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services_2.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols_2, block_data_2))\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa_data = load_wsa_data()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    make_waste_services_table(tds, wsa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_waste_assets_table(cons_tds, wsa_data, counts_dict=counts):\n",
    "    dev_list = counts_dict[cons_tds]['developments']\n",
    "    cons_data = wsa_data.query(f\"TDS in {dev_list}\")\n",
    "    num_devs = counts_dict[cons_tds]['count']\n",
    "    \n",
    "    header = r'''\n",
    "    \\begin{tabular}{V{.25\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.25\\columnwidth}|V{.15\\columnwidth}|}\n",
    "\\cline{2-5}\n",
    "                                                                                              & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Internal Compactors} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} External Compactors} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Other External Assets}   & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Recycling Bins\\tnote{1}} \\\\ \\hline'''+'\\n'\n",
    "    line_format = r'\\multicolumn{1}{|V{.25\\columnwidth}|}{\\cellcolor{ccorange}{\\color[HTML]{FFFFFF} %s}}        & %s                                                & %s                                                                  & %s & %s                                                            \\\\ \\hline'+'\\n'\n",
    "    \n",
    "    latex_block = r''''''\n",
    "    latex_block += header\n",
    "    \n",
    "    for dev in cons_data.itertuples():\n",
    "        line_data = []\n",
    "        line_data.append(str(dev.DEV_NAME).title())\n",
    "        \n",
    "        if (dev.INT_COMP == 0):\n",
    "            int_comp_string = '0'\n",
    "        elif pd.isna(dev.INT_COMP_DATE):\n",
    "            int_comp_string = str(int(dev.INT_COMP))\n",
    "        else:\n",
    "            int_comp_string = f'{str(int(dev.INT_COMP))}; last replaced {str(dev.INT_COMP_DATE.year)}'\n",
    "        \n",
    "        line_data.append(int_comp_string)\n",
    "        line_data.append(str(int(dev.EXT_COMP_BE)))\n",
    "        \n",
    "        #if (dev.BULK_CRUSHERS == 0) and (dev.BALERS == 0)... REDO THIS ONCE DATA ARE COMPLETE\n",
    "        line_data.append('PLACEHOLDER UNTIL DATA ARE COMPLETE')\n",
    "        line_data.append(str(int(dev.RECYCLING_BINS)))\n",
    "        \n",
    "        latex_block += line_format % tuple(line_data)\n",
    "    \n",
    "    latex_block += r'\\end{tabular}'\n",
    "    \n",
    "    with open(f'TABLES/waste_assets/{cons_tds}_waste_assets.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa_data = load_wsa_data()\n",
    "for tds in consolidations.keys():\n",
    "    make_waste_assets_table(tds, wsa_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waste Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_waste_cols(overview_data):\n",
    "    conversion_factors = {'units_to_tons_day': 0.0025,\n",
    "                         'cy_per_ton': {'trash': 21.05,\n",
    "                                        'trash_actual': 0,\n",
    "                                       'MGP': 18.02,\n",
    "                                       'cardboard': 26.67,\n",
    "                                       'paper': 6.19,\n",
    "                                       'organics': 4.32,\n",
    "                                       'ewaste': 5.65,\n",
    "                                       'textiles': 13.33},\n",
    "                         'gallons_per_cy': 201.974,\n",
    "                         'gallons_per_64gal': 64,\n",
    "                         'gallons_per_40lb_bag': 44,\n",
    "                         'cy_per_44gal_bag':0.174,\n",
    "                         'cy_per_cardboard_bale':0.193}\n",
    "\n",
    "    waste_percentages = {'trash': .26,\n",
    "                         'trash_actual':.894,\n",
    "                        'MGP': .19,\n",
    "                        'cardboard': .07,\n",
    "                        'paper': .07,\n",
    "                        'organics':.32,\n",
    "                        'ewaste': .01,\n",
    "                        'textiles': .08}\n",
    "\n",
    "    capture_rates = {'trash_primary': .75,\n",
    "                    'trash_secondary': .25,\n",
    "                    'mgp': .30,\n",
    "                    'cardboard': .50,\n",
    "                    'paper': .20}\n",
    "\n",
    "    overview_data['WASTE_TONS_DAY'] = overview_data['CURRENT_APTS'].apply(lambda x: x * conversion_factors['units_to_tons_day'])\n",
    "\n",
    "    for key, value in waste_percentages.items():\n",
    "        overview_data[f'{key.upper()}_CY'] = overview_data['WASTE_TONS_DAY'].apply(lambda x: x * value * conversion_factors['cy_per_ton'][key])\n",
    "        overview_data[f'{key.upper()}_TONS'] = overview_data['WASTE_TONS_DAY'].apply(lambda x: x * value)\n",
    "    \n",
    "    overview_data['TRASH_ACTUAL_CY'] = (overview_data['TRASH_CY']+\n",
    "                                           overview_data['MGP_CY']+\n",
    "                                           overview_data['CARDBOARD_CY']+\n",
    "                                           overview_data['PAPER_CY']+\n",
    "                                           overview_data['ORGANICS_CY']+\n",
    "                                           overview_data['EWASTE_CY']+\n",
    "                                           overview_data['TEXTILES_CY'])-(overview_data['MGP_CY']*capture_rates['mgp']+\n",
    "                                                                         overview_data['CARDBOARD_CY']*capture_rates['cardboard']+\n",
    "                                                                         overview_data['PAPER_CY']*capture_rates['paper'])\n",
    "\n",
    "    overview_data['TRASH_CHUTE_CY'] = overview_data['TRASH_ACTUAL_CY']*capture_rates['trash_primary']\n",
    "    overview_data['TRASH_CHUTE_TONS'] = overview_data['TRASH_ACTUAL_TONS']*capture_rates['trash_primary']\n",
    "    overview_data['TRASH_CHUTE_SAUSAGE'] = ((overview_data['TRASH_CHUTE_CY'])/conversion_factors['cy_per_ton']['trash'])*(2000/40)\n",
    "    overview_data['TRASH_DROP_CY'] = overview_data['TRASH_ACTUAL_CY']*capture_rates['trash_secondary']\n",
    "    overview_data['TRASH_DROP_TONS'] = overview_data['TRASH_ACTUAL_TONS']*capture_rates['trash_secondary']\n",
    "    overview_data['TRASH_DROP_BINS'] = overview_data['TRASH_DROP_CY']*conversion_factors['gallons_per_cy']/64\n",
    "    overview_data['CAPTURED_MGP_TONS_WEEK'] = overview_data['MGP_TONS']*capture_rates['mgp']*7\n",
    "    overview_data['CAPTURED_CARDBOARD_TONS_WEEK'] = overview_data['CARDBOARD_TONS']*capture_rates['cardboard']*7\n",
    "    overview_data['CAPTURED_PAPER_TONS_WEEK'] = overview_data['PAPER_TONS']*capture_rates['paper']*7\n",
    "    overview_data['MGP_BAGS_WEEK'] = overview_data['MGP_CY']*capture_rates['mgp']*7/conversion_factors['cy_per_44gal_bag']\n",
    "    overview_data['PAPER_BAGS_WEEK'] = overview_data['PAPER_CY']*capture_rates['paper']*7/conversion_factors['cy_per_44gal_bag']\n",
    "    overview_data['CARDBOARD_BALES_WEEK'] = overview_data['CARDBOARD_CY']*capture_rates['cardboard']*7/conversion_factors['cy_per_cardboard_bale']\n",
    "    \n",
    "    return overview_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_waste_distribution_table(cons_tds, overview_data):\n",
    "    cons_data = overview_data[overview_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "    \n",
    "    if num_devs == 1:\n",
    "        num_cols = num_devs\n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)+1\n",
    "    else:\n",
    "        num_cols = num_devs+1\n",
    "    \n",
    "    if num_devs != 1:\n",
    "        cons_data.loc['Total']= cons_data.sum(numeric_only=True, axis=0)\n",
    "        cons_data.loc['Total','DEV_NAME'] = 'Total'\n",
    "    \n",
    "    def make_waste_distribution_table_block(cons_data, num_cols):\n",
    "        dev_col_format = r'X|'\n",
    "\n",
    "        opening = r'''\n",
    "        \\begin{tabularx}{\\textwidth}{V{1.5in}|%s}\n",
    "        \\cline{2-%s}\n",
    "        ''' % (dev_col_format*num_cols, (num_cols+1))\n",
    "\n",
    "        top_row = r'''\n",
    "                                                                       '''+(r\"& \\multicolumn{1}{V{1.5in}|}{\\cellcolor{ccorange}%s}\"*(num_cols))+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        standard_row = r\"\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                 \"+(r\"& %s                                    \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        captured_row = r\"\\multicolumn{1}{|Y{1.5in}|}{\\cellcolor{ccorangelight}Captured / Week (tons)\\tnote{4}}                        \"+(r\"& %s                                    \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        chute_row = r\"\\multicolumn{1}{|Y{1.5in}|}{\\cellcolor{ccorangelight}Trash Chutes\\tnote{2}}                 \"+(r\"& %s tons or (%s) 40 lbs. sausage bags      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        dropsite_row = r\"\\multicolumn{1}{|Y{1.5in}|}{\\cellcolor{ccorangelight}Drop Sites\\tnote{3}}                 \"+(r\"& %s tons or (%s) 64-gallon bins      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        OET_row = r\"\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s / Day (CY)\\tnote{5}}              \"+(r\"& %s                                    \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        recycling_row = r\"\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                 \"+(r\"& %s tons or (%s) 44-gallon bags                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        cardboard_row = r\"\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                 \"+(r\"& %s tons or (%s) bales                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "\n",
    "        def make_trash_text(row, text_var, cy_col, other_col):\n",
    "            text_var.append(round(row[cy_col],2))\n",
    "            text_var.append(round(row[other_col], 2))\n",
    "            pass\n",
    "\n",
    "        latex_block = opening\n",
    "        latex_block += top_row % tuple(cons_data['DEV_NAME'].apply(lambda x: str(x).title()).tolist())\n",
    "        latex_block += standard_row % tuple([r\"Waste Generated / Day (Tons)\\tnote{1}\"]+[round(item, 2) for item in cons_data['WASTE_TONS_DAY'].tolist()])\n",
    "        latex_block += standard_row % tuple([r\"Trash / Day (tons)\\tnote{2}\"]+cons_data['TRASH_ACTUAL_TONS'].apply(lambda x: str(round(x,2))).tolist())\n",
    "\n",
    "        trash_chute_text = []\n",
    "        dropsite_text = []\n",
    "\n",
    "        cons_data.apply(lambda row: make_trash_text(row, trash_chute_text, 'TRASH_CHUTE_TONS', 'TRASH_CHUTE_SAUSAGE'), axis=1)\n",
    "        cons_data.apply(lambda row: make_trash_text(row, dropsite_text, 'TRASH_DROP_TONS', 'TRASH_DROP_BINS'), axis=1)\n",
    "\n",
    "        latex_block += chute_row % tuple(trash_chute_text)\n",
    "        latex_block += dropsite_row % tuple(dropsite_text)\n",
    "\n",
    "        latex_block += r\"\\end{tabularx}\\bigskip\"\n",
    "\n",
    "        latex_block += opening\n",
    "        latex_block += top_row % tuple(cons_data['DEV_NAME'].apply(lambda x: str(x).title()).tolist())\n",
    "\n",
    "        mgp_text = []\n",
    "        cardboard_text= []\n",
    "        paper_text = []\n",
    "\n",
    "        cons_data.apply(lambda row: make_trash_text(row, mgp_text, 'CAPTURED_MGP_TONS_WEEK', 'MGP_BAGS_WEEK'), axis=1)\n",
    "\n",
    "        cons_data.apply(lambda row: make_trash_text(row, cardboard_text, 'CAPTURED_CARDBOARD_TONS_WEEK', 'CARDBOARD_BALES_WEEK'), axis=1)\n",
    "\n",
    "        cons_data.apply(lambda row: make_trash_text(row, paper_text, 'CAPTURED_PAPER_TONS_WEEK', 'PAPER_BAGS_WEEK'), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        latex_block += recycling_row % tuple([r\"Metal, Glass, Plastic Captured / Week (tons)\"]+mgp_text)\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_MGP_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        latex_block += cardboard_row % tuple([r\"Cardboard Captured / Week (tons)\"]+cardboard_text)\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_CARDBOARD_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        latex_block += recycling_row % tuple([r\"Paper Captured / Week (tons)\"]+paper_text)\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_PAPER_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "\n",
    "        #latex_block += OET_row % tuple(['Organics']+cons_data['ORGANICS_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        #latex_block += OET_row % tuple(['E-Waste']+cons_data['EWASTE_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        #latex_block += OET_row % tuple(['Textiles']+cons_data['TEXTILES_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "\n",
    "        latex_block += r\"\\end{tabularx}\"\n",
    "\n",
    "        return latex_block\n",
    "    \n",
    "    if num_devs<= 4:\n",
    "        latex_block = make_waste_distribution_table_block(cons_data, num_cols)\n",
    "\n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block)\n",
    "    \n",
    "    else:\n",
    "        latex_block_1 = make_waste_distribution_table_block(cons_data.iloc[0:num_cols_1], num_cols_1)\n",
    "        latex_block_2 = make_waste_distribution_table_block(cons_data.iloc[num_cols_1:], num_cols_2)\n",
    "        \n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table_1.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block_1)\n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table_2.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block_2)\n",
    "\n",
    "    text_block = r''''''\n",
    "\n",
    "    text_line_multi = r\"\\bf{%s}: This development has %s apartment units and %s stairhalls.\\\\\"\n",
    "\n",
    "    text_line_singular = r\"\\bf{%s}: This development has %s apartment units and one stairhall.\\\\\"\n",
    "\n",
    "    for row in cons_data.itertuples():\n",
    "\n",
    "        if int(row.STAIRHALLS) == 1:\n",
    "            text_block += text_line_singular % (row.DEV_NAME.title(), int(row.CURRENT_APTS))\n",
    "        else:\n",
    "            text_block += text_line_multi % (row.DEV_NAME.title(), int(row.CURRENT_APTS), int(row.STAIRHALLS))\n",
    "\n",
    "\n",
    "    with open(f'TEXT/waste_distribution_bottom/{cons_tds}_wd_bottom.tex', 'w') as file_handle:\n",
    "        file_handle.write(text_block)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised by 091\n"
     ]
    }
   ],
   "source": [
    "overview_data = add_waste_cols(load_overview_data())\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_waste_distribution_table(tds, overview_data)\n",
    "    except:\n",
    "        print(f'Exception raised by {tds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Capital Improvements Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asset_data():\n",
    "    asset_data = {'fwd': ['In-Sink Food Grinders', pd.read_csv('DATA/capital_fwd.csv')],\n",
    "                  'ehd': ['Enlarged Hopper Doors', pd.read_csv('DATA/capital_ehd.csv')],\n",
    "                  'int_compactor':['Interior Compactor Replacement', pd.read_csv('DATA/capital_intcom.csv')],\n",
    "                  'wasteyard':['Waste Yard Redesign', pd.read_csv('DATA/capital_wasteyard.csv')]}\n",
    "\n",
    "    for value in asset_data.values():\n",
    "        value[1].columns = [item.strip() for item in value[1].columns]\n",
    "\n",
    "    asset_data['wasteyard'][1]['ESTIMATE'] = asset_data['wasteyard'][1]['TOT_EST']\n",
    "    asset_data['wasteyard'][1]['COST'] = np.nan\n",
    "    \n",
    "    def year_to_string(year):\n",
    "        if pd.isna(year):\n",
    "            return 'N/A'\n",
    "        else:\n",
    "            if int(year) <= 2022:\n",
    "                return str(int(year))\n",
    "            elif (int(year) > 2022) & (int(year) <= 2025):\n",
    "                return '2023-2025'\n",
    "            elif (int(year)>2025) and (int(year)<=2030):\n",
    "                return '2026-2030'\n",
    "            else:\n",
    "                return 'After 2030'\n",
    "    \n",
    "    asset_data['fwd'][1]['_YEAR'] = asset_data['fwd'][1]['EST_YEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['ehd'][1]['_YEAR'] = asset_data['fwd'][1]['CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['int_compactor'][1]['_YEAR'] = asset_data['int_compactor'][1]['CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['wasteyard'][1]['_YEAR'] = asset_data['wasteyard'][1]['CONS_CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    return asset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_capital_table(cons_tds, asset_data, overview_data=overview_data):\n",
    "    cons_data = overview_data[overview_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "\n",
    "    def make_capital_table_block(block_data, num_devs):\n",
    "        dev_col_format = r'X|'\n",
    "        header = r'''\n",
    "        \\begin{tabularx}{\\textwidth}{r|%s}\n",
    "        \\cline{2-%s}\n",
    "        ''' % ((dev_col_format*num_devs), num_devs)\n",
    "\n",
    "        top_row = r\"\\multicolumn{1}{l|}{}                                                        \"+r\"& \\cellcolor{ccorange}{\\color[HTML]{FFFFFF}%s} \"*num_devs+r\"\\\\ \\hline\"+\"\\n\"\n",
    "\n",
    "        project_block = r\"\\multicolumn{1}{|V{.2\\columnwidth}|}{\\cellcolor{ccorangelight}%s}          \"+(r\"&                                                                  \"*num_devs)+r\"\\\\\"+r'''\n",
    "        \\multicolumn{1}{|r|}{\\cellcolor{ccorangelight}\\textit{Status}}                '''+(r\"& %s                                                         \"*num_devs)+r'''\\\\\n",
    "        \\multicolumn{1}{|r|}{\\cellcolor{ccorangelight}\\textit{%s}}                  '''+(\"& %s                                                     \"*num_devs)+r\"\\\\ \\hline\"+\"\\n\"\n",
    "\n",
    "        devs = block_data['DEV_NAME'].apply(lambda x: str(x).upper()).tolist()\n",
    "        devs_title = block_data['DEV_NAME'].apply(lambda x: str(x).title()).tolist()\n",
    "        latex_block = ''\n",
    "        latex_block += header\n",
    "        latex_block += top_row % tuple(block_data['DEV_NAME'].apply(lambda x: str(x).title()).tolist())\n",
    "\n",
    "        for asset in asset_data.keys():\n",
    "            asset_df = asset_data[asset][1]\n",
    "            #print(asset_data[asset][0])\n",
    "            #print(devs)\n",
    "            #print(asset_df['DEVELOPMENT'].tolist())\n",
    "            if any((dev in asset_df['DEVELOPMENT'].tolist()) for dev in devs):\n",
    "                status_list = []\n",
    "                year_list = []\n",
    "\n",
    "                for dev in devs:\n",
    "                    if dev in asset_df['DEVELOPMENT'].tolist():\n",
    "                        #print(dev)\n",
    "                        if pd.isna(asset_df.loc[asset_df['DEVELOPMENT']== dev,'STATUS'].iloc[0]):\n",
    "                            status_list.append('Not Yet Scheduled')\n",
    "                        else:\n",
    "                            status_list.append(str(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'STATUS'].iloc[0]).title())\n",
    "\n",
    "                        #print(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'STATUS'])\n",
    "                        #print(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'COST'])\n",
    "                        #try:\n",
    "                        year_list.append(str(asset_df.loc[asset_df['DEVELOPMENT']== dev,'_YEAR'].iloc[0]))\n",
    "                    #except:\n",
    "                            #year_list.append('TBD')\n",
    "\n",
    "                    else:\n",
    "                        status_list.append('N/A')\n",
    "                        year_list.append(' ')\n",
    "\n",
    "                asset_block = project_block % tuple([asset_data[asset][0]]+status_list+['Year Planned']+year_list)\n",
    "\n",
    "                latex_block += asset_block\n",
    "\n",
    "        latex_block += r\"\\end{tabularx}\"\n",
    "\n",
    "        return latex_block\n",
    "    \n",
    "    \n",
    "    if num_devs <= 4:\n",
    "        num_cols = num_devs\n",
    "        block_data = cons_data\n",
    "        \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data, num_cols))\n",
    "        \n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)\n",
    "        block_data_1 = cons_data.iloc[0:num_cols_1]\n",
    "        block_data_2 = cons_data.iloc[num_cols_1:]\n",
    "        \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects_1.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data_1, num_cols_1))\n",
    "            \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects_2.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data_1, num_cols_1))\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_data= load_asset_data()\n",
    "overview_data = load_overview_data()\n",
    "for tds in consolidations.keys(): \n",
    "    make_capital_table(tds, asset_data, overview_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Staff Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staff_data(name_dict):\n",
    "    #Read budgeted staff and formula allocation\n",
    "    dev_staff = pd.read_csv('DATA/staff_for_table.csv')\n",
    "    dev_staff.fillna(0,inplace=True)\n",
    "    \n",
    "    def find_cons_tds(name, name_dict):\n",
    "        for key, value in name_dict.items():\n",
    "            if (name == value['name']) | (name in value['alternates']):\n",
    "                return key\n",
    "            \n",
    "    dev_staff['CONS_TDS'] = dev_staff['Consolidation'].apply(lambda x: find_cons_tds(x, name_dict))\n",
    "    dev_staff['CONS_NAME'] = dev_staff['CONS_TDS'].apply(lambda x: name_dict[x]['name'] if x is not None else 'NO NAME FOUND')\n",
    "    #Note: Staff list missing for Armstrong, Ft. Washington, and Williams Plaza, as well as scatter-site third-party-managed consolidations\n",
    " \n",
    "    #Read budgeted staff and actuals\n",
    "    actuals_data = pd.read_csv('DATA/Staffing_Analysis/DEVHC.csv')\n",
    "    actuals_data.fillna(0, inplace=True)\n",
    "    actuals_data = actuals_data[actuals_data['RC Name'].apply(lambda x: \"total\" not in str(x).lower()) & actuals_data['Department'].apply(lambda x: \"total\" not in str(x).lower())]\n",
    "    actuals_data['CONS_TDS'] = actuals_data['RC Name'].apply(lambda x: find_cons_tds(x, name_dict))\n",
    "    actuals_data['CONS_NAME'] = actuals_data['CONS_TDS'].apply(lambda x: name_dict[x]['name'] if x is not None else 'NO NAME FOUND')\n",
    "\n",
    "    def convert_neg(x):\n",
    "        try:\n",
    "            return int(x)\n",
    "        except:\n",
    "            return int('-'+str(x).replace('(','').replace(')',''))\n",
    "\n",
    "    actuals_data['VARIANCE'] = actuals_data['Unnamed: 5'].apply(lambda x: convert_neg(x))\n",
    "    actuals_data['ACT'] = actuals_data['13']\n",
    "    \n",
    "    table_frame = pd.read_csv('DATA/Table_Keys.csv')\n",
    "    actuals_keys = pd.read_csv('DATA/Staffing_Analysis/DEVHC_CODES.csv')\n",
    "    \n",
    "    actuals_data = actuals_data.merge(actuals_keys, how='left', left_on='CST_NAME', right_on='TITLE_NAME')\n",
    "    for column in ['Current Modified', 'ACT', 'VARIANCE']:\n",
    "        actuals_data[column] = actuals_data[column].astype(int)\n",
    "        \n",
    "\n",
    "    \n",
    "    return (dev_staff, actuals_data, table_frame, actuals_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_staff_table(cons_tds, dev_staff, actuals_data, table_frame, actuals_keys):\n",
    "    #Fetching staff data for consolidation\n",
    "    cons_data = dev_staff.loc[dev_staff['CONS_TDS'] == cons_tds]\n",
    "    if cons_data.shape[0] == 0:\n",
    "        return(f'Consolidation {cons_tds} not found in staffing data.')\n",
    "    #print(cons_tds)\n",
    "    #print(dev_staff)\n",
    "    \n",
    "    # Isolate and process actuals data for consolidation\n",
    "    try:\n",
    "        cons_actuals = actuals_data[actuals_data['CONS_TDS'] == cons_tds]\n",
    "    except:\n",
    "        print(f'{cons_tds} not found in actuals.')\n",
    "        return np.NaN\n",
    "    \n",
    "    cons_actuals = cons_actuals[['CONS_NAME', 'CONS_TDS', 'Current Modified', 'ACT', \n",
    "                                 'CODE_KEY', 'CODE_NAME']].groupby(by='CODE_KEY', as_index=False).agg({'CONS_NAME': 'first',\n",
    "                                                                                                       'CONS_TDS': 'first',\n",
    "                                                                                                     'Current Modified':sum,\n",
    "                                                                                                     'ACT':sum,\n",
    "                                                                                                     'CODE_NAME':'first'})\n",
    "    cons_actuals\n",
    "    cons_actuals.loc['Total']= cons_actuals.sum(numeric_only=True, axis=0)\n",
    "    cons_actuals.loc['Total','CODE_KEY'] = 11\n",
    "    cons_actuals.loc['Total','CODE_NAME'] = 'TOT'\n",
    "    \n",
    "    for row in cons_actuals.itertuples():\n",
    "        cons_data[f'{row.CODE_NAME}_ACT'] = row.ACT\n",
    "    #print(cons_data)\n",
    "    #Setting up table and transposing data\n",
    "    cons_table_frame = table_frame\n",
    "    cons_table_frame['Formula'] = cons_table_frame['FORMULA_KEY'].iloc[:-1].apply(lambda key: cons_data[key].iloc[0])\n",
    "    cons_table_frame['Budgeted'] = cons_table_frame['BUDG_KEY'].apply(lambda key: cons_data[key].iloc[0])\n",
    "    cons_table_frame['Actual'] = cons_table_frame['ACTUALS_KEY'].iloc[:-2].apply(lambda key: cons_data[key].iloc[0] if key in cons_data.columns else 0)\n",
    "\n",
    "    \n",
    "    #Simplifying table\n",
    "    cons_table = cons_table_frame[['CHART_LINE', 'Formula', 'Budgeted', 'Actual']]\n",
    "    #print(cons_table)\n",
    "    \n",
    "    #Defining LaTeX table format\n",
    "    \n",
    "    def make_staff_table_block(staff_data):\n",
    "    \n",
    "        table_template = r'''\n",
    "        \\begin{tabular}{l|c|c|c|}\n",
    "        \\cline{2-4}\n",
    "                                                                                     & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Formula Allocation} & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Budgeted} & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Actual} \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Employees}                      & %s                                                      & %s                                                                & %s                                                        \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Property Manager}               & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Asst. Property Manager}         & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Secretaries}                    & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Housing Assistants}             & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Superintendent}                 & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Assistant Superintendent}       & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Supervisor of Caretakers (SOC)} & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Supervisor of Grounds (SOG)}    & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Maintenance Workers}            & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers X}                   & %s                                                      & %s                                                                &                                                       \\\\ \\cline{1-3}\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers J\\tnote{1}}                   &                                                       & %s                                                                &                                                         \\\\ \\cline{1-1} \\cline{3-3}\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers G}                   & \\multirow{-2}{*}{%s}                                                      & %s                                     & \\multirow{-3}{*}{%s}                           \\\\ \\hline\n",
    "        \\end{tabular}\n",
    "        \n",
    "        '''\n",
    "\n",
    "        values = []\n",
    "\n",
    "        def extract_data_through_mw(row):\n",
    "            [values.append(item) for item in [str(int(row['Formula'])), \n",
    "                                              str(int(row['Budgeted'])), \n",
    "                                              str(int(row['Actual']))]]\n",
    "            pass\n",
    "\n",
    "        #Processing through Maintenance Worker\n",
    "        staff_data.iloc[0:-3].apply(lambda row: extract_data_through_mw(row), axis=1)\n",
    "\n",
    "        #Processing Caretakers\n",
    "        values.append(str(int(staff_data.iloc[-3, 1])))\n",
    "        values.append(str(int(staff_data.iloc[-3, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-2, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-2, 1])))\n",
    "        values.append(str(int(staff_data.iloc[-1, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-3, 3])))\n",
    "\n",
    "        return table_template % tuple(values)\n",
    "    \n",
    "    #Make and export LaTeX code\n",
    "    with open(f'TABLES/staff_table/{cons_tds}_staff_table.tex', 'w') as file_handle:\n",
    "        file_handle.write(make_staff_table_block(cons_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "staff_data = load_staff_data(consolidations)\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    make_staff_table(tds, *staff_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making and Compiling LaTeX Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_latex_file(tds, counts=counts):\n",
    "    #SET UTILITY PATHS HERE\n",
    "    pdflatex_path = '/usr/local/texlive/2018/bin/x86_64-darwin/pdflatex'\n",
    "    ghostscript_path = '/usr/local/bin/gs'\n",
    "    \n",
    "    if counts[tds]['count'] <= 4:\n",
    "        with open('REPORT_TEMPLATE/report.tex', 'r') as file_handle:\n",
    "            text = file_handle.read()\n",
    "            \n",
    "        new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "        with open(f'REPORTS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "            outfile.write(new_text)\n",
    "    \n",
    "    else:\n",
    "        with open('REPORT_TEMPLATE/report_long.tex', 'r') as file_handle:\n",
    "            text = file_handle.read()\n",
    "            \n",
    "        new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "        with open(f'REPORTS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "            outfile.write(new_text)\n",
    "\n",
    "    subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/LaTeX', f'REPORTS/LaTeX/{tds}_report.tex'])\n",
    "    subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/LaTeX', f'REPORTS/LaTeX/{tds}_report.tex'])\n",
    "    \n",
    "    #Be sure to install ghostscript (to compress pdfs), or comment out next line. Available via homebrew.\n",
    "    subprocess.check_call([ghostscript_path, '-sDEVICE=pdfwrite', '-dCompatibilityLevel=1.5', '-dNOPAUSE', '-dQUIET', '-dBATCH', f'-sOutputFile=REPORTS/{tds}_report.pdf', f'REPORTS/LaTeX/{tds}_report.pdf'])\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('REPORTS'):\n",
    "    os.makedirs('REPORTS')\n",
    "\n",
    "if not os.path.exists('REPORTS/LaTeX'):\n",
    "    os.makedirs('REPORTS/LaTeX')\n",
    "\n",
    "os.system(\"cp REPORT_TEMPLATE/content.tex REPORTS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/preface.tex REPORTS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/content_long.tex REPORTS/LaTeX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidation_list = consolidations.keys()\n",
    "\n",
    "for tds in consolidation_list:\n",
    "    try:\n",
    "        compile_latex_file(tds)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "filelist = [f for f in os.listdir('REPORTS/LaTeX') if not f.endswith(\".tex\")]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join('REPORTS/LaTeX', f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
