{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaTeX Automation for NYCHA Waste Individual Action Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import subprocess\n",
    "import shutil\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from pylatexenc.latexencode import unicode_to_latex\n",
    "from pylatexenc.latexencode import UnicodeToLatexEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Global Vars and Options\n",
    "os.chdir('/Users/kyleslugg/Documents/NYCHA/Production')\n",
    "pd.set_option('display.max_columns', None)\n",
    "fha_tds_list =['226', '283', '212', '226', '283', '212', '213', '274', '275', '260', '273', '284', '209']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standardized_names(drop_fha=True):\n",
    "    databook_names = pd.read_csv('DATA/name_tables/dev_data_book_name.csv')\n",
    "    databook_names['CONS_TDS'] = databook_names['CONS_TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    databook_names['TDS'] = databook_names['TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    staff_names = pd.read_csv('DATA/name_tables/staff_cons_name.csv')\n",
    "    staff_names['RC_Name'] = staff_names['RC Name']\n",
    "    \n",
    "    if drop_fha:\n",
    "        databook_names = databook_names.query(f'TDS not in {fha_tds_list}')\n",
    "    \n",
    "    consolidations = {}\n",
    "    developments = {}\n",
    "\n",
    "    for row in databook_names.itertuples():\n",
    "        consolidations[row.CONS_TDS] = {'name':row.CONS_NAME, 'alternates':[row.MANAGED_BY]}\n",
    "        developments[row.TDS] = {'name':row.DEV_NAME, 'name_alternates':[], 'cons_tds':row.CONS_TDS}\n",
    "        \n",
    "    def find_closest_fuzzy_match(name, comp_df, comp_col_name, return_col_name):\n",
    "        values = comp_df[comp_col_name].unique()\n",
    "        comp_df_copy = pd.DataFrame(data=values, index=[i for i in range(0,len(values))], columns=[comp_col_name])\n",
    "\n",
    "        '''\n",
    "        def strip_name(x):\n",
    "            string = str(x).lower()\n",
    "            string = string.replace('consolidated','')\n",
    "            string = string.replace('consolidation', '')\n",
    "            string = string.replace('houses', '')\n",
    "            return string\n",
    "\n",
    "        comp_df_copy['partial_ratio'] = comp_df_copy[comp_col_name].apply(lambda x: fuzz.partial_ratio(strip_name(name), strip_name(x)))\n",
    "        highest_match = comp_df_copy['partial_ratio'].max()\n",
    "\n",
    "        matches = comp_df_copy.loc[comp_df_copy['partial_ratio']==highest_match, 'CONS_NAME']\n",
    "\n",
    "        if matches.shape[0] == 1:\n",
    "            return matches.iloc[0]\n",
    "        else:\n",
    "            print(matches)\n",
    "            return 'ZZZ MULTIPLE MATCHES FOUND'\n",
    "\n",
    "        '''\n",
    "        return process.extractOne(str(name).lower(), values.tolist())[0]\n",
    "    \n",
    "    staff_names['NAME_MATCH'] = staff_names['RC Name'].apply(lambda x: find_closest_fuzzy_match(x, databook_names, 'CONS_NAME', 'CONS_NAME'))\n",
    "\n",
    "    match_corrections = {'Justice Sonia Sotomayor  Consolidated': 'SOTOMAYOR HOUSES CONSOLIDATED',\n",
    "                        'Murphy Consolidated': ''\n",
    "                        }\n",
    "\n",
    "    def make_corrections(row, index_col, data_col, dictionary):\n",
    "        if str(row[index_col]).strip() in dictionary.keys():\n",
    "            return dictionary[row[index_col]]\n",
    "        else:\n",
    "            return row[data_col]\n",
    "\n",
    "    staff_names['AMENDED_MATCHES'] = staff_names.apply(lambda row: make_corrections(row, 'RC Name', 'NAME_MATCH', match_corrections), axis=1)\n",
    "\n",
    "    staff_names = staff_names.merge(databook_names[['CONS_NAME', 'CONS_TDS']], left_on='AMENDED_MATCHES', right_on='CONS_NAME', how='left')\n",
    "\n",
    "    for row in staff_names.itertuples():\n",
    "        try:\n",
    "            consolidations[row.CONS_TDS]['alternates'].append(row.RC_Name)\n",
    "        except:\n",
    "            print(f'TDS #{row.CONS_TDS} raised an exception.')\n",
    "    \n",
    "    #From vehicle data...\n",
    "    consolidation_corrections = {'Brooklyn Borough Management':'N/A',\n",
    "                            'LaGuardia Houses':'LA GUARDIA CONSOLIDATED',\n",
    "                            'Hylan':'BUSHWICK CONSOLIDATED',\n",
    "                            'Manhattan Property Management':'N/A',\n",
    "                            'NYCHA - Brooklyn Property Mgmt':'N/A',\n",
    "                            'Queens-Staten Island Borough Manag':'N/A',\n",
    "                            'Webster-Morrisania Houses': 'WEBSTER CONSOLIDATED',\n",
    "                            'NGO':'N/A',\n",
    "                            'Millbrook Houses':'MILL BROOK CONSOLIDATED',\n",
    "                            'Van Dyke Houses':'VAN DYKE I',\n",
    "                            'UPACA':'JACKIE ROBINSON CONSOLIDATED',\n",
    "                            'Department of Mixed Finance Asset':'N/A',\n",
    "                            'Ocean Hill-Saratoga Village':'OCEAN HILL CONSOLIDATED',\n",
    "                            'nan':'N/A',\n",
    "                            'L.E.S. II/Campos':'LOWER EAST SIDE CONSOLIDATED',\n",
    "                            'St. Marys Park/Moore': \"SAINT MARY'S PARK CONSOLIDATED\",\n",
    "                            'Seth Low/Glenmore Plaza':'SETH LOW CONSOLIDATED',\n",
    "                            'Woodson/Van Dyke II':'WOODSON',\n",
    "                            'Beach 41st Street/Oceanside':'BEACH 41ST STREET-BEACH CHANNEL DRIVE',\n",
    "                            'CONEY ISLAND' : 'SURFSIDE GARDENS CONSOLIDATED',\n",
    "                            'BLAND' : 'LATIMER GARDENS CONSOLIDATED',\n",
    "                            'GRAVESEND' : \"O'DWYER GARDENS CONSOLIDATED\",\n",
    "                            'LES 2' : 'LOWER EAST SIDE CONSOLIDATED',\n",
    "                            'OCEAN BAY' : 'BEACH 41ST STREET-BEACH CHANNEL DRIVE',\n",
    "                            'OCEANBAY' : 'BEACH 41ST STREET-BEACH CHANNEL DRIVE',\n",
    "                            \"ST. MARY'S\" : \"SAINT MARY'S PARK CONSOLIDATED\",\n",
    "                            \"ST. NICHOLAS\" : 'SAINT NICHOLAS',\n",
    "                            'UNION AVE. CON.' : 'UNION AVENUE CONSOLIDATED',\n",
    "                            'WILLIAM REID' : 'REID APARTMENTS CONSOLIDATED',\n",
    "                            'MURPHY CONSOLIDATED':'1010 EAST 178TH STREET',\n",
    "                            'Murphy Consolidated':'1010 EAST 178TH STREET'}\n",
    "    \n",
    "    consolidations['NaN'] = {'name':'N/A',\n",
    "                            'alternates':[]}\n",
    "    for key, value in consolidation_corrections.items():\n",
    "        for key_c, value_c in consolidations.items():\n",
    "            if value_c['name'] == value: \n",
    "                try:\n",
    "                    consolidations[key_c]['alternates'].append(key)\n",
    "                except:\n",
    "                    consolidations[key_c]['alternates'] = key\n",
    "    \n",
    "    for key, value in consolidations.items():\n",
    "        for key_dev, value_dev in developments.items():\n",
    "            if key == value_dev['cons_tds']:\n",
    "                try:\n",
    "                    value['developments'].append(key_dev)\n",
    "                except:\n",
    "                    value['developments'] = [key_dev]\n",
    "    \n",
    "    return(consolidations, developments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDS #nan raised an exception.\n"
     ]
    }
   ],
   "source": [
    "consolidations, developments = get_standardized_names()\n",
    "\n",
    "counts ={}\n",
    "for key, value in developments.items():\n",
    "    if value['cons_tds'] not in counts.keys():\n",
    "        counts[value['cons_tds']] = {'developments':[key],\n",
    "                             'count':1}\n",
    "    else:\n",
    "        counts[value['cons_tds']]['developments'].append(key)\n",
    "        counts[value['cons_tds']]['count']+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list = [value['count'] for key, value in counts.items()]\n",
    "high_count_cons = [key for key, value in counts.items() if value['count']>=8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_overview_data(drop_fha=True):#Load Data\n",
    "    overview_data = pd.read_csv('DATA/overview_table_data.csv')\n",
    "    overview_data['CONS_TDS'] = overview_data['CONS_TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    overview_data['TDS'] = overview_data['TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    if drop_fha:\n",
    "        overview_data = overview_data.query(f'TDS not in {fha_tds_list}')\n",
    "    \n",
    "    return overview_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_data = load_overview_data()\n",
    "cons_list = overview_data['CONS_TDS'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and Process Text Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lightly modified version of example found at http://etienned.github.io/posts/extract-text-from-word-docx-simply/\n",
    "\n",
    "try:\n",
    "    from xml.etree.cElementTree import XML\n",
    "except ImportError:\n",
    "    from xml.etree.ElementTree import XML\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Module that extract text from MS XML Word document (.docx).\n",
    "(Inspired by python-docx <https://github.com/mikemaccana/python-docx>)\n",
    "\"\"\"\n",
    "\n",
    "WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "PARA = WORD_NAMESPACE + 'p'\n",
    "TEXT = WORD_NAMESPACE + 't'\n",
    "\n",
    "\n",
    "def get_docx_text(path):\n",
    "    \"\"\"\n",
    "    Take the path of a docx file as argument, return the text in unicode.\n",
    "    \"\"\"\n",
    "    document = zipfile.ZipFile(path)\n",
    "    try:\n",
    "        xml_content = document.read('word/document.xml')\n",
    "    except:\n",
    "        xml_content = document.read('word/document2.xml')\n",
    "        \n",
    "    document.close()\n",
    "    tree = XML(xml_content)\n",
    "\n",
    "    paragraphs = []\n",
    "    for paragraph in tree.getiterator(PARA):\n",
    "        texts = [node.text\n",
    "                 for node in paragraph.getiterator(TEXT)\n",
    "                 if node.text]\n",
    "        if texts:\n",
    "            paragraphs.append(''.join(texts))\n",
    "\n",
    "    return '\\n\\n'.join(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character Substitutions for LaTeX -- set and define \"clean\" method\n",
    "def clean_text(text):\n",
    "    substitutions = {'“':\"``\",\n",
    "                '”': \"''\",\n",
    "                '’':\"'\",\n",
    "                ' ':' ',\n",
    "                '–':'--',\n",
    "                ' ':' ',\n",
    "                '\\xa0':' ',\n",
    "                '&':r'\\&',\n",
    "                    ':':':',\n",
    "                    '#':'\\#'}\n",
    "    \n",
    "    for key, value in substitutions.items():\n",
    "        text = text.replace(key, value)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preface -- What is an IAP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preface_data():\n",
    "    about_text = clean_text(get_docx_text('TEXT/preface_text/about_IAPs.docx'))\n",
    "    staff_names = pd.read_excel('DATA/Dev_Staff_Names.xlsx')\n",
    "    candidate_list = []\n",
    "\n",
    "    for key, value in consolidations.items():\n",
    "        candidate_list.append(str(value['name']).upper())\n",
    "        for item in value['alternates']:\n",
    "            candidate_list.append(item.upper())\n",
    "\n",
    "    def get_cons_name(name):\n",
    "        match = process.extractOne(str(name).upper(), candidate_list)[0]\n",
    "        #print(match)\n",
    "        for key, value in consolidations.items():\n",
    "            if (match.upper() == value['name'].upper()) or (match.upper() in [val.upper() for val in value['alternates']]):\n",
    "                return value['name']\n",
    "\n",
    "        return '!!!NOT FOUND'\n",
    "\n",
    "    def get_tds_from_name(x):\n",
    "        for key, value in consolidations.items():\n",
    "            if str(value['name']).upper().strip() == x.upper().strip():\n",
    "                return key\n",
    "\n",
    "        return 'N/A'\n",
    "\n",
    "    staff_names['CONS_MATCH'] = staff_names['CONS'].apply(lambda x: get_cons_name(x))\n",
    "    staff_names['CONS_TDS'] = staff_names['CONS_MATCH'].apply(lambda x: get_tds_from_name(x))\n",
    "    \n",
    "    return (staff_names, about_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preface_text(tds, preface_data, about_text):\n",
    "    cons_data = preface_data[preface_data['CONS_TDS'] == tds]\n",
    "    \n",
    "    latex_block = r'''\\chapter{\\textcolor{darkBlue}{Preface}}\n",
    "\n",
    "    \\section{Letter from the Chair}\\label{sec:Section1}\n",
    "    \\clearpage\n",
    "    {\\fontfamily{phv}\\selectfont\n",
    "    \\section{What is an Individual Action Plan?}'''+'\\n\\n'+about_text\n",
    "    \n",
    "    #Following is no longer included:\n",
    "    '''\n",
    "    Below is a list of %s Management Personnel as of August 2020:\n",
    "    \\begin{itemize}\n",
    "    \\item Operations VP: %s\n",
    "    \\item %s Borough Director: %s\n",
    "    \\item Regional Asset Manager: %s\n",
    "    \\item Property Manager: %s\n",
    "    \\item Superintendent: %s\n",
    "    \\end{itemize}\n",
    "    }'''\n",
    "    \n",
    "    '''\n",
    "    data = [cons name, ops vp, borough name, borough dir, RAM, PM, super]\n",
    "    preface_data = []\n",
    "    \n",
    "    preface_data.append(str(cons_data['CONS_MATCH'].iloc[0]).title())\n",
    "    \n",
    "    for col in ['OPS_VP', 'BORO', 'BORO_DIR', 'RAM', 'PM', 'PMS']:\n",
    "        preface_data.append(cons_data[col].iloc[0])\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    with open(f'TEXT/preface_text/{tds}_preface.tex', 'w') as file_handle:\n",
    "        file_handle.write(clean_text(latex_block % tuple(preface_data)))\n",
    "    '''\n",
    "    with open(f'TEXT/preface_text/{tds}_preface.tex', 'w') as file_handle:\n",
    "        file_handle.write(clean_text(latex_block))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preface_data = load_preface_data()\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_preface_text(tds, *preface_data)\n",
    "    except:\n",
    "        print(f'{tds} raised an exception')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overview_text(cons_tds):\n",
    "    u = UnicodeToLatexEncoder(non_ascii_only = True, unknown_char_policy = (lambda x: ' '))\n",
    "    header = re.compile(r'((\\w*\\s)*(Overview))\\s*(:{0,2})\\s*')\n",
    "    linebreaks = re.compile(r'[\\n]+')\n",
    "    \n",
    "    overview_text = get_docx_text(f'TEXT/overview_text/{cons_tds}_Overview.docx')\n",
    "    overview_text = clean_text(overview_text)\n",
    "    \n",
    "    if len(header.findall(overview_text)) == 0:\n",
    "            overview_text = overview_text\n",
    "    else:\n",
    "        try:\n",
    "            overview_text = overview_text.replace(header.findall(overview_text)[0],'')\n",
    "            \n",
    "        except:\n",
    "            overview_text = overview_text.replace(header.findall(overview_text)[0][0],'')\n",
    "    \n",
    "    if overview_text[0] == ':':\n",
    "        overview_text = overview_text[1:]\n",
    "    \n",
    "    overview_text = re.sub(linebreaks, r\"\\\\par \\\\vspace{.7\\\\baselineskip}\", overview_text.strip())\n",
    "    with open(f'TEXT/overview_text/{cons_tds}_overview.tex', 'w') as file_handle:\n",
    "        #file_handle.write(u.unicode_to_latex(overview_text))\n",
    "        file_handle.write(overview_text)\n",
    "    \n",
    "    #return overview_text\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_overview_text(tds)\n",
    "    except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    #except:\n",
    "     #   print(f\"{tds} raised error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_analysis_text(cons_tds):\n",
    "    analysis_text = get_docx_text(f'TEXT/analysis_text/{cons_tds}_Analysis.docx')\n",
    "\n",
    "    header = re.compile(r'((\\w*\\s)*(Analysis)):{0,1}\\s*')\n",
    "    \n",
    "    analysis_text = clean_text(analysis_text)\n",
    "\n",
    "    section_headings = {'Inspection and Collection Requirement':['Inspection and Collection Requirements',\n",
    "                                                                 'Inspection and Collection Requirement',\n",
    "                                                                 'Collection and Inspection Requirements',\n",
    "                                                                'Collection and Inspection Requirement'],\n",
    "                        'Removal or Storage Requirement':['Removal or Storage Requirements',\n",
    "                                                          'Removal or Storage Requirement',\n",
    "                                                          'Removal and Storage Requirements',\n",
    "                                                         'Removal and Storage Requirement',\n",
    "                                                         'Storage or Removal Requirement',\n",
    "                                                          'Storage and Removal Requirements',\n",
    "                                                         'Storage and Removal Requirement',\n",
    "                                                         'Removal or Storage Requirement '],\n",
    "                       'Additional Context':['Additional Context']}\n",
    "    \n",
    "    for heading, variants in section_headings.items():\n",
    "        for variant in variants:\n",
    "            if variant in analysis_text:\n",
    "                analysis_text = analysis_text.replace(variant, r'\\textbf{%s}' % (heading))\n",
    "                break\n",
    "\n",
    "    #if len(header.findall(analysis_text)) == 0:\n",
    "     #   pass\n",
    "    #else:\n",
    "    try:\n",
    "        analysis_text = analysis_text.replace(header.findall(analysis_text)[0][0],'')\n",
    "\n",
    "    except:\n",
    "        analysis_text = analysis_text.replace(header.findall(analysis_text)[0],'')\n",
    "        \n",
    "    if analysis_text[0] == ':':\n",
    "        analysis_text = analysis_text[1:]\n",
    "\n",
    "    latex_block = analysis_text\n",
    "\n",
    "    with open(f'TEXT/analysis_text/{cons_tds}_analysis.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_analysis_text(tds)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    #except:\n",
    "     #   print(f\"{tds} raised error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and Prepare Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set asset map path\n",
    "#asset_map_path = f\"MAPS/asset_maps/{cons_tds}_asset_map.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split context map into two pages\n",
    "def process_context_map(cons_tds):\n",
    "    image = Image.open(f'MAPS/context_maps/{cons_tds}_context_map.png')\n",
    "    width, height = image.size\n",
    "\n",
    "    bb1 = (0,0,width/2,height)\n",
    "    bb2 = (width/2, 0, width, height)\n",
    "\n",
    "    img_1 = image.crop(bb1)\n",
    "    img_2 = image.crop(bb2)\n",
    "\n",
    "    img_1.save(f'MAPS/context_maps/{cons_tds}_context_1.png', format=\"PNG\")\n",
    "    img_2.save(f'MAPS/context_maps/{cons_tds}_context_2.png', format=\"PNG\")\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        process_context_map(tds)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO COME:\n",
    "- Consolidation Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Overview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overview_table(cons_tds, overview_data=overview_data):\n",
    "    \n",
    "    cons_data = overview_data.loc[overview_data['CONS_TDS']== cons_tds]\n",
    "    \n",
    "    overview_table = ''\n",
    "\n",
    "    overview_frame = r'''\n",
    "    \\resizebox{\\textwidth}{!}{\n",
    "    \\begin{tabular}{l|c|c|c|c|c|c|}\n",
    "    \\cline{2-7}\n",
    "                                                                           & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} TDS \\#} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Stairhalls \\#} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Units}  & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Households} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Official Population} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Average Family Size} \\\\ \\hline\n",
    "\n",
    "    '''\n",
    "\n",
    "    development_template = r'''\\multicolumn{1}{|l|}{\\cellcolor{ccteallight}%s}        & %s                                                   & %s                            & %s                                                   & %s                                                           & %s                                                                & %s                                                                \\\\ \\hline'''\n",
    "\n",
    "\n",
    "    overview_table += overview_frame\n",
    "\n",
    "    for row in cons_data.itertuples():\n",
    "        dev_name = clean_text(row.DEV_NAME.title())\n",
    "        dev_tds = row.TDS\n",
    "        stairhalls = int(row.STAIRHALLS)\n",
    "        units = f\"{int(row.TOTAL_APTS):,d}\" #Adds thousands comma sep.\n",
    "        total_hhs = row.TOTAL_HH\n",
    "        official_population = row.TOTAL_POP\n",
    "        avg_family_size = row.AVG_FAMILY_SIZE\n",
    "\n",
    "        overview_table += development_template % (dev_name, dev_tds, stairhalls, units, total_hhs, official_population, avg_family_size)\n",
    "\n",
    "    overview_table += r'''\n",
    "    \\end{tabular}\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    with open(f'TABLES/overview_table/{cons_tds}_overview_table.tex', 'w') as file_handle:\n",
    "        file_handle.write(overview_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_data = load_overview_data()\n",
    "for tds in consolidations.keys():\n",
    "    make_overview_table(tds, overview_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typology Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_typology_data(drop_fha=True):\n",
    "    #Cleaning and Shaping Data\n",
    "    typ_1 = pd.read_csv('DATA/typologies_1.csv')\n",
    "    typ_2 = pd.read_csv('DATA/typologies_2.csv')\n",
    "\n",
    "    typ_1.columns = ['CONS_NAME', 'DEV_NAME', 'TDS', 'TYPOLOGY']\n",
    "    typ_2.columns = ['CONS_NAME', 'CONS_TDS', 'DEV_NAME', 'TDS', 'METHOD', \n",
    "                     'CONSTRUCTION_DATE', 'BLDG_AGE', 'STORIES', 'BLDG_COVERAGE_SQFT', 'OPEN_SPACE_RATIO', 'SCATTERED_SITE_FLAG']\n",
    "\n",
    "    def make_dates(date_col):\n",
    "        date = str(date_col).split('/')\n",
    "        try:\n",
    "            if int(date[2]) > 18:\n",
    "                return datetime.date(int(f'19{date[2]}'), int(date[0]), int(date[1]))\n",
    "            else:\n",
    "                return datetime.date(int(f'20{date[2]}'), int(date[0]), int(date[1]))\n",
    "        except IndexError:\n",
    "            return datetime.date(1900,1,1)\n",
    "\n",
    "    typ_2['CONSTRUCTION_DATE'] = typ_2['CONSTRUCTION_DATE'].apply(lambda x: make_dates(x))\n",
    "    typ_2['SCATTERED_SITE_FLAG'] = typ_2['SCATTERED_SITE_FLAG'].apply(lambda x: x == 'YES')\n",
    "    typ_2.loc[typ_2['SCATTERED_SITE_FLAG']=='YES','SCATTERED_SITE_FLAG'] = 1\n",
    "\n",
    "    typology = typ_1.merge(typ_2[['CONS_TDS', 'TDS', 'METHOD',\n",
    "                                 'CONSTRUCTION_DATE', 'BLDG_AGE', \n",
    "                                 'STORIES', 'BLDG_COVERAGE_SQFT', \n",
    "                                 'OPEN_SPACE_RATIO', 'SCATTERED_SITE_FLAG']], how='left', on='TDS')\n",
    "\n",
    "    typology['CONS_TDS'] = typology['CONS_TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    typology['PREWAR'] = typology['CONSTRUCTION_DATE'].apply(lambda x: x < datetime.date(1945,1,1))\n",
    "    \n",
    "    if drop_fha:\n",
    "        typology = typology.query(f'TDS not in {fha_tds_list}')\n",
    "    #Adding Typology Icons\n",
    "\n",
    "    typ_icons = [r'\\rootpath/IMAGES/typology_earlytower.png', r'\\rootpath/IMAGES/typology_towerpark.png', r'\\rootpath/IMAGES/typology_prewar.png', r'\\rootpath/IMAGES/typology_scatteredsite.png']\n",
    "    typ_dict = {}\n",
    "    [typ_dict.setdefault(key, '') for key in typology['TYPOLOGY'].unique().tolist()]\n",
    "\n",
    "    typ_dict['1 - High-rise in the park'] = typ_icons[1]\n",
    "    typ_dict['2 - Mid-rise in the park'] = typ_icons[1]\n",
    "    typ_dict['3 - Low-rise in the park'] = typ_icons[0]\n",
    "    typ_dict['4 - Context Towers'] = typ_icons[3]\n",
    "    typ_dict['5 - Context Mid-rises'] = typ_icons[2]\n",
    "    typ_dict['6 - Walkups & Brownstones'] = typ_icons[2]\n",
    "\n",
    "    typ_header = re.compile(r'\\d\\s-\\s')\n",
    "\n",
    "    typology['TYP_NAME'] = typology['TYPOLOGY'].apply(lambda x: typ_header.sub('', str(x)))\n",
    "    typology['IMAGE_PATH'] = typology['TYPOLOGY'].apply(lambda x: typ_dict[x])\n",
    "    \n",
    "    return typology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_typology_table_block(cons_tds, typ_data):\n",
    "    cons_data = typ_data[typ_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "    \n",
    "    if num_devs < 5:\n",
    "        block_1 = cons_data\n",
    "    \n",
    "    elif num_devs >=5 and num_devs < 7:\n",
    "        block_1 = cons_data.iloc[0:3]\n",
    "        block_2 = cons_data.iloc[3:]\n",
    "    \n",
    "    else:\n",
    "        block_1 = cons_data.iloc[0:4]\n",
    "        block_2 = cons_data.iloc[4:]\n",
    "    \n",
    "    len_1 = block_1.shape[0]\n",
    "    \n",
    "    try:\n",
    "        len_2 = block_2.shape[0]\n",
    "    except:\n",
    "        len_2 = 0\n",
    "    \n",
    "    headers = {1:r\"\\begin{tabular}{m{1.5in} m{2in}}\"+'\\n',\n",
    "              2:r\"\\begin{tabular}{m{1.25in} m{2in} m{.1in} m{1.25in} m{2in}}\"+'\\n',\n",
    "              3:r\"\\begin{tabular}{m{1.25in} m{1.5in} m{.2in} m{1.25in} m{1.5in} m{.2in} m{1.25in} m{1.5in}}\"+'\\n',\n",
    "              4:r\"\\begin{tabular}{m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in}}\"+'\\n'}\n",
    "         \n",
    "    lines = {1:r'''\\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            2:r'''\\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            3:r'''\\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            4:r'''\\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}& & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}'''+'\\n'+r'\\end{tabular}'}\n",
    "    \n",
    "    \n",
    "    data_1 = []\n",
    "    data_2 = []\n",
    "    \n",
    "    for row in block_1.itertuples():\n",
    "        data_1.append(clean_text(str(row.DEV_NAME).title()))\n",
    "        data_1.append(str(row.TYP_NAME).replace('&', '\\&'))\n",
    "        data_1.append(row.IMAGE_PATH)\n",
    "    \n",
    "    if len_2 > 0:\n",
    "        for row in block_2.itertuples():\n",
    "            data_2.append(clean_text(str(row.DEV_NAME.title())))\n",
    "            data_2.append(str(row.TYP_NAME).replace('&', '\\&'))\n",
    "            data_2.append(row.IMAGE_PATH)\n",
    "    \n",
    "    # Assembling Nested Tables\n",
    "    latex_block = ''\n",
    "    if num_devs >= 2:\n",
    "        latex_block += r'''\\begin{table}[H]\n",
    "        \\resizebox{.9\\textwidth}{!}{\n",
    "        \\begin{tabular}{c}\n",
    "        '''\n",
    "    else:\n",
    "        latex_block += r'''\\begin{table}[H]\n",
    "        \\begin{tabular}{c}\n",
    "        '''\n",
    "    \n",
    "    latex_block += headers[len_1]\n",
    "    latex_block += lines[len_1] % tuple(data_1)\n",
    "    \n",
    "    if len_2 > 0:\n",
    "        latex_block += r'''\\\\\n",
    "        '''\n",
    "        latex_block += headers[len_2]\n",
    "        latex_block += lines[len_2] % tuple(data_2)\n",
    "    \n",
    "    if num_devs >= 2:\n",
    "        latex_block += r'''\\end{tabular}}\n",
    "        \\end{table}'''\n",
    "    else:\n",
    "        latex_block += r'''\\end{tabular}\n",
    "        \\end{table}'''\n",
    "    \n",
    "    with open(f'TABLES/typology_table/{cons_tds}_typology.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n",
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 raised an exception.\n",
      "128 raised an exception.\n",
      "NaN raised an exception.\n"
     ]
    }
   ],
   "source": [
    "typology = load_typology_data()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_typology_table_block(tds, typology)\n",
    "    except:\n",
    "        print(f'{tds} raised an exception.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waste Services and Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_container_counts = pd.read_csv('DATA/BULK_CONTAINER_COUNTS.csv')\n",
    "bulk_container_counts.drop(columns=bulk_container_counts.columns[3:], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEV_NAME</th>\n",
       "      <th>COUNT</th>\n",
       "      <th>ADDRESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>3</td>\n",
       "      <td>9820 SEAVIEW AVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOULEVARD</td>\n",
       "      <td>3</td>\n",
       "      <td>812 ASHFORD ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BREUKELEN</td>\n",
       "      <td>2</td>\n",
       "      <td>618 EAST 108TH ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAREY</td>\n",
       "      <td>2</td>\n",
       "      <td>2955 WEST 24TH ST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CYPRESS</td>\n",
       "      <td>2</td>\n",
       "      <td>600 EUCLID AVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>QUEENSBRIDGE SOUTH</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>RAVENSWOOD</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>REDFERN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>SOUTH JAMAICA</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>WOODSIDE</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               DEV_NAME COUNT             ADDRESS\n",
       "0             BAYVIEW       3    9820 SEAVIEW AVE\n",
       "1             BOULEVARD     3      812 ASHFORD ST\n",
       "2           BREUKELEN       2  618 EAST 108TH ST \n",
       "3               CAREY       2  2955 WEST 24TH ST \n",
       "4             CYPRESS       2      600 EUCLID AVE\n",
       "..                  ...   ...                 ...\n",
       "135  QUEENSBRIDGE SOUTH     6                 NaN\n",
       "136          RAVENSWOOD     3                 NaN\n",
       "137             REDFERN     2                 NaN\n",
       "138       SOUTH JAMAICA     3                 NaN\n",
       "139            WOODSIDE     4                 NaN\n",
       "\n",
       "[140 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bulk_container_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_wsa_data():\n",
    "    wsa_data = pd.read_csv('DATA/WASTE_SERVICES_ASSETS.csv')\n",
    "    wsa_data['TDS'] = wsa_data['DEV_TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    wsa_data['DEV_TDS'] = wsa_data['DEV_TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    wsa_data['INT_COMP_DATE'] = pd.to_datetime(wsa_data['INT_COMP_INSTALL_DATE'], errors='ignore')\n",
    "    \n",
    "    waste_collection_days = pd.read_csv('DATA/WASTE_COLLECTION_SCHEDULE.csv')\n",
    "    waste_collection_days['DEV_TDS'] = waste_collection_days['DEV_TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    day_abbreviations = {'Mon':'M',\n",
    "                        'Tue':'T',\n",
    "                        'Wed':'W',\n",
    "                        'Thu':'Th',\n",
    "                        'Fri':'F',\n",
    "                        'Sat':'Sa',\n",
    "                        'Sun':'Su'}\n",
    "    \n",
    "    def convert_days(x):\n",
    "        new_x = ''\n",
    "        try:\n",
    "            if ',' in str(x):\n",
    "                for part in str(x).split(','):\n",
    "                    new_x += day_abbreviations[part.strip()]\n",
    "\n",
    "            else:\n",
    "                new_x = day_abbreviations[str(x).strip()]\n",
    "\n",
    "            return new_x\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for col in ['FREQ_REFUS','FREQ_RECYC','FREQ_BULK']:\n",
    "        waste_collection_days[col] = waste_collection_days[col].apply(lambda x: convert_days(x))\n",
    "    \n",
    "    \n",
    "    def get_date(x):\n",
    "        try:\n",
    "            return x.strftime('%Y')\n",
    "        except:\n",
    "            return ' '\n",
    "\n",
    "    extcomp_data = pd.read_csv('DATA/EXT_COMPACTORS.csv')\n",
    "    extcomp_data['TDS'] = extcomp_data['LOCATION'].apply(lambda x: x.split('.')[0])\n",
    "    extcomp_data['INSTALLDATE'] = pd.to_datetime(extcomp_data['INSTALLDATE'])\n",
    "    extcomp_data.head()\n",
    "    groupby = extcomp_data.groupby('TDS').agg({'ASSETNUM':'count', 'INSTALLDATE': max}).reset_index()\n",
    "    groupby['EXT_COMP_YEAR'] = groupby['INSTALLDATE'].apply(lambda x: get_date(x))\n",
    "    groupby['TDS'] = groupby['TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    \n",
    "    #Adding number of bulk containers\n",
    "    bulk_container_counts = pd.read_csv('DATA/BULK_CONTAINER_COUNTS.csv')\n",
    "    candidate_list = []\n",
    "    for key, value in developments.items():\n",
    "        candidate_list.append(str(value['name']).upper())\n",
    "        for item in value['name_alternates']:\n",
    "            candidate_list.append(item.upper())\n",
    "\n",
    "    def get_dev_name(name):\n",
    "        match = process.extractOne(str(name).upper(), candidate_list)[0]\n",
    "        #print(match)\n",
    "        for key, value in developments.items():\n",
    "            if (match.upper() == value['name'].upper()) or (match.upper() in [val.upper() for val in value['name_alternates']]):\n",
    "                return value['name']\n",
    "\n",
    "        return '!!!NOT FOUND'\n",
    "\n",
    "    bulk_container_counts['DEV_NAME'] = bulk_container_counts['DEV_NAME'].apply(lambda x: get_dev_name(x))\n",
    "    wsa_data = wsa_data.merge(bulk_container_counts, on='DEV_NAME', how='left')\n",
    "\n",
    "    def get_count(x):\n",
    "        try:\n",
    "            return (str(int(x)))\n",
    "        except:\n",
    "            return '0'\n",
    "\n",
    "    wsa_data['COUNT'] = wsa_data['COUNT'].apply(lambda x: get_count(x))\n",
    "    \n",
    "    wsa_data = wsa_data.merge(waste_collection_days, on='DEV_TDS', how='left')\n",
    "    wsa_data = wsa_data.merge(groupby, on='TDS', how='left')\n",
    "    \n",
    "    #wsa_data[wsa_data['EXT_COMP'] != 0]\n",
    "\n",
    "    return wsa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_waste_services_table(cons_tds, wsa_data, counts_dict=counts):\n",
    "    dev_list = counts_dict[cons_tds]['developments']\n",
    "    cons_data = wsa_data.query(f\"TDS in {dev_list}\")\n",
    "    num_devs = counts_dict[cons_tds]['count']\n",
    "    \n",
    "    bulk_pickup_site = ''\n",
    "    \n",
    "    for dev in cons_data.itertuples():\n",
    "        if pd.isna(dev.BULK_HAULER):\n",
    "            pass\n",
    "        else:\n",
    "            bulk_pickup_site = str(dev.DEV_NAME).title()\n",
    "    \n",
    "    def make_waste_services_block(num_cols, block_data, bulk_pickup_site = bulk_pickup_site):\n",
    "        col_format = r'X|'\n",
    "        header = r'\\begin{tabularx}{\\textwidth}{V{1.5in}|'+col_format*(num_cols)+r'''}\n",
    "    \\cline{2-%s}\n",
    "                                                                                       '''% (num_cols)+r'& \\cellcolor{ccorange}{\\color[HTML]{FFFFFF} %s}'*num_cols+r' \\\\ \\hline'+'\\n'\n",
    "        hh_waste_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}Household Waste (DSNY)}               '+r'& %s'*num_cols+r'\\\\ \\hline'+'\\n'\n",
    "        bulk_waste_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}Bulk Waste}                  '+r'& %s'*num_cols+r' \\\\ \\hline'+'\\n'\n",
    "        norm_recycling_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                   '+r'& DSNY Curb Setout; collected %s'*num_cols + r'\\\\ \\hline'+'\\n'\n",
    "        special_recycling_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                   '+r'& %s'*num_cols +r'\\\\ \\hline' + '\\n'\n",
    "        \n",
    "        latex_block = r''''''\n",
    "        latex_block += header % tuple(block_data['DEV_NAME'].apply(lambda x: clean_text(str(x).title())).tolist())\n",
    "        \n",
    "        \n",
    "        hh_waste_data = []\n",
    "        bulk_waste_data = []\n",
    "        ewaste_data = []\n",
    "        textiles_data = []\n",
    "    \n",
    "        if bulk_pickup_site == '':\n",
    "            for dev in block_data.itertuples():\n",
    "                if dev.CURBSIDE == 1:\n",
    "                    hh_waste_data.append(f'Curbside Pickup {dev.FREQ_REFUS}')\n",
    "                elif dev.SHARE == 1:\n",
    "                    hh_waste_data.append(f'Transfer to {clean_text(str(dev.SHARE_SITE).title())}')\n",
    "                else:\n",
    "                    if True: \n",
    "                        if (dev.EXT_COMP_BE == 1) and (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactor in {int(dev.COMPACTOR_YARDS)} waste yard; collected {dev.FREQ_REFUS}')\n",
    "                        elif (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yard; collected {dev.FREQ_REFUS}')\n",
    "                        else:\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yards; collected {dev.FREQ_REFUS}')\n",
    "                    else:\n",
    "                        if (dev.EXT_COMP_BE == 1) and (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactor in {int(dev.COMPACTOR_YARDS)} waste yard; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                        elif (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yard; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                        else:\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yards; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                    \n",
    "                if pd.isna(dev.BULK_HAULER):\n",
    "                    if int(dev.BULK_SITES) == 0:\n",
    "                        bulk_waste_data.append(f\"Transferred for Pickup\")\n",
    "                    elif int(dev.BULK_SITES) == 1:\n",
    "                        bulk_waste_data.append(f\"One Bulk Waste Holding Site; Transferred for Pickup\")\n",
    "                    else:\n",
    "                        bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Waste Holding Sites; Transferred for Pickup\")\n",
    "                else:\n",
    "                    if int(dev.BULK_SITES) == 1:\n",
    "                        bulk_waste_data.append(f\"One Bulk Waste Holding; Picked up by {dev.BULK_HAULER}\")\n",
    "                    elif int(dev.BULK_SITES) > 1:\n",
    "                        bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Waste Holding Sites; Picked up by {dev.BULK_HAULER}\")\n",
    "                    else:\n",
    "                        bulk_waste_data.append(f\"Picked up by {dev.BULK_HAULER}\")\n",
    "\n",
    "                if dev.ECYCLE == 1:\n",
    "                    ewaste_data.append('Previously available through ECycle')\n",
    "                else:\n",
    "                    ewaste_data.append('N/A')\n",
    "\n",
    "                if dev.REFASHION == 1:\n",
    "                    textiles_data.append('Previously available through Refashion')\n",
    "                else:\n",
    "                    textiles_data.append('N/A')\n",
    "        else:\n",
    "            for dev in block_data.itertuples():\n",
    "                if dev.CURBSIDE == 1:\n",
    "                    hh_waste_data.append(f'Curbside Pickup {dev.FREQ_REFUS}')\n",
    "                elif dev.SHARE == 1:\n",
    "                    hh_waste_data.append(f'Transfer to {clean_text(str(dev.SHARE_SITE).title())}')\n",
    "                else:\n",
    "                    if True: \n",
    "                        if (dev.EXT_COMP_BE == 1) and (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactor in {int(dev.COMPACTOR_YARDS)} waste yard; collected {dev.FREQ_REFUS}')\n",
    "                        elif (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yard; collected {dev.FREQ_REFUS}')\n",
    "                        else:\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yards; collected {dev.FREQ_REFUS}')\n",
    "                    else:\n",
    "                        if (dev.EXT_COMP_BE == 1) and (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactor in {int(dev.COMPACTOR_YARDS)} waste yard; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                        elif (dev.COMPACTOR_YARDS == 1):\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yard; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                        else:\n",
    "                            hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yards; last replaced {dev.EXT_COMP_YEAR}')\n",
    "                    \n",
    "                if pd.isna(dev.BULK_HAULER):\n",
    "                    if int(dev.BULK_SITES) == 0:\n",
    "                        bulk_waste_data.append(f\"Transferred to {bulk_pickup_site} for Pickup\")\n",
    "                    elif int(dev.BULK_SITES) == 1:\n",
    "                        bulk_waste_data.append(f\"One Bulk Waste Holding Site; Transferred to {bulk_pickup_site} for Pickup\")\n",
    "                    else:\n",
    "                        bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Waste Holding Sites; Transferred to {bulk_pickup_site} for Pickup\")\n",
    "                else:\n",
    "                    if int(dev.BULK_SITES) == 1:\n",
    "                        bulk_waste_data.append(f\"One Bulk Waste Holding Site; Picked up by {dev.BULK_HAULER}\")\n",
    "                    elif int(dev.BULK_SITES) > 1:\n",
    "                        bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Waste Holding Sites; Picked up by {dev.BULK_HAULER}\")\n",
    "                    else:\n",
    "                        bulk_waste_data.append(f\"Picked up by {dev.BULK_HAULER}\")\n",
    "\n",
    "                if dev.ECYCLE == 1:\n",
    "                    ewaste_data.append('Previously available through ECycle')\n",
    "                else:\n",
    "                    ewaste_data.append('N/A')\n",
    "\n",
    "                if dev.REFASHION == 1:\n",
    "                    textiles_data.append('Previously available through Refashion')\n",
    "                else:\n",
    "                    textiles_data.append('N/A')\n",
    "        \n",
    "        latex_block += hh_waste_line % tuple(hh_waste_data)\n",
    "        latex_block += bulk_waste_line % tuple(bulk_waste_data)\n",
    "        latex_block += norm_recycling_line % tuple(['Recycling: Paper and Cardboard']+block_data['FREQ_RECYC'].tolist())\n",
    "        latex_block += norm_recycling_line % tuple(['Recycling: Metal, Glass and Plastic']+block_data['FREQ_RECYC'].tolist())\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: Mattresses']+['N/A' for i in range(0, num_cols)])\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: E-Waste']+ewaste_data)\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: Textiles']+textiles_data)\n",
    "        latex_block += r'\\end{tabularx}'\n",
    "        \n",
    "        return latex_block\n",
    "    \n",
    "    if num_devs <= 4:\n",
    "        num_cols = num_devs\n",
    "        block_data = cons_data\n",
    "        \n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols, block_data))\n",
    "        \n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)\n",
    "        block_data_1 = cons_data.iloc[0:num_cols_1]\n",
    "        block_data_2 = cons_data.iloc[num_cols_1:]\n",
    "        \n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services_1.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols_1, block_data_1))\n",
    "            \n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services_2.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols_2, block_data_2))\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "091 raised exception\n",
      "NaN raised exception\n"
     ]
    }
   ],
   "source": [
    "wsa_data = load_wsa_data()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_waste_services_table(tds, wsa_data)\n",
    "    except:\n",
    "        print(f'{tds} raised exception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_waste_assets_table(cons_tds, wsa_data, counts_dict=counts):\n",
    "    dev_list = counts_dict[cons_tds]['developments']\n",
    "    cons_data = wsa_data.query(f\"TDS in {dev_list}\")\n",
    "    num_devs = counts_dict[cons_tds]['count']\n",
    "    \n",
    "#    header = r'''\n",
    "#    \\begin{tabular}{V{.25\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.25\\columnwidth}|V{.15\\columnwidth}|}\n",
    "#\\cline{2-5}\n",
    "#                                                                                              & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Internal Compactors} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} External Compactors} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Other External Assets}   & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Recycling Bins\\tnote{1}} \\\\ \\hline'''+'\\n'\n",
    "#    line_format = r'\\multicolumn{1}{|V{.25\\columnwidth}|}{\\cellcolor{ccorange}{\\color[HTML]{FFFFFF} %s}}        & %s                                                & %s                                                                  & %s & %s                                                            \\\\ \\hline'+'\\n'\n",
    "    \n",
    "    header = r'''\n",
    "    \\begin{tabular}{V{.15\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|}\n",
    "\\cline{2-6}\n",
    "                                                                                              & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Internal Compactors} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} External Compactors}  & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Bulk Containers} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Cardboard Balers} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Mattress Containers} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Recycling Bins\\tnote{1}} \\\\ \\hline'''+'\\n'\n",
    "    line_format = r'\\multicolumn{1}{|V{.15\\columnwidth}|}{\\cellcolor{ccorange}{\\color[HTML]{FFFFFF} %s}}        & %s    & %s                                               & %s           & %s      & %s                                                             & %s                                                             \\\\ \\hline'+'\\n'\n",
    "    \n",
    "    \n",
    "    latex_block = r''''''\n",
    "    latex_block += header\n",
    "    \n",
    "    for dev in cons_data.itertuples():\n",
    "        line_data = []\n",
    "        line_data.append(clean_text(str(dev.DEV_NAME).title()))\n",
    "        \n",
    "        if (dev.INT_COMP == 0):\n",
    "            int_comp_string = '0'\n",
    "        elif pd.isna(dev.INT_COMP_DATE):\n",
    "            int_comp_string = str(int(dev.INT_COMP))\n",
    "        else:\n",
    "            int_comp_string = f'{str(int(dev.INT_COMP))}; last replaced {str(dev.INT_COMP_DATE.year)}'\n",
    "        \n",
    "        line_data.append(int_comp_string)\n",
    "        \n",
    "        if pd.isna(dev.EXT_COMP_YEAR):\n",
    "            line_data.append(str(int(dev.EXT_COMP_BE)))\n",
    "        else:\n",
    "            line_data.append(f\"{int(dev.EXT_COMP_BE)}; last replaced {dev.EXT_COMP_YEAR}\")\n",
    "        \n",
    "        #if (dev.BULK_CRUSHERS == 0) and (dev.BALERS == 0)... REDO THIS ONCE DATA ARE COMPLETE\n",
    "        #line_data.append('PLACEHOLDER UNTIL DATA ARE COMPLETE')\n",
    "        try:\n",
    "            line_data.append(str(int(dev.COUNT)))\n",
    "        except:\n",
    "            line_data.append(str(dev.COUNT))\n",
    "        \n",
    "        line_data.append(str(int(dev.BALERS)))\n",
    "        \n",
    "        line_data.append(str(int(0)))\n",
    "        \n",
    "        line_data.append(str(int(dev.RECYCLING_BINS)))\n",
    "        \n",
    "        latex_block += line_format % tuple(line_data)\n",
    "    \n",
    "    latex_block += r'\\end{tabular}'\n",
    "    \n",
    "    with open(f'TABLES/waste_assets/{cons_tds}_waste_assets.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN raised exception\n"
     ]
    }
   ],
   "source": [
    "wsa_data = load_wsa_data()\n",
    "for tds in consolidations.keys():  \n",
    "    try:\n",
    "        make_waste_assets_table(tds, wsa_data)\n",
    "    except:\n",
    "        print(f'{tds} raised exception')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidation Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#IN DEVELOPMENT\n",
    "\n",
    "def load_vehicle_data():\n",
    "    vehicle_data = pd.read_excel('DATA/vehicle_inventory.xlsx')\n",
    "    vehicle_data['CONS'] = vehicle_data['WORK LOCATION'].apply(lambda x: str(x).replace('NYCHA-',''))\n",
    "\n",
    "    candidate_list = []\n",
    "    for key, value in consolidations.items():\n",
    "        candidate_list.append(str(value['name']).upper())\n",
    "        for item in value['alternates']:\n",
    "            candidate_list.append(item.upper())\n",
    "\n",
    "    def get_cons_name(name):\n",
    "        match = process.extractOne(str(name).upper(), candidate_list)[0]\n",
    "        #print(match)\n",
    "        for key, value in consolidations.items():\n",
    "            if (match.upper() == value['name'].upper()) or (match.upper() in [val.upper() for val in value['alternates']]):\n",
    "                return value['name']\n",
    "\n",
    "        return '!!!NOT FOUND'\n",
    "\n",
    "\n",
    "    def get_tds_from_name(x):\n",
    "        for key, value in consolidations.items():\n",
    "            if str(value['name']).upper().strip() == x.upper().strip():\n",
    "                return key\n",
    "\n",
    "        return 'N/A'\n",
    "    \n",
    "    \n",
    "    def get_vehicle_type(x):\n",
    "        \n",
    "        if type(x) != str:\n",
    "            return 'N/A'\n",
    "        \n",
    "        van_keys = ['VAN', 'SPRINTER', 'ECONOLINE', 'TRANSIT']\n",
    "        truck_keys = ['PICK-UP', 'F250', 'F450', 'SIERRA', 'RANGER', 'Pick-Up']\n",
    "        for key in van_keys:\n",
    "            if key in x:\n",
    "                return 'VAN'\n",
    "\n",
    "        for key in truck_keys:\n",
    "            if key in x:\n",
    "                return 'TRUCK'\n",
    "\n",
    "        return 'OTHER'\n",
    "    \n",
    "    vehicle_data['CONS_MATCH'] = vehicle_data['CONS'].apply(lambda x: get_cons_name(str(x)))\n",
    "\n",
    "    vehicle_data['TDS'] = vehicle_data['CONS_MATCH'].apply(lambda x: get_tds_from_name(str(x)))\n",
    "    #vehicle_data['CONS_TDS'] = vehicle_data['TDS'].apply(lambda x: developments[x]['cons_tds'] if x in developments.keys() else np.NaN)\n",
    "    vehicle_data['TYPE'] = vehicle_data['DESCRIPTION'].apply(lambda x: get_vehicle_type(x))\n",
    "    \n",
    "    return vehicle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_horticultural_data():\n",
    "    equip_table = pd.read_csv('DATA/hort_equipment.csv')\n",
    "    equip_types = pd.read_csv('DATA/hort_equipment_types.csv')\n",
    "    equip_table.rename(columns={'MODL':'MODEL'}, inplace=True)\n",
    "    equip_table = equip_table.merge(equip_types, on=['MAKE', 'MODEL'], how='left')\n",
    "    \n",
    "    candidate_list = []\n",
    "    for key, value in consolidations.items():\n",
    "        candidate_list.append(str(value['name']).upper())\n",
    "        for item in value['alternates']:\n",
    "            candidate_list.append(item.upper())\n",
    "\n",
    "    def get_cons_name(name):\n",
    "        match = process.extractOne(str(name).upper(), candidate_list)[0]\n",
    "        #print(match)\n",
    "        for key, value in consolidations.items():\n",
    "            if (match.upper() == value['name'].upper()) or (match.upper() in [val.upper() for val in value['alternates']]):\n",
    "                return value['name']\n",
    "\n",
    "        return '!!!NOT FOUND'\n",
    "\n",
    "\n",
    "    def get_tds_from_name(x):\n",
    "        for key, value in consolidations.items():\n",
    "            if str(value['name']).upper().strip() == x.upper().strip():\n",
    "                return key\n",
    "\n",
    "        return 'N/A'\n",
    "    \n",
    "    def clean_consolidations(x):\n",
    "        correction_dict = {'': 'N/A',\n",
    "                            'Loaner': 'N/A',\n",
    "                            'ATLANTIC TERMINAL': 'WYCKOFF GARDENS CONSOLIDATED',\n",
    "                            'Assignment':'N/A',\n",
    "                            'BAYCHESTER':'N/A',\n",
    "                            'FLEET (GSD)' : 'N/A',\n",
    "                            'FLEET LOANER' : 'N/A',\n",
    "                            'LOANER' : 'N/A',\n",
    "                            'Loaner' : 'N/A',\n",
    "                            'MAR' : 'N/A',\n",
    "                            'MRST' : 'N/A',\n",
    "                            'MURPHY CONSOL.' : '1010 EAST 178TH STREET',\n",
    "                            'MURPHY CONSOLIDATED' : '1010 EAST 178TH STREET',\n",
    "                            'MURPHY' : '1010 EAST 178TH STREET',\n",
    "                            \"TBD\" : 'N/A',\n",
    "                            'loaner':'N/A'}\n",
    "        if str(x).strip() in correction_dict.keys():\n",
    "            return correction_dict[str(x).strip()]\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    equip_table['CONS'] = equip_table['LOCATION'].apply(lambda x: get_cons_name(clean_consolidations(x)))\n",
    "    equip_table['CONS_TDS'] = equip_table['CONS'].apply(lambda x: get_tds_from_name(x))\n",
    "    \n",
    "    return equip_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_consolidation_assets_table(cons_tds, vehicle_data, hort_data):\n",
    "    cons_vehicle_data = vehicle_data[vehicle_data['TDS'] == cons_tds]\n",
    "    cons_hort_data = hort_data[hort_data['CONS_TDS'] == cons_tds]\n",
    "    \n",
    "    block_template = r'''\\begin{tabular}{m{.25\\columnwidth}m{.25\\columnwidth}m{.25\\columnwidth}m{.25\\columnwidth}}\n",
    "    {\\color{ccorange} %s Trucks} & {\\color{ccorange} %s Skid Steers} & {\\color{ccorange} %s Tractors} & {\\color{ccorange} %s Sweepers} \\\\\n",
    "    \\includegraphics[width=.15\\columnwidth]{\\rootpath/IMAGES/truck.png}  & \\includegraphics[width=.15\\columnwidth]{\\rootpath/IMAGES/bobcat.png} & \\includegraphics[width=.15\\columnwidth]{\\rootpath/IMAGES/tractor.png} & \\includegraphics[width=.15\\columnwidth]{\\rootpath/IMAGES/road-sweeper.png}                         \n",
    "    \\end{tabular}'''\n",
    "    \n",
    "    num_trucks = cons_vehicle_data[cons_vehicle_data['TYPE'] == 'TRUCK'].shape[0]\n",
    "    num_vans = cons_vehicle_data[cons_vehicle_data['TYPE'] == 'VAN'].shape[0]\n",
    "    num_skidsteers = cons_hort_data[cons_hort_data['TYPE'] == 'SKIDSTEER'].shape[0]\n",
    "    num_mowers = cons_hort_data[cons_hort_data['TYPE'] == 'MOWER'].shape[0]\n",
    "    num_tractors = cons_hort_data.query(f\"TYPE in {['TRACTOR', 'TOOLCAT']}\").shape[0]\n",
    "    num_sweepers = cons_hort_data.query(f\"TYPE in {['SWEEPER', 'VAC']}\").shape[0]\n",
    "    \n",
    "    block_data = [str(num_trucks), str(num_skidsteers), str(num_tractors), str(num_sweepers)]\n",
    "    \n",
    "    with open(f'TABLES/consolidation_assets/{cons_tds}_consolidation_assets.tex', 'w') as file_handle:\n",
    "        file_handle.write(block_template % tuple(block_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_data = load_vehicle_data()\n",
    "hort_data = load_horticultural_data()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_consolidation_assets_table(tds, vehicle_data, hort_data)\n",
    "    except:\n",
    "        print(f'{tds} raised exception')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waste Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_waste_cols(overview_data):\n",
    "    conversion_factors = {'units_to_tons_day': 0.0025,\n",
    "                         'cy_per_ton': {'trash': 21.05,\n",
    "                                        'trash_actual': 0,\n",
    "                                       'MGP': 18.02,\n",
    "                                       'cardboard': 26.67,\n",
    "                                       'paper': 6.19,\n",
    "                                       'organics': 4.32,\n",
    "                                       'ewaste': 5.65,\n",
    "                                       'textiles': 13.33},\n",
    "                         'gallons_per_cy': 201.974,\n",
    "                         'gallons_per_64gal': 64,\n",
    "                         'gallons_per_40lb_bag': 44,\n",
    "                         'cy_per_44gal_bag':0.174,\n",
    "                         'cy_per_cardboard_bale':0.193}\n",
    "\n",
    "    waste_percentages = {'trash': .26,\n",
    "                         'trash_actual':.894,\n",
    "                        'MGP': .19,\n",
    "                        'cardboard': .07,\n",
    "                        'paper': .07,\n",
    "                        'organics':.32,\n",
    "                        'ewaste': .01,\n",
    "                        'textiles': .08}\n",
    "\n",
    "    capture_rates = {'trash_primary': .75,\n",
    "                    'trash_secondary': .25,\n",
    "                    'mgp': .30,\n",
    "                    'cardboard': .50,\n",
    "                    'paper': .20}\n",
    "\n",
    "    overview_data['WASTE_TONS_DAY'] = overview_data['CURRENT_APTS'].apply(lambda x: x * conversion_factors['units_to_tons_day'])\n",
    "\n",
    "    for key, value in waste_percentages.items():\n",
    "        overview_data[f'{key.upper()}_CY'] = overview_data['WASTE_TONS_DAY'].apply(lambda x: x * value * conversion_factors['cy_per_ton'][key])\n",
    "        overview_data[f'{key.upper()}_TONS'] = overview_data['WASTE_TONS_DAY'].apply(lambda x: x * value)\n",
    "    \n",
    "    overview_data['TRASH_ACTUAL_CY'] = (overview_data['TRASH_CY']+\n",
    "                                           overview_data['MGP_CY']+\n",
    "                                           overview_data['CARDBOARD_CY']+\n",
    "                                           overview_data['PAPER_CY']+\n",
    "                                           overview_data['ORGANICS_CY']+\n",
    "                                           overview_data['EWASTE_CY']+\n",
    "                                           overview_data['TEXTILES_CY'])-(overview_data['MGP_CY']*capture_rates['mgp']+\n",
    "                                                                         overview_data['CARDBOARD_CY']*capture_rates['cardboard']+\n",
    "                                                                         overview_data['PAPER_CY']*capture_rates['paper'])\n",
    "\n",
    "    overview_data['TRASH_CHUTE_CY'] = overview_data['TRASH_ACTUAL_CY']*capture_rates['trash_primary']\n",
    "    overview_data['TRASH_CHUTE_TONS'] = overview_data['TRASH_ACTUAL_TONS']*capture_rates['trash_primary']\n",
    "    overview_data['TRASH_CHUTE_SAUSAGE'] = ((overview_data['TRASH_CHUTE_CY'])/conversion_factors['cy_per_ton']['trash'])*(2000/40)\n",
    "    overview_data['TRASH_DROP_CY'] = overview_data['TRASH_ACTUAL_CY']*capture_rates['trash_secondary']\n",
    "    overview_data['TRASH_DROP_TONS'] = overview_data['TRASH_ACTUAL_TONS']*capture_rates['trash_secondary']\n",
    "    overview_data['TRASH_DROP_BINS'] = overview_data['TRASH_DROP_CY']*conversion_factors['gallons_per_cy']/64\n",
    "    overview_data['CAPTURED_MGP_TONS_WEEK'] = overview_data['MGP_TONS']*capture_rates['mgp']*7\n",
    "    overview_data['CAPTURED_CARDBOARD_TONS_WEEK'] = overview_data['CARDBOARD_TONS']*capture_rates['cardboard']*7\n",
    "    overview_data['CAPTURED_PAPER_TONS_WEEK'] = overview_data['PAPER_TONS']*capture_rates['paper']*7\n",
    "    overview_data['MGP_BAGS_WEEK'] = overview_data['MGP_CY']*capture_rates['mgp']*7/conversion_factors['cy_per_44gal_bag']\n",
    "    overview_data['PAPER_BAGS_WEEK'] = overview_data['PAPER_CY']*capture_rates['paper']*7/conversion_factors['cy_per_44gal_bag']\n",
    "    overview_data['CARDBOARD_BALES_WEEK'] = overview_data['CARDBOARD_CY']*capture_rates['cardboard']*7/conversion_factors['cy_per_cardboard_bale']\n",
    "    \n",
    "    actual_tonnage = pd.read_csv('DATA/WASTE_TONNAGE_2017.csv').dropna()\n",
    "    overview_data = overview_data.merge(actual_tonnage, on='DEV_NAME', how='left')\n",
    "    \n",
    "    return overview_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_waste_distribution_table(cons_tds, overview_data, wsa_data):\n",
    "    cons_data = overview_data[overview_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "    \n",
    "    if num_devs == 1:\n",
    "        num_cols = num_devs\n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)+1\n",
    "    else:\n",
    "        num_cols = num_devs+1\n",
    "    \n",
    "    if num_devs != 1:\n",
    "        cons_data.loc['Total']= cons_data.sum(numeric_only=True, axis=0)\n",
    "        cons_data.loc['Total','DEV_NAME'] = 'Total'\n",
    "    \n",
    "    def make_waste_distribution_table_block(cons_data, num_cols):\n",
    "        dev_col_format = r'X|'\n",
    "\n",
    "        opening = r'''\n",
    "        \\begin{tabularx}{\\textwidth}{V{1.25in}|%s}\n",
    "        \\cline{2-%s}\n",
    "        ''' % (dev_col_format*num_cols, (num_cols+1))\n",
    "\n",
    "        top_row = r'''\n",
    "                                                                       '''+(r\"& \\multicolumn{1}{V{1.25in}|}{\\cellcolor{ccorange}{\\color[HTML]{FFFFFF}%s}}\"*(num_cols))+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        standard_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s}                 \"+(r\"& %s                                    \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        captured_row = r\"\\multicolumn{1}{|Y{1.25in}|}{\\cellcolor{ccorangelight}Captured / Week (tons)\\tnote{4}}                        \"+(r\"& %s                                    \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        #chute_row = r\"\\multicolumn{1}{|Y{1.25in}|}{\\cellcolor{ccorangelight}Trash Chutes\\tnote{3}}                 \"+(r\"& %s tons [%s 40 lbs. sausage bags]      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "        chute_row = r\"\\multicolumn{1}{|Y{1.25in}|}{\\cellcolor{ccorangelight}Trash Chutes\\tnote{3}}                 \"+(r\"& %s sausage bags      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        #dropsite_row = r\"\\multicolumn{1}{|Y{1.25in}|}{\\cellcolor{ccorangelight}Drop Sites\\tnote{4}}                 \"+(r\"& %s tons [%s 64-gal. bins]      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "        dropsite_row = r\"\\multicolumn{1}{|Y{1.25in}|}{\\cellcolor{ccorangelight}Drop Sites\\tnote{4}}                 \"+(r\"& %s 64-gal. bins      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        OET_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s / Day (CY)}              \"+(r\"& %s                                    \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        #recycling_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s \\tnote{5}}                 \"+(r\"& %s tons [%s 44-gal. bags]                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "        recycling_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s \\tnote{5}}                 \"+(r\"& %s 44-gal. bags                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        #cardboard_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s \\tnote{5}}                 \"+(r\"& %s tons [%s bales]                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "        cardboard_row = r\"\\multicolumn{1}{|V{1.25in}|}{\\cellcolor{ccorangelight}%s \\tnote{5}}                 \"+(r\"& %s bales                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "\n",
    "        def make_trash_text(row, text_var, cy_col, other_col):\n",
    "            text_var.append(round(row[cy_col],1))\n",
    "            text_var.append(round(row[other_col], 1))\n",
    "            pass\n",
    "\n",
    "        latex_block = r'\\textbf{Projected Daily Trash Volumes}'\n",
    "        latex_block += opening\n",
    "        latex_block += top_row % tuple(cons_data['DEV_NAME'].apply(lambda x: clean_text(str(x).title())).tolist())\n",
    "        latex_block += standard_row % tuple([r\"Waste Generated / Day (Tons)\\tnote{1}\"]+[round(item, 1) for item in cons_data['WASTE_TONS_DAY'].tolist()])\n",
    "        latex_block += standard_row % tuple([r\"Trash / Day (tons)\\tnote{2}\"]+cons_data['TRASH_ACTUAL_TONS'].apply(lambda x: str(round(x,1))).tolist())\n",
    "\n",
    "        trash_chute_text = []\n",
    "        dropsite_text = []\n",
    "\n",
    "        #cons_data.apply(lambda row: make_trash_text(row, trash_chute_text, 'TRASH_CHUTE_TONS', 'TRASH_CHUTE_SAUSAGE'), axis=1)\n",
    "        #cons_data.apply(lambda row: make_trash_text(row, dropsite_text, 'TRASH_DROP_TONS', 'TRASH_DROP_BINS'), axis=1)\n",
    "\n",
    "        #latex_block += chute_row % tuple(trash_chute_text)\n",
    "        #latex_block += dropsite_row % tuple(dropsite_text)\n",
    "        latex_block += chute_row % tuple([round(item, 1) for item in cons_data['TRASH_CHUTE_SAUSAGE'].tolist()])\n",
    "        latex_block += dropsite_row % tuple([round(item, 1) for item in cons_data['TRASH_DROP_BINS'].tolist()])\n",
    "        latex_block += standard_row % tuple([r\"Est. Drop Sites\"]+cons_data['BLDGS'].apply(lambda x: str(int(x))).tolist())\n",
    "\n",
    "        \n",
    "        latex_block += r\"\\end{tabularx}\\bigskip\"\n",
    "        \n",
    "        latex_block += r'\\textbf{Projected Weekly Recycling Volumes}'\n",
    "        latex_block += opening\n",
    "        latex_block += top_row % tuple(cons_data['DEV_NAME'].apply(lambda x: clean_text(str(x).title())).tolist())\n",
    "\n",
    "        mgp_text = []\n",
    "        cardboard_text= []\n",
    "        paper_text = []\n",
    "\n",
    "        #cons_data.apply(lambda row: make_trash_text(row, mgp_text, 'CAPTURED_MGP_TONS_WEEK', 'MGP_BAGS_WEEK'), axis=1)\n",
    "\n",
    "        #cons_data.apply(lambda row: make_trash_text(row, cardboard_text, 'CAPTURED_CARDBOARD_TONS_WEEK', 'CARDBOARD_BALES_WEEK'), axis=1)\n",
    "\n",
    "        #cons_data.apply(lambda row: make_trash_text(row, paper_text, 'CAPTURED_PAPER_TONS_WEEK', 'PAPER_BAGS_WEEK'), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        latex_block += recycling_row % tuple([r\"Metal, Glass, Plastic Captured / Week (tons)\"]+[round(item, 1) for item in cons_data['MGP_BAGS_WEEK'].tolist()])\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_MGP_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        latex_block += cardboard_row % tuple([r\"Cardboard Captured / Week (tons)\"]+[round(item, 1) for item in cons_data['CARDBOARD_BALES_WEEK'].tolist()])\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_CARDBOARD_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        latex_block += recycling_row % tuple([r\"Paper Captured / Week (tons)\"]+[round(item, 1) for item in cons_data['PAPER_BAGS_WEEK'].tolist()])\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_PAPER_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "\n",
    "        #latex_block += OET_row % tuple(['Organics']+cons_data['ORGANICS_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        #latex_block += OET_row % tuple(['E-Waste']+cons_data['EWASTE_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        #latex_block += OET_row % tuple(['Textiles']+cons_data['TEXTILES_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "\n",
    "        latex_block += r\"\\end{tabularx}\"\n",
    "\n",
    "        return latex_block\n",
    "    \n",
    "    if num_devs<= 4:\n",
    "        latex_block = make_waste_distribution_table_block(cons_data, num_cols)\n",
    "\n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block)\n",
    "    \n",
    "    else:\n",
    "        latex_block_1 = make_waste_distribution_table_block(cons_data.iloc[0:num_cols_1], num_cols_1)\n",
    "        latex_block_2 = make_waste_distribution_table_block(cons_data.iloc[num_cols_1:], num_cols_2)\n",
    "        \n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table_1.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block_1)\n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table_2.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block_2)\n",
    "            \n",
    "    '''\n",
    "\n",
    "    text_block = r''''''\n",
    "\n",
    "    text_line_multi = r\"{%s}: This development has %s apartment units and %s stairhalls.\\\\\"\n",
    "\n",
    "    text_line_singular = r\"{%s}: This development has %s apartment units and one stairhall.\\\\\"\n",
    "\n",
    "    for row in cons_data.itertuples():\n",
    "\n",
    "        if int(row.STAIRHALLS) == 1:\n",
    "            text_block += text_line_singular % (clean_text(row.DEV_NAME.title()), int(row.CURRENT_APTS))\n",
    "        else:\n",
    "            text_block += text_line_multi % (clean_text(row.DEV_NAME.title()), int(row.CURRENT_APTS), int(row.STAIRHALLS))\n",
    "\n",
    "\n",
    "    with open(f'TEXT/waste_distribution_bottom/{cons_tds}_wd_bottom.tex', 'w') as file_handle:\n",
    "        file_handle.write(text_block)\n",
    "    \n",
    "    '''\n",
    "    with open(f'TEXT/waste_distribution_bottom/{cons_tds}_wd_bottom.tex', 'w') as file_handle:\n",
    "        file_handle.write('')  \n",
    "        \n",
    "    top_block_template = r'''\n",
    "    Quantifying how much waste is generated at each consolidation will inform how well current assets and services serve current needs, and what additional elements are necessary for each consolidation to operate as efficiently as possible.\n",
    "    \n",
    "    %s has %s 30-cubic yard exterior compactors. %s'''\n",
    "    \n",
    "    #wsa_data = load_wsa_data()\n",
    "    wsa_data = wsa_data.query(f\"TDS in {counts[cons_tds]['developments']}\")\n",
    "    extcomp_total = int(wsa_data['EXT_COMP_BE'].sum())\n",
    "    #print(cons_data['TRASH_ACTUAL_CY'])\n",
    "    #print(cons_data['TRASH_ACTUAL_CY'].sum())\n",
    "    #days_to_fill = extcomp_total*(cons_data['TRASH_ACTUAL_CY'].sum())/(30)\n",
    "    weight_df = cons_data[['TDS','DEV_NAME','TONS_PER_CONTAINER']].dropna()\n",
    "    \n",
    "    if weight_df.shape[0] > 0:\n",
    "        weight_at_collection = round(weight_df['TONS_PER_CONTAINER'].mean(),1)\n",
    "        weight_text = f'On average, the exterior compactors at this consolidation contain {weight_at_collection} tons of waste at the time of collection. DSNY prefers compactors to contain more than 7 tons and up to 12 tons at collection. The closer to 12 tons, the more efficient collection is for both DSNY and the consolidation.'\n",
    "    else:\n",
    "        weight_at_collection = None\n",
    "        weight_text = 'The average weight of DSNY collections at this consolidation are unknown.'\n",
    "    \n",
    "    top_block_data = []\n",
    "    top_block_data.append(clean_text(str(consolidations[cons_tds]['name']).title()))\n",
    "    \n",
    "    if extcomp_total > 0:\n",
    "        top_block_data.append(f'({str(extcomp_total)})')\n",
    "    else:\n",
    "        top_block_data.append('no')\n",
    "        \n",
    "    top_block_data.append(weight_text)\n",
    "    \n",
    "    with open(f'TEXT/waste_distribution_top/{cons_tds}_wd_top.tex', 'w') as file_handle:\n",
    "        file_handle.write(top_block_template % tuple(top_block_data))\n",
    "    \n",
    "    #print(top_block_data)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised by NaN\n"
     ]
    }
   ],
   "source": [
    "overview_data = add_waste_cols(load_overview_data())\n",
    "wsa_data = load_wsa_data()\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_waste_distribution_table(tds, overview_data, wsa_data)\n",
    "    except:\n",
    "        print(f'Exception raised by {tds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Capital Improvements Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_asset_data():\n",
    "    asset_data = {'fwd': ['In-Sink Food Grinders', pd.read_csv('DATA/capital_fwd.csv')],\n",
    "                  'ehd': ['Enlarged Hopper Doors', pd.read_csv('DATA/capital_ehd.csv')],\n",
    "                  'int_compactor':['Interior Compactor Replacement', pd.read_csv('DATA/capital_intcom.csv')],\n",
    "                  'wasteyard':['Waste Yard Redesign', pd.read_csv('DATA/capital_wasteyard.csv')]}\n",
    "\n",
    "    for value in asset_data.values():\n",
    "        value[1].columns = [item.strip() for item in value[1].columns]\n",
    "\n",
    "    asset_data['wasteyard'][1]['ESTIMATE'] = asset_data['wasteyard'][1]['TOT_EST']\n",
    "    asset_data['wasteyard'][1]['COST'] = np.nan\n",
    "    \n",
    "    def year_to_string(year):\n",
    "        if pd.isna(year):\n",
    "            return 'N/A'\n",
    "        else:\n",
    "            if int(year) <= 2022:\n",
    "                return str(int(year))\n",
    "            elif (int(year) > 2022) & (int(year) <= 2025):\n",
    "                return '2023-2025'\n",
    "            elif (int(year)>2025) and (int(year)<=2030):\n",
    "                return '2026-2030'\n",
    "            else:\n",
    "                return 'After 2030'\n",
    "    \n",
    "    asset_data['fwd'][1]['_YEAR'] = asset_data['fwd'][1]['EST_YEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['ehd'][1]['_YEAR'] = asset_data['ehd'][1]['CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['int_compactor'][1]['_YEAR'] = asset_data['int_compactor'][1]['CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['wasteyard'][1]['_YEAR'] = asset_data['wasteyard'][1]['CONS_CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    return asset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_capital_table(cons_tds, asset_data, overview_data=overview_data):\n",
    "    cons_data = overview_data[overview_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "\n",
    "    def make_capital_table_block(block_data, num_devs):\n",
    "        dev_col_format = r'X|'\n",
    "        header = r'''\n",
    "        \\begin{tabularx}{\\textwidth}{r|%s}\n",
    "        \\cline{2-%s}\n",
    "        ''' % ((dev_col_format*num_devs), num_devs)\n",
    "\n",
    "        top_row = r\"\\multicolumn{1}{l|}{}                                                        \"+r\"& \\cellcolor{ccorange}{\\color[HTML]{FFFFFF}%s} \"*num_devs+r\"\\\\ \\hline\"+\"\\n\"\n",
    "\n",
    "        project_block = r\"\\multicolumn{1}{|V{.2\\columnwidth}|}{\\cellcolor{ccorangelight}%s}          \"+(r\"&                                                                  \"*num_devs)+r\"\\\\\"+r'''\n",
    "        \\multicolumn{1}{|r|}{\\cellcolor{ccorangelight}\\textit{Status}}                '''+(r\"& %s                                                         \"*num_devs)+r'''\\\\\n",
    "        \\multicolumn{1}{|r|}{\\cellcolor{ccorangelight}\\textit{%s}}                  '''+(\"& %s                                                     \"*num_devs)+r\"\\\\ \\hline\"+\"\\n\"\n",
    "\n",
    "        devs = block_data['DEV_NAME'].apply(lambda x: str(x).upper()).tolist()\n",
    "        devs_title = block_data['DEV_NAME'].apply(lambda x: clean_text(str(x).title())).tolist()\n",
    "        latex_block = ''\n",
    "        latex_block += header\n",
    "        latex_block += top_row % tuple(block_data['DEV_NAME'].apply(lambda x: clean_text(str(x).title())).tolist())\n",
    "\n",
    "        for asset in asset_data.keys():\n",
    "            asset_df = asset_data[asset][1]\n",
    "            #print(asset_data[asset][0])\n",
    "            #print(devs)\n",
    "            #print(asset_df['DEVELOPMENT'].tolist())\n",
    "            if any((dev in asset_df['DEVELOPMENT'].tolist()) for dev in devs):\n",
    "                status_list = []\n",
    "                year_list = []\n",
    "\n",
    "                for dev in devs:\n",
    "                    if dev in asset_df['DEVELOPMENT'].tolist():\n",
    "                        #print(dev)\n",
    "                        if pd.isna(asset_df.loc[asset_df['DEVELOPMENT']== dev,'STATUS'].iloc[0]):\n",
    "                            status_list.append('Not Yet Scheduled')\n",
    "                        else:\n",
    "                            status_list.append(str(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'STATUS'].iloc[0]).title())\n",
    "\n",
    "                        #print(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'STATUS'])\n",
    "                        #print(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'COST'])\n",
    "                        #try:\n",
    "                        year_list.append(str(asset_df.loc[asset_df['DEVELOPMENT']== dev,'_YEAR'].iloc[0]))\n",
    "                    #except:\n",
    "                            #year_list.append('TBD')\n",
    "\n",
    "                    else:\n",
    "                        status_list.append('N/A')\n",
    "                        year_list.append(' ')\n",
    "\n",
    "                asset_block = project_block % tuple([asset_data[asset][0]]+status_list+['Year Planned']+year_list)\n",
    "\n",
    "                latex_block += asset_block\n",
    "\n",
    "        latex_block += r\"\\end{tabularx}\"\n",
    "\n",
    "        return latex_block\n",
    "    \n",
    "    \n",
    "    if num_devs <= 4:\n",
    "        num_cols = num_devs\n",
    "        block_data = cons_data\n",
    "        \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data, num_cols))\n",
    "        \n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)\n",
    "        block_data_1 = cons_data.iloc[0:num_cols_1]\n",
    "        block_data_2 = cons_data.iloc[num_cols_1:]\n",
    "        \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects_1.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data_1, num_cols_1))\n",
    "            \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects_2.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data_2, num_cols_2))\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_data= load_asset_data()\n",
    "overview_data = load_overview_data()\n",
    "for tds in consolidations.keys(): \n",
    "    make_capital_table(tds, asset_data, overview_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Staff Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_staff_data(name_dict):\n",
    "    #Read budgeted staff and formula allocation\n",
    "    dev_staff = pd.read_csv('DATA/staff_for_table.csv')\n",
    "    dev_staff.fillna(0,inplace=True)\n",
    "    \n",
    "    def find_cons_tds(name, name_dict):\n",
    "        for key, value in name_dict.items():\n",
    "            if (name == value['name']) | (name in value['alternates']):\n",
    "                return key\n",
    "            \n",
    "    dev_staff['CONS_TDS'] = dev_staff['Consolidation'].apply(lambda x: find_cons_tds(x, name_dict))\n",
    "    dev_staff['CONS_NAME'] = dev_staff['CONS_TDS'].apply(lambda x: name_dict[x]['name'] if x is not None else 'NO NAME FOUND')\n",
    "    #Note: Staff list missing for Armstrong, Ft. Washington, and Williams Plaza, as well as scatter-site third-party-managed consolidations\n",
    " \n",
    "    #Read budgeted staff and actuals\n",
    "    actuals_data = pd.read_csv('DATA/Staffing_Analysis/DEVHC.csv')\n",
    "    actuals_data.fillna(0, inplace=True)\n",
    "    actuals_data = actuals_data[actuals_data['RC Name'].apply(lambda x: \"total\" not in str(x).lower()) & actuals_data['Department'].apply(lambda x: \"total\" not in str(x).lower())]\n",
    "    actuals_data['CONS_TDS'] = actuals_data['RC Name'].apply(lambda x: find_cons_tds(x, name_dict))\n",
    "    actuals_data['CONS_NAME'] = actuals_data['CONS_TDS'].apply(lambda x: name_dict[x]['name'] if x is not None else 'NO NAME FOUND')\n",
    "\n",
    "    def convert_neg(x):\n",
    "        try:\n",
    "            return int(x)\n",
    "        except:\n",
    "            return int('-'+str(x).replace('(','').replace(')',''))\n",
    "\n",
    "    actuals_data['VARIANCE'] = actuals_data['Unnamed: 5'].apply(lambda x: convert_neg(x))\n",
    "    actuals_data['ACT'] = actuals_data['13']\n",
    "    \n",
    "    table_frame = pd.read_csv('DATA/Table_Keys.csv')\n",
    "    actuals_keys = pd.read_csv('DATA/Staffing_Analysis/DEVHC_CODES.csv')\n",
    "    \n",
    "    actuals_data = actuals_data.merge(actuals_keys, how='left', left_on='CST_NAME', right_on='TITLE_NAME')\n",
    "    for column in ['Current Modified', 'ACT', 'VARIANCE']:\n",
    "        actuals_data[column] = actuals_data[column].astype(int)\n",
    "        \n",
    "\n",
    "    \n",
    "    return (dev_staff, actuals_data, table_frame, actuals_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_staff_table(cons_tds, dev_staff, actuals_data, table_frame, actuals_keys):\n",
    "    #Fetching staff data for consolidation\n",
    "    cons_data = dev_staff.loc[dev_staff['CONS_TDS'] == cons_tds]\n",
    "    if cons_data.shape[0] == 0:\n",
    "        with open(f'TABLES/staff_table/{cons_tds}_staff_table.tex', 'w') as file_handle:\n",
    "            file_handle.write('')\n",
    "        return(f'Consolidation {cons_tds} not found in staffing data.')\n",
    "    #print(cons_tds)\n",
    "    #print(dev_staff)\n",
    "    \n",
    "    # Isolate and process actuals data for consolidation\n",
    "    try:\n",
    "        cons_actuals = actuals_data[actuals_data['CONS_TDS'] == cons_tds]\n",
    "    except:\n",
    "        print(f'{cons_tds} not found in actuals.')\n",
    "        return np.NaN\n",
    "    \n",
    "    cons_actuals = cons_actuals[['CONS_NAME', 'CONS_TDS', 'Current Modified', 'ACT', \n",
    "                                 'CODE_KEY', 'CODE_NAME']].groupby(by='CODE_KEY', as_index=False).agg({'CONS_NAME': 'first',\n",
    "                                                                                                       'CONS_TDS': 'first',\n",
    "                                                                                                     'Current Modified':sum,\n",
    "                                                                                                     'ACT':sum,\n",
    "                                                                                                     'CODE_NAME':'first'})\n",
    "    cons_actuals\n",
    "    cons_actuals.loc['Total']= cons_actuals.sum(numeric_only=True, axis=0)\n",
    "    cons_actuals.loc['Total','CODE_KEY'] = 11\n",
    "    cons_actuals.loc['Total','CODE_NAME'] = 'TOT'\n",
    "    \n",
    "    for row in cons_actuals.itertuples():\n",
    "        cons_data[f'{row.CODE_NAME}_ACT'] = row.ACT\n",
    "    #print(cons_data)\n",
    "    #Setting up table and transposing data\n",
    "    cons_table_frame = table_frame\n",
    "    cons_table_frame['Formula'] = cons_table_frame['FORMULA_KEY'].iloc[:-1].apply(lambda key: cons_data[key].iloc[0])\n",
    "    cons_table_frame['Budgeted'] = cons_table_frame['BUDG_KEY'].apply(lambda key: cons_data[key].iloc[0])\n",
    "    cons_table_frame['Actual'] = cons_table_frame['ACTUALS_KEY'].iloc[:-2].apply(lambda key: cons_data[key].iloc[0] if key in cons_data.columns else 0)\n",
    "\n",
    "    \n",
    "    #Simplifying table\n",
    "    cons_table = cons_table_frame[['CHART_LINE', 'Formula', 'Budgeted', 'Actual']]\n",
    "    #print(cons_table)\n",
    "    \n",
    "    #Defining LaTeX table format\n",
    "    \n",
    "    def make_staff_table_block(staff_data):\n",
    "    \n",
    "        table_template = r'''\n",
    "        \\begin{tabular}{l|c|c|c|}\n",
    "        \\cline{2-4}\n",
    "                                                                                     & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Formula Allocation \\tnote{1}} & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Budgeted} & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Actual Staff (June 2020)} \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Employees}                      & %s                                                      & %s                                                                & %s                                                        \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Property Manager}               & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Asst. Property Manager}         & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Secretaries}                    & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Housing Assistants}             & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Superintendent}                 & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Assistant Superintendent}       & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Supervisor of Caretakers (SOC)} & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Supervisor of Grounds (SOG)}    & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Maintenance Workers}            & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers X}                   & %s                                                      & %s                                                                &                                                       \\\\ \\cline{1-3}\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers J\\tnote{2}}                   &                                                       & %s                                                                &                                                         \\\\ \\cline{1-1} \\cline{3-3}\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers G}                   & \\multirow{-2}{*}{%s}                                                      & %s                                     & \\multirow{-3}{*}{%s \\tnote{3}}                           \\\\ \\hline\n",
    "        \\end{tabular}\n",
    "        \n",
    "        '''\n",
    "\n",
    "        values = []\n",
    "\n",
    "        def extract_data_through_mw(row):\n",
    "            [values.append(item) for item in [str(int(row['Formula'])), \n",
    "                                              str(int(row['Budgeted'])), \n",
    "                                              str(int(row['Actual']))]]\n",
    "            pass\n",
    "\n",
    "        #Processing through Maintenance Worker\n",
    "        staff_data.iloc[0:-3].apply(lambda row: extract_data_through_mw(row), axis=1)\n",
    "\n",
    "        #Processing Caretakers\n",
    "        values.append(str(int(staff_data.iloc[-3, 1])))\n",
    "        values.append(str(int(staff_data.iloc[-3, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-2, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-2, 1])))\n",
    "        values.append(str(int(staff_data.iloc[-1, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-3, 3])))\n",
    "\n",
    "        return table_template % tuple(values)\n",
    "    \n",
    "    #Make and export LaTeX code\n",
    "    with open(f'TABLES/staff_table/{cons_tds}_staff_table.tex', 'w') as file_handle:\n",
    "        file_handle.write(make_staff_table_block(cons_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staff_description_text():\n",
    "    text = get_docx_text('TEXT/WM_Role_Descriptions.docx')\n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "staff_data = load_staff_data(consolidations)\n",
    "#staff_descriptions = load_staff_description_text()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    make_staff_table(tds, *staff_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Analysis Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process File Names\n",
    "def process_analysis_graphic_paths():\n",
    "    \n",
    "    def clean_paths(path_list):\n",
    "        clean_path_list = []\n",
    "        for path in path_list:\n",
    "            if (' ' in path.split('/')[-1]) or ('&' in path.split('/')[-1]):\n",
    "                os.rename(path, path.replace(' ','-').replace('&','-'))\n",
    "                clean_path_list.append(path)\n",
    "            else:\n",
    "                clean_path_list.append(path)\n",
    "            \n",
    "        return clean_path_list\n",
    "            \n",
    "    cons_bar_charts_raw = list(glob.glob('WORK_ORDER_ANALYSIS/Consolidation_BarCharts/png/*'))\n",
    "    cons_bar_charts = clean_paths(cons_bar_charts_raw)\n",
    "    \n",
    "    tds_nums = [path.split('/')[-1].split('_')[0].zfill(3) for path in cons_bar_charts]\n",
    "\n",
    "    cons_chart_paths = {}\n",
    "    for pair in list(zip(tds_nums, cons_bar_charts)):\n",
    "        cons_chart_paths[pair[0]] = pair[1]\n",
    "    \n",
    "    dev_chart_paths = {}\n",
    "\n",
    "    dev_bar_charts_raw = glob.glob('WORK_ORDER_ANALYSIS/Development_BarCharts/png/*')\n",
    "    dev_bar_charts = clean_paths(dev_bar_charts_raw)\n",
    "    \n",
    "    dev_tds_nums = [path.split('/')[-1].split('_')[0].zfill(3) for path in dev_bar_charts]\n",
    "\n",
    "    for pair in list(zip(dev_tds_nums, dev_bar_charts)):\n",
    "        dev_chart_paths[pair[0]] = {'Development_BarCharts': pair[1]}\n",
    "\n",
    "    for directory in ['Dev_Interior_Comp_Repair_BarCharts', 'Dev_Exterior_Comp_Repair_BarCharts']:\n",
    "        paths_raw = glob.glob(f'WORK_ORDER_ANALYSIS/{directory}/png/*')\n",
    "        paths = clean_paths(paths_raw)\n",
    "        tds_nums = [path.split('/')[-1].split('_')[3].zfill(3) for path in paths]\n",
    "\n",
    "        for pair in list(zip(tds_nums, paths)):\n",
    "            try:\n",
    "                dev_chart_paths[pair[0]][directory] = pair[1]\n",
    "            except:\n",
    "                dev_chart_paths[pair[0]] = {directory: pair[1]}\n",
    "                \n",
    "    return (cons_chart_paths, dev_chart_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_layout(tds, cons_chart_paths, dev_chart_paths, cons_dict):\n",
    "    analysis_image_layout = r''''''\n",
    "    \n",
    "    cons_devs = cons_dict[tds]['developments']\n",
    "    \n",
    "    dev_bar_paths = []\n",
    "    dev_int_paths = []\n",
    "    dev_ext_paths = []\n",
    "    \n",
    "    for dev in cons_devs:\n",
    "        try:\n",
    "            dev_bar_paths.append(dev_chart_paths[dev]['Development_BarCharts'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            dev_int_paths.append(dev_chart_paths[dev]['Dev_Interior_Comp_Repair_BarCharts'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            dev_ext_paths.append(dev_chart_paths[dev]['Dev_Exterior_Comp_Repair_BarCharts'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #Adding consolidation and development bar charts\n",
    "    bar_charts_heading = r'''\\begin{center}\n",
    "                                \\tablehead{\\hspace{1cm}\\\\}\n",
    "                                \\tabletail{\\hspace{1cm}\\\\}\n",
    "                                \\begin{supertabular}{p{0.5\\textwidth}p{0.5\\textwidth}}\n",
    "                                \\shrinkheight{1in}\n",
    "                                \\multicolumn{2}{p{\\textwidth}}{The following bar charts show how frequently various types of maintenance issue -- including compactor-related problems, pest problems, and plumbing issues -- occur in compactor locations consolidation-wide as well as at major developments.} \\\\\n",
    "                                \\multicolumn{2}{c}{\\includegraphics[width=0.6\\textwidth]{\\rootpath/'''+cons_chart_paths[tds]+r'''}} \\\\\n",
    "                                '''\n",
    "    \n",
    "    if len(dev_bar_paths) > 1:\n",
    "        analysis_image_layout += bar_charts_heading\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(dev_bar_paths):\n",
    "            if (len(dev_bar_paths)-i) >= 2:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_bar_paths[i]+r'''} & \\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_bar_paths[i+1]+r'''} \\\\\n",
    "                                        '''\n",
    "            else:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_bar_paths[i]+r'''} &  \\hspace{1cm} \\\\\n",
    "                                        '''\n",
    "            \n",
    "            i += 2\n",
    "        analysis_image_layout += r'\\end{supertabular}'+'\\n'+r'\\end{center}'+'\\n'\n",
    "        \n",
    "    elif len(dev_bar_paths) == 1:\n",
    "        analysis_image_layout += bar_charts_heading\n",
    "        analysis_image_layout += r'''\\multicolumn{2}{c}{\\includegraphics[width=0.6\\textwidth]{\\rootpath/'''+dev_bar_paths[0]+r'''}} \\\\\n",
    "                                    \\end{supertabular}\n",
    "                                    \\end{center}\n",
    "                                    '''\n",
    "    else:\n",
    "        analysis_image_layout += bar_charts_heading.replace(' as well as at major developments','').replace('bar charts show','bar chart shows')\n",
    "        analysis_image_layout += r'\\end{supertabular}'+'\\n'+r'\\end{center}'+'\\n'\n",
    "        \n",
    "    \n",
    "    #Adding interior compactor section, including tables\n",
    "    int_comp_heading = r'''\n",
    "                        \\begin{center}\n",
    "                        \\tablehead{\\hspace{1cm}\\\\}\n",
    "                        \\tabletail{\\hspace{1cm}\\\\}\n",
    "                        \\begin{supertabular}{p{0.5\\textwidth}p{0.5\\textwidth}}\n",
    "                        \\multicolumn{2}{p{\\textwidth}}{The following figures highlight repairs conducted in interior compactor locations at each major development, as well as within up to five buildings at each development.} \\\\\n",
    "                        '''\n",
    "    if len(dev_int_paths) > 1:\n",
    "        analysis_image_layout += int_comp_heading\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(dev_int_paths):\n",
    "            if (len(dev_int_paths)-i) >= 2:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_int_paths[i]+r'''} & \\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_int_paths[i+1]+r'''} \\\\\n",
    "                                        '''\n",
    "            else:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_int_paths[i]+r'''} &  \\hspace{1cm} \\\\\n",
    "                                        '''\n",
    "            i += 2\n",
    "            \n",
    "        analysis_image_layout += r'\\multicolumn{2}{c}{\\input{\\rootpath/WORK_ORDER_ANALYSIS/Dev_Interior_Comp_Repair_Tables/\\tds_repair_table}} \\\\'+'\\n'\n",
    "        analysis_image_layout += r'\\end{supertabular}'+'\\n'+r'\\end{center}'+'\\n'\n",
    "        \n",
    "    elif len(dev_int_paths) == 1:\n",
    "        analysis_image_layout += int_comp_heading\n",
    "        analysis_image_layout += r'''\\multicolumn{2}{c}{\\includegraphics[width=0.6\\textwidth]{\\rootpath/'''+dev_int_paths[0]+r'''}} \\\\\n",
    "                                    \\multicolumn{2}{c}{\\input{\\rootpath/WORK_ORDER_ANALYSIS/Dev_Interior_Comp_Repair_Tables/\\tds_repair_table}} \\\\\n",
    "                                    \\end{supertabular}\n",
    "                                    \\end{center}\n",
    "                                    '''\n",
    "    else:\n",
    "        analysis_image_layout += int_comp_heading.replace('at each major development, as well as within','in').replace('figures highlight','tables highlight')\n",
    "        analysis_image_layout += r'\\multicolumn{2}{c}{\\input{\\rootpath/WORK_ORDER_ANALYSIS/Dev_Interior_Comp_Repair_Tables/\\tds_repair_table}} \\\\'+'\\n'\n",
    "        analysis_image_layout += r'''\\end{supertabular}\n",
    "                                    \\end{center}\n",
    "                                    '''\n",
    "    \n",
    "    #Adding exterior compactor charts\n",
    "    ext_comp_heading = r'''\n",
    "                        \\begin{center}\n",
    "                        \\tablehead{\\hspace{1cm}\\\\}\n",
    "                        \\tabletail{\\hspace{1cm}\\\\}\n",
    "                        \\begin{supertabular}{p{0.5\\textwidth}p{0.5\\textwidth}}\n",
    "                        \\multicolumn{2}{p{\\textwidth}}{The following charts examine repairs made at exterior compactor locations at major developments.} \\\\\n",
    "                        '''\n",
    "    if len(dev_ext_paths) > 1:\n",
    "        analysis_image_layout += ext_comp_heading\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(dev_ext_paths):\n",
    "            if (len(dev_ext_paths)-i) >= 2:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_ext_paths[i]+r'''} & \\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_ext_paths[i+1]+r'''} \\\\\n",
    "                                        '''\n",
    "            else:\n",
    "                analysis_image_layout += r'''\\includegraphics[width=0.45\\textwidth]{\\rootpath/'''+dev_ext_paths[i]+r'''} &  \\hspace{1cm} \\\\\n",
    "                                        '''\n",
    "            \n",
    "            i += 2\n",
    "        analysis_image_layout += r'\\end{supertabular}'+'\\n'+r'\\end{center}'+'\\n'\n",
    "        \n",
    "    elif len(dev_ext_paths) == 1:\n",
    "        analysis_image_layout += ext_comp_heading.replace('charts examine', 'chart examines').replace(' at major developments','')\n",
    "        analysis_image_layout += r'''\\multicolumn{2}{c}{\\includegraphics[width=0.6\\textwidth]{\\rootpath/'''+dev_ext_paths[0]+r'''}} \\\\\n",
    "                                    \\end{supertabular}\n",
    "                                    \\end{center}\n",
    "                                    '''\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    with open(f'WORK_ORDER_ANALYSIS/image_layouts/{tds}_layout.tex', 'w') as file_handle:\n",
    "        file_handle.write(analysis_image_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 raised exception\n",
      "128 raised exception\n",
      "NaN raised exception\n"
     ]
    }
   ],
   "source": [
    "cons_chart_paths, dev_chart_paths = process_analysis_graphic_paths()\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_image_layout(tds, cons_chart_paths, dev_chart_paths, consolidations)\n",
    "    except:\n",
    "        print(f'{tds} raised exception')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Site Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_site_plans():\n",
    "    plan_format = re.compile(r'[A-z0-9\\s]+_[0-9]{3}_[A-z\\s]+.pdf')\n",
    "    site_plan_pdf_candidates = glob.glob('APPENDICES/site_plans/*/*/*.*', recursive=True)\n",
    "    site_plan_pdfs = [path for path in site_plan_pdf_candidates if bool(plan_format.match(path.split('/')[-1]))]\n",
    "\n",
    "    for f in site_plan_pdf_candidates:\n",
    "        if f not in site_plan_pdfs:\n",
    "            os.remove(f)\n",
    "        else:\n",
    "            tds = f.split('/')[-1].split('_')[1]\n",
    "            try:\n",
    "                file = convert_from_path(f, dpi=300, single_file=True)[0]\n",
    "                path = f'APPENDICES/site_plans/{tds}.png'\n",
    "                height = file.height\n",
    "                width = file.width\n",
    "                if file.width > file.height:\n",
    "                    file = file.rotate(90, expand=True)#.resize((height, width))\n",
    "                if file.height > 3300:\n",
    "                    scale_ratio = 3300/float(file.height)\n",
    "                    new_height = 3300\n",
    "                    new_width = int(float(file.width)*scale_ratio)\n",
    "                    file = file.resize((new_width, new_height))\n",
    "                file.save(path, 'PNG')\n",
    "                #print(f\"saved {tds}\")\n",
    "            except:\n",
    "                print(f'{tds} raised exception')\n",
    "                \n",
    "    site_plans = glob.glob('APPENDICES/site_plans/*.png')\n",
    "    dev_list = [path.split('/')[-1].split('.')[0] for path in site_plans]\n",
    "    for cons, content in consolidations.items():\n",
    "        img_list = []\n",
    "        pdf_filename = f'APPENDICES/site_plans/{cons}.pdf'\n",
    "        for dev in content['developments']:\n",
    "            if dev in dev_list:\n",
    "                img_list.append(Image.open(f'APPENDICES/site_plans/{dev}.png'))\n",
    "        if len(img_list)==1:\n",
    "            img_list[0].save(pdf_filename, 'PDF', resolution=300.0)\n",
    "        elif len(img_list)>1:\n",
    "            img_list[0].save(pdf_filename, 'PDF', resolution=300.0, save_all=True, append_images=img_list[1:])\n",
    "        else:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN raised exception\n"
     ]
    }
   ],
   "source": [
    "site_plans = glob.glob('APPENDICES/site_plans/*.png')\n",
    "dev_list = [path.split('/')[-1].split('.')[0] for path in site_plans]\n",
    "for cons, content in consolidations.items():\n",
    "    img_list = []\n",
    "    pdf_filename = f'APPENDICES/site_plans/{cons}.pdf'\n",
    "    try:\n",
    "        for dev in content['developments']:\n",
    "            if dev in dev_list:\n",
    "                img_list.append(Image.open(f'APPENDICES/site_plans/{dev}.png'))\n",
    "    except:\n",
    "        print(f'{cons} raised exception')\n",
    "    if len(img_list)==1:\n",
    "        img_list[0].save(pdf_filename, 'PDF', resolution=300.0)\n",
    "    elif len(img_list)>1:\n",
    "        img_list[0].save(pdf_filename, 'PDF', resolution=300.0, save_all=True, append_images=img_list[1:])\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Floorplans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_plan_paths = glob.glob('APPENDICES/floorplans/*')\n",
    "\n",
    "candidate_list = []\n",
    "for key, value in developments.items():\n",
    "    candidate_list.append(str(value['name']).upper())\n",
    "    for item in value['name_alternates']:\n",
    "        candidate_list.append(item.upper())\n",
    "\n",
    "def get_dev_name(name):\n",
    "    match = process.extractOne(str(name).upper(), candidate_list)[0]\n",
    "    #print(match)\n",
    "    for key, value in developments.items():\n",
    "        if (match.upper() == value['name'].upper()) or (match.upper() in [val.upper() for val in value['name_alternates']]):\n",
    "            return value['name']\n",
    "\n",
    "    return '!!!NOT FOUND'\n",
    "\n",
    "\n",
    "def get_tds_from_name(x):\n",
    "    for key, value in developments.items():\n",
    "        if x == value['name']:\n",
    "            return key\n",
    "        \n",
    "    return 'N/A'\n",
    "\n",
    "floor_plan_names = [get_dev_name(path.split('/')[-1].replace('.pdf','')) for path in floor_plan_paths]\n",
    "floor_plan_tds = [get_tds_from_name(name) for name in floor_plan_names]\n",
    "\n",
    "floor_plans = pd.DataFrame(data=list(zip(floor_plan_tds, floor_plan_names, floor_plan_paths)), columns=['TDS', 'NAME', 'PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "floor_plans.to_csv('APPENDICES/floorplans/floor_plans_for_screening.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making and Compiling LaTeX Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_latex_file(tds, counts=counts, spreadlist=None, no_sections=False):\n",
    "    #SET UTILITY PATHS HERE\n",
    "    pdflatex_path = '/usr/local/texlive/2018/bin/x86_64-darwin/pdflatex'\n",
    "    ghostscript_path = '/usr/local/bin/gs'\n",
    "    pdfjam_path = '/usr/local/texlive/2018/texmf-dist/scripts/pdfjam/pdfjam'\n",
    "    if no_sections == False:\n",
    "        if counts[tds]['count'] <= 4:\n",
    "            with open('REPORT_TEMPLATE/report.tex', 'r') as file_handle:\n",
    "                text = file_handle.read()\n",
    "\n",
    "            new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "            with open(f'REPORTS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "                outfile.write(new_text)\n",
    "\n",
    "        else:\n",
    "            with open('REPORT_TEMPLATE/report_long.tex', 'r') as file_handle:\n",
    "                text = file_handle.read()\n",
    "\n",
    "            new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "            with open(f'REPORTS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "                outfile.write(new_text)\n",
    "\n",
    "        subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/LaTeX', f'REPORTS/LaTeX/{tds}_report.tex'])\n",
    "        subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/LaTeX', f'REPORTS/LaTeX/{tds}_report.tex'])\n",
    "\n",
    "        #Be sure to install ghostscript (to compress pdfs), or comment out next line. Available via homebrew.\n",
    "        subprocess.check_call([ghostscript_path, '-sDEVICE=pdfwrite', '-dCompatibilityLevel=1.5', '-dNOPAUSE', '-dQUIET', '-dBATCH', f'-sOutputFile=REPORTS/{tds}_report.pdf', f'REPORTS/LaTeX/{tds}_report.pdf'])\n",
    "        #if tds in spreadlist:\n",
    "            #handle = subprocess.Popen([pdfjam_path, '--nup 2x1', \"--openright 'true'\", f'REPORTS/{tds}_report.pdf', f'--outfile REPORTS/{tds}_spread.pdf', '--landscape', '--no-tidy'], stdout=subprocess.PIPE,stderr=subprocess.PIPE) \n",
    "            #err = handle.communicate() \n",
    "            #print(err)\n",
    "            #subprocess.check_call([pdfjam_path, '--nup 2x1', \"--openright 'true'\", \"--frame 'true'\", f'REPORTS/{tds}_report.pdf', f'--outfile {tds}_spread.pdf', '--landscape'])\n",
    "    else:\n",
    "        if counts[tds]['count'] <= 4:\n",
    "            with open('REPORT_TEMPLATE/report_nosec.tex', 'r') as file_handle:\n",
    "                text = file_handle.read()\n",
    "\n",
    "            new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "            with open(f'REPORTS/NO_SECTIONS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "                outfile.write(new_text)\n",
    "\n",
    "        else:\n",
    "            with open('REPORT_TEMPLATE/report_long_nosec.tex', 'r') as file_handle:\n",
    "                text = file_handle.read()\n",
    "\n",
    "            new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "            with open(f'REPORTS/NO_SECTIONS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "                outfile.write(new_text)\n",
    "\n",
    "        subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/NO_SECTIONS/LaTeX', f'REPORTS/NO_SECTIONS/LaTeX/{tds}_report.tex'])\n",
    "        subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/NO_SECTIONS/LaTeX', f'REPORTS/NO_SECTIONS/LaTeX/{tds}_report.tex'])\n",
    "\n",
    "        #Be sure to install ghostscript (to compress pdfs), or comment out next line. Available via homebrew.\n",
    "        subprocess.check_call([ghostscript_path, '-sDEVICE=pdfwrite', '-dCompatibilityLevel=1.5', '-dNOPAUSE', '-dQUIET', '-dBATCH', f'-sOutputFile=REPORTS/NO_SECTIONS/{tds}_report.pdf', f'REPORTS/NO_SECTIONS/LaTeX/{tds}_report.pdf'])\n",
    "        #if tds in spreadlist:\n",
    "            #handle = subprocess.Popen([pdfjam_path, '--nup 2x1', \"--openright 'true'\", f'REPORTS/{tds}_report.pdf', f'--outfile REPORTS/{tds}_spread.pdf', '--landscape', '--no-tidy'], stdout=subprocess.PIPE,stderr=subprocess.PIPE) \n",
    "            #err = handle.communicate() \n",
    "            #print(err)\n",
    "\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('REPORTS'):\n",
    "    os.makedirs('REPORTS')\n",
    "\n",
    "if not os.path.exists('REPORTS/LaTeX'):\n",
    "    os.makedirs('REPORTS/LaTeX')\n",
    "\n",
    "os.system(\"cp REPORT_TEMPLATE/content.tex REPORTS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/preface.tex REPORTS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/content_long.tex REPORTS/LaTeX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 raised exception: <class 'subprocess.CalledProcessError'>\n",
      "210 raised exception: <class 'subprocess.CalledProcessError'>\n",
      "128 raised exception: <class 'subprocess.CalledProcessError'>\n",
      "NaN raised exception: <class 'KeyError'>\n"
     ]
    }
   ],
   "source": [
    "consolidation_list = consolidations.keys()\n",
    "#consolidation_list = ['073','127','067','003']\n",
    "error_list = []\n",
    "for tds in consolidation_list:\n",
    "    try:\n",
    "        compile_latex_file(tds, spreadlist=consolidation_list)\n",
    "    except:\n",
    "        print(f'{tds} raised exception: {sys.exc_info()[0]}')\n",
    "        error_list.append(tds)\n",
    "\n",
    "filelist = [f for f in os.listdir('REPORTS/LaTeX') if not f.endswith(\".tex\")]\n",
    "save_files = [f'{tds}_report.log' for tds in error_list]\n",
    "\n",
    "for f in filelist:\n",
    "    if f not in save_files:\n",
    "        os.remove(os.path.join('REPORTS/LaTeX', f))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdfjam --nup 2x1 --openright 'true' --frame 'true' REPORTS/tds_report.pdf --outfile tds_spread.pdf --landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('REPORTS/NO_SECTIONS'):\n",
    "    os.makedirs('REPORTS/NO_SECTIONS')\n",
    "\n",
    "if not os.path.exists('REPORTS/NO_SECTIONS/LaTeX'):\n",
    "    os.makedirs('REPORTS/NO_SECTIONS/LaTeX')\n",
    "\n",
    "os.system(\"cp REPORT_TEMPLATE/content.tex REPORTS/NO_SECTIONS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/preface.tex REPORTS/NO_SECTIONS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/content_long.tex REPORTS/NO_SECTIONS/LaTeX\")\n",
    "\n",
    "\n",
    "consolidation_list = consolidations.keys()\n",
    "#consolidation_list = ['073','127','067','003']\n",
    "error_list = []\n",
    "for tds in consolidation_list:\n",
    "    try:\n",
    "        compile_latex_file(tds, spreadlist=consolidation_list, no_section=True)\n",
    "    except:\n",
    "        print(f'{tds} raised exception: {sys.exc_info()[0]}')\n",
    "        error_list.append(tds)\n",
    "\n",
    "filelist = [f for f in os.listdir('REPORTS/NO_SECTIONS/LaTeX') if not f.endswith(\".tex\")]\n",
    "save_files = [f'{tds}_report.log' for tds in error_list]\n",
    "\n",
    "for f in filelist:\n",
    "    if f not in save_files:\n",
    "        os.remove(os.path.join('REPORTS/NO_SECTIONS/LaTeX', f))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
