{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaTeX Automation for NYCHA Waste Individual Action Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import subprocess\n",
    "import shutil\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Global Vars and Options\n",
    "os.chdir('/Users/kyleslugg/Documents/NYCHA/Production')\n",
    "cons_tds = '073'\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standardized_names():\n",
    "    databook_names = pd.read_csv('DATA/name_tables/dev_data_book_name.csv')\n",
    "    databook_names['CONS_TDS'] = databook_names['CONS_TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    databook_names['TDS'] = databook_names['TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "\n",
    "    staff_names = pd.read_csv('DATA/name_tables/staff_cons_name.csv')\n",
    "    staff_names['RC_Name'] = staff_names['RC Name']\n",
    "    \n",
    "    consolidations = {}\n",
    "    developments = {}\n",
    "\n",
    "    for row in databook_names.itertuples():\n",
    "        consolidations[row.CONS_TDS] = {'name':row.CONS_NAME, 'alternates':[row.MANAGED_BY]}\n",
    "        developments[row.TDS] = {'name':row.DEV_NAME, 'name_alternates':[], 'cons_tds':row.CONS_TDS}\n",
    "        \n",
    "    def find_closest_fuzzy_match(name, comp_df, comp_col_name, return_col_name):\n",
    "        values = comp_df[comp_col_name].unique()\n",
    "        comp_df_copy = pd.DataFrame(data=values, index=[i for i in range(0,len(values))], columns=[comp_col_name])\n",
    "\n",
    "        '''\n",
    "        def strip_name(x):\n",
    "            string = str(x).lower()\n",
    "            string = string.replace('consolidated','')\n",
    "            string = string.replace('consolidation', '')\n",
    "            string = string.replace('houses', '')\n",
    "            return string\n",
    "\n",
    "        comp_df_copy['partial_ratio'] = comp_df_copy[comp_col_name].apply(lambda x: fuzz.partial_ratio(strip_name(name), strip_name(x)))\n",
    "        highest_match = comp_df_copy['partial_ratio'].max()\n",
    "\n",
    "        matches = comp_df_copy.loc[comp_df_copy['partial_ratio']==highest_match, 'CONS_NAME']\n",
    "\n",
    "        if matches.shape[0] == 1:\n",
    "            return matches.iloc[0]\n",
    "        else:\n",
    "            print(matches)\n",
    "            return 'ZZZ MULTIPLE MATCHES FOUND'\n",
    "\n",
    "        '''\n",
    "        return process.extractOne(str(name).lower(), values.tolist())[0]\n",
    "    \n",
    "    staff_names['NAME_MATCH'] = staff_names['RC Name'].apply(lambda x: find_closest_fuzzy_match(x, databook_names, 'CONS_NAME', 'CONS_NAME'))\n",
    "\n",
    "    match_corrections = {'Justice Sonia Sotomayor  Consolidated': 'SOTOMAYOR HOUSES CONSOLIDATED',\n",
    "                        'Murphy Consolidated': 'ZZZUNKNOWN'\n",
    "                        }\n",
    "\n",
    "    def make_corrections(row, index_col, data_col, dictionary):\n",
    "        if str(row[index_col]).strip() in dictionary.keys():\n",
    "            return dictionary[row[index_col]]\n",
    "        else:\n",
    "            return row[data_col]\n",
    "\n",
    "    staff_names['AMENDED_MATCHES'] = staff_names.apply(lambda row: make_corrections(row, 'RC Name', 'NAME_MATCH', match_corrections), axis=1)\n",
    "\n",
    "    staff_names = staff_names.merge(databook_names[['CONS_NAME', 'CONS_TDS']], left_on='AMENDED_MATCHES', right_on='CONS_NAME', how='left')\n",
    "\n",
    "    for row in staff_names.itertuples():\n",
    "        try:\n",
    "            consolidations[row.CONS_TDS]['alternates'].append(row.RC_Name)\n",
    "        except:\n",
    "            print(f'TDS #{row.CONS_TDS} raised an exception.')\n",
    "    \n",
    "    return(consolidations, developments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDS #nan raised an exception.\n"
     ]
    }
   ],
   "source": [
    "consolidations, developments = get_standardized_names()\n",
    "\n",
    "counts ={}\n",
    "for key, value in developments.items():\n",
    "    if value['cons_tds'] not in counts.keys():\n",
    "        counts[value['cons_tds']] = {'developments':[key],\n",
    "                             'count':1}\n",
    "    else:\n",
    "        counts[value['cons_tds']]['developments'].append(key)\n",
    "        counts[value['cons_tds']]['count']+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['167', '359', '091', '530', '127']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_list = [value['count'] for key, value in counts.items()]\n",
    "high_count_cons = [key for key, value in counts.items() if value['count']>=8]\n",
    "high_count_cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_overview_data():#Load Data\n",
    "    overview_data = pd.read_csv('DATA/overview_table_data.csv')\n",
    "    overview_data['CONS_TDS'] = overview_data['CONS_TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    overview_data['TDS'] = overview_data['TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    \n",
    "    return overview_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_data = load_overview_data()\n",
    "cons_list = overview_data['CONS_TDS'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and Process Text Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO COME:\n",
    "- What Is an IAP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lightly modified version of example found at http://etienned.github.io/posts/extract-text-from-word-docx-simply/\n",
    "\n",
    "try:\n",
    "    from xml.etree.cElementTree import XML\n",
    "except ImportError:\n",
    "    from xml.etree.ElementTree import XML\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Module that extract text from MS XML Word document (.docx).\n",
    "(Inspired by python-docx <https://github.com/mikemaccana/python-docx>)\n",
    "\"\"\"\n",
    "\n",
    "WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "PARA = WORD_NAMESPACE + 'p'\n",
    "TEXT = WORD_NAMESPACE + 't'\n",
    "\n",
    "\n",
    "def get_docx_text(path):\n",
    "    \"\"\"\n",
    "    Take the path of a docx file as argument, return the text in unicode.\n",
    "    \"\"\"\n",
    "    document = zipfile.ZipFile(path)\n",
    "    try:\n",
    "        xml_content = document.read('word/document.xml')\n",
    "    except:\n",
    "        xml_content = document.read('word/document2.xml')\n",
    "        \n",
    "    document.close()\n",
    "    tree = XML(xml_content)\n",
    "\n",
    "    paragraphs = []\n",
    "    for paragraph in tree.getiterator(PARA):\n",
    "        texts = [node.text\n",
    "                 for node in paragraph.getiterator(TEXT)\n",
    "                 if node.text]\n",
    "        if texts:\n",
    "            paragraphs.append(''.join(texts))\n",
    "\n",
    "    return '\\n\\n'.join(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character Substitutions for LaTeX -- set and define \"clean\" method\n",
    "def clean_text(text):\n",
    "    substitutions = {'“':\"``\",\n",
    "                '”': \"''\",\n",
    "                '’':\"'\",\n",
    "                ' ':' ',\n",
    "                '–':'--',\n",
    "                ' ':' ',\n",
    "                '\\xa0':' '}\n",
    "    \n",
    "    for key, value in substitutions.items():\n",
    "        text = text.replace(key, value)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preface -- What is an IAP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preface_text(cons_tds):\n",
    "    #staff_names = ...make and load csv of staff names.\n",
    "    \n",
    "    latex_block = '''\\chapter{Preface}\n",
    "\n",
    "    \\section{Letter from the Chair}\\label{sec:Section1}\n",
    "    \\clearpage\n",
    "    {\\fontfamily{phv}\\selectfont\n",
    "    \\section{What is an Individual Action Plan?}\n",
    "\n",
    "    The Individual Action Plans (IAPs) were born out of the collaboration between Capital Planning, Strategic Planning, Operations, and the Federal Monitor during the Fall of 2019. For years, NYCHA residents have faced waste-strewn campuses caused by insufficient staffing and equipment. The waste situation on our properties is not only an issue of poor sanitation and safety but also of human dignity -- everyone deserves a home they can feel proud of that is not covered with litter. It is also important to highlight that improperly handled waste is a leading non-point source pollutant contributing to the degradation of our waterways and harming the natural environment. We want the IAPs to be a stepping-stone towards project-based property management as no two consolidations are the same. \n",
    "\n",
    "    We have three main goals for the IAPs: \n",
    "    \\begin{enumerate}\n",
    "    \\item We hope that the IAPs will empower the consolidation staff who run developments to better coordinate and communicate with Central Office by having the proper resources. \n",
    "    \\item We want the IAPs to serve as an educational tool for all stakeholders to understand the complex system of waste management at consolidations. \n",
    "    \\item We aim to use these plans to understand and learn from the changing assets and flows at each consolidation to make life cleaner, safer, healthier, and happier for our NYCHA residents and employees. \n",
    "    \\end{enumerate}\n",
    "\n",
    "    The IAP is a living, breathing document that will be modified as information and data change. We strive to create the most transparent and accurate IAP as possible, but there is room for error, and we cannot guarantee that all information is correct at this point in the process. That is why this document will be updated every quarter, and in each iteration, the goal is to create a more robust IAP. The IAPs will be printed out and distributed to each consolidation via mail. They will be available for all staff at the Property Managers office. They will also be made available digitally. Please feel free to contact us if you think there has been a mistake or information needs updating, and we will act accordingly. \n",
    "\n",
    "\n",
    "    Please feel free to contact Jane Doe with any questions or concerns at: jane.doe@nycha.nyc.gov\n",
    "\n",
    "    Below is a most up-to-date list of %s Management Personnel:\n",
    "    \\begin{itemize}\n",
    "    \\item Operations VP: %s\n",
    "    \\item %s Borough Director: %s\n",
    "    \\item Regional Asset Manager: %s\n",
    "    \\item Property Manager: %s\n",
    "    \\item Superintendent: %s\n",
    "    \\end{itemize}\n",
    "    }'''\n",
    "    \n",
    "    #data = [cons name, ops vp, borough name, borough dir, RAM, PM, super]\n",
    "    preface_data = []\n",
    "    preface_data.append(str(consolidations[cons_tds]['name']).title())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overview_text(cons_tds):\n",
    "    header = re.compile(r'((\\w*\\s)*(Overview)):')\n",
    "    \n",
    "    overview_text = get_docx_text(f'TEXT/overview_text/{cons_tds}_Overview.docx')\n",
    "    if len(header.findall(overview_text)) == 0:\n",
    "            overview_text = overview_text\n",
    "    else:\n",
    "        try:\n",
    "            overview_text = overview_text.replace(header.findall(overview_text)[0],'')\n",
    "        except:\n",
    "            overview_text = overview_text.replace(header.findall(overview_text)[0][0],'')\n",
    "    \n",
    "    overview_text = clean_text(overview_text)\n",
    "    \n",
    "    with open(f'TEXT/overview_text/{cons_tds}_overview.tex', 'w') as file_handle:\n",
    "        file_handle.write(overview_text)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_overview_text(tds)\n",
    "    #except KeyError:\n",
    "        #pass\n",
    "    except FileNotFoundError:\n",
    "            pass\n",
    "    except:\n",
    "        print(f\"{tds} raised error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_analysis_text(cons_tds):\n",
    "    analysis_text = get_docx_text(f'TEXT/analysis_text/{cons_tds}_Analysis.docx')\n",
    "\n",
    "    header = re.compile(r'([\\w\\s]*:)')\n",
    "    \n",
    "    analysis_text = clean_text(analysis_text)\n",
    "\n",
    "    section_headings = {'Inspection and Collection Requirement':['Inspection and Collection Requirements',\n",
    "                                                                 'Inspection and Collection Requirement',\n",
    "                                                                 'Collection and Inspection Requirements',\n",
    "                                                                'Collection and Inspection Requirement'],\n",
    "                        'Removal or Storage Requirement':['Removal or Storage Requirements',\n",
    "                                                          'Removal or Storage Requirement',\n",
    "                                                          'Removal and Storage Requirements',\n",
    "                                                         'Removal and Storage Requirement',\n",
    "                                                         'Storage or Removal Requirement',\n",
    "                                                          'Storage and Removal Requirements',\n",
    "                                                         'Storage and Removal Requirement',\n",
    "                                                         'Removal or Storage Requirement '],\n",
    "                       'Additional Context':['Additional Context']}\n",
    "    \n",
    "    for heading, variants in section_headings.items():\n",
    "        for variant in variants:\n",
    "            if variant in analysis_text:\n",
    "                analysis_text = analysis_text.replace(variant, r'\\textbf{%s}' % (heading))\n",
    "                break\n",
    "\n",
    "    if len(header.findall(analysis_text)) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        analysis_text = analysis_text.replace(header.findall(analysis_text)[0]+'\\n','')\n",
    "\n",
    "    latex_block = analysis_text\n",
    "\n",
    "    with open(f'TEXT/analysis_text/{cons_tds}_analysis.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 raised error\n",
      "097 raised error\n",
      "359 raised error\n",
      "073 raised error\n",
      "153 raised error\n",
      "210 raised error\n",
      "091 raised error\n",
      "165 raised error\n",
      "337 raised error\n",
      "056 raised error\n",
      "065 raised error\n",
      "252 raised error\n",
      "016 raised error\n",
      "530 raised error\n",
      "086 raised error\n",
      "113 raised error\n",
      "075 raised error\n",
      "134 raised error\n",
      "170 raised error\n",
      "351 raised error\n",
      "127 raised error\n",
      "041 raised error\n",
      "169 raised error\n",
      "044 raised error\n",
      "341 raised error\n",
      "010 raised error\n",
      "081 raised error\n",
      "021 raised error\n",
      "083 raised error\n",
      "112 raised error\n",
      "093 raised error\n",
      "035 raised error\n",
      "005 raised error\n",
      "055 raised error\n",
      "377 raised error\n",
      "128 raised error\n"
     ]
    }
   ],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_analysis_text(tds)\n",
    "    #except FileNotFoundError:\n",
    "        #pass\n",
    "    except:\n",
    "        print(f\"{tds} raised error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and Prepare Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set asset map path\n",
    "asset_map_path = f\"MAPS/asset_maps/{cons_tds}_asset_map.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split context map into two pages\n",
    "def process_context_map(cons_tds):\n",
    "    image = Image.open(f'MAPS/context_maps/{cons_tds}_context_map.png')\n",
    "    width, height = image.size\n",
    "\n",
    "    bb1 = (0,0,width/2,height)\n",
    "    bb2 = (width/2, 0, width, height)\n",
    "\n",
    "    img_1 = image.crop(bb1)\n",
    "    img_2 = image.crop(bb2)\n",
    "\n",
    "    img_1.save(f'MAPS/context_maps/{cons_tds}_context_1.png', format=\"PNG\")\n",
    "    img_2.save(f'MAPS/context_maps/{cons_tds}_context_2.png', format=\"PNG\")\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        process_context_map(tds)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO COME:\n",
    "- Consolidation Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Overview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overview_table(cons_tds, overview_data=overview_data):\n",
    "    \n",
    "    cons_data = overview_data.loc[overview_data['CONS_TDS']== cons_tds]\n",
    "    \n",
    "    overview_table = ''\n",
    "\n",
    "    overview_frame = r'''\n",
    "    \\resizebox{\\textwidth}{!}{\n",
    "    \\begin{tabular}{l|c|c|c|c|}\n",
    "    \\cline{2-5}\n",
    "                                                                           & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} TDS \\#} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Total Households} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Official Population} & \\cellcolor{ccteal}{\\color[HTML]{FFFFFF} Average Family Size} \\\\ \\hline\n",
    "\n",
    "    '''\n",
    "\n",
    "    development_template = r'''\\multicolumn{1}{|l|}{\\cellcolor{ccteallight}%s}        & %s                                                   & %s                                                           & %s                                                                & %s                                                                \\\\ \\hline'''\n",
    "\n",
    "\n",
    "    overview_table += overview_frame\n",
    "\n",
    "    for row in cons_data.itertuples():\n",
    "        dev_name = row.DEV_NAME.title()\n",
    "        dev_tds = row.TDS\n",
    "        total_hhs = row.TOTAL_HH\n",
    "        official_population = row.TOTAL_POP\n",
    "        avg_family_size = row.AVG_FAMILY_SIZE\n",
    "\n",
    "        overview_table += development_template % (dev_name, dev_tds, total_hhs, official_population, avg_family_size)\n",
    "\n",
    "    overview_table += r'''\n",
    "    \\end{tabular}\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    with open(f'TABLES/overview_table/{cons_tds}_overview_table.tex', 'w') as file_handle:\n",
    "        file_handle.write(overview_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tds in consolidations.keys():\n",
    "    make_overview_table(tds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typology Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_typology_data():\n",
    "    #Cleaning and Shaping Data\n",
    "    typ_1 = pd.read_csv('DATA/typologies_1.csv')\n",
    "    typ_2 = pd.read_csv('DATA/typologies_2.csv')\n",
    "\n",
    "    typ_1.columns = ['CONS_NAME', 'DEV_NAME', 'TDS', 'TYPOLOGY']\n",
    "    typ_2.columns = ['CONS_NAME', 'CONS_TDS', 'DEV_NAME', 'TDS', 'METHOD', \n",
    "                     'CONSTRUCTION_DATE', 'BLDG_AGE', 'STORIES', 'BLDG_COVERAGE_SQFT', 'OPEN_SPACE_RATIO', 'SCATTERED_SITE_FLAG']\n",
    "\n",
    "    def make_dates(date_col):\n",
    "        date = str(date_col).split('/')\n",
    "        try:\n",
    "            if int(date[2]) > 18:\n",
    "                return datetime.date(int(f'19{date[2]}'), int(date[0]), int(date[1]))\n",
    "            else:\n",
    "                return datetime.date(int(f'20{date[2]}'), int(date[0]), int(date[1]))\n",
    "        except IndexError:\n",
    "            return datetime.date(1900,1,1)\n",
    "\n",
    "    typ_2['CONSTRUCTION_DATE'] = typ_2['CONSTRUCTION_DATE'].apply(lambda x: make_dates(x))\n",
    "    typ_2['SCATTERED_SITE_FLAG'] = typ_2['SCATTERED_SITE_FLAG'].apply(lambda x: x == 'YES')\n",
    "    typ_2.loc[typ_2['SCATTERED_SITE_FLAG']=='YES','SCATTERED_SITE_FLAG'] = 1\n",
    "\n",
    "    typology = typ_1.merge(typ_2[['CONS_TDS', 'TDS', 'METHOD',\n",
    "                                 'CONSTRUCTION_DATE', 'BLDG_AGE', \n",
    "                                 'STORIES', 'BLDG_COVERAGE_SQFT', \n",
    "                                 'OPEN_SPACE_RATIO', 'SCATTERED_SITE_FLAG']], how='left', on='TDS')\n",
    "\n",
    "    typology['CONS_TDS'] = typology['CONS_TDS'].apply(lambda x: str(int(x)).zfill(3))\n",
    "    typology['PREWAR'] = typology['CONSTRUCTION_DATE'].apply(lambda x: x < datetime.date(1945,1,1))\n",
    "\n",
    "    #Adding Typology Icons\n",
    "\n",
    "    typ_icons = [r'\\rootpath/IMAGES/typology_earlytower.png', r'\\rootpath/IMAGES/typology_towerpark.png', r'\\rootpath/IMAGES/typology_prewar.png', r'\\rootpath/IMAGES/typology_scatteredsite.png']\n",
    "    typ_dict = {}\n",
    "    [typ_dict.setdefault(key, '') for key in typology['TYPOLOGY'].unique().tolist()]\n",
    "\n",
    "    typ_dict['1 - High-rise in the park'] = typ_icons[1]\n",
    "    typ_dict['2 - Mid-rise in the park'] = typ_icons[1]\n",
    "    typ_dict['3 - Low-rise in the park'] = typ_icons[0]\n",
    "    typ_dict['4 - Context Towers'] = typ_icons[3]\n",
    "    typ_dict['5 - Context Mid-rises'] = typ_icons[2]\n",
    "    typ_dict['6 - Walkups & Brownstones'] = typ_icons[2]\n",
    "\n",
    "    typ_header = re.compile(r'\\d\\s-\\s')\n",
    "\n",
    "    typology['TYP_NAME'] = typology['TYPOLOGY'].apply(lambda x: typ_header.sub('', str(x)))\n",
    "    typology['IMAGE_PATH'] = typology['TYPOLOGY'].apply(lambda x: typ_dict[x])\n",
    "    \n",
    "    return typology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "typology = load_typology_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_typology_table_block(cons_tds, typ_data):\n",
    "    cons_data = typ_data[typ_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "    \n",
    "    if num_devs < 5:\n",
    "        block_1 = cons_data\n",
    "    \n",
    "    elif num_devs >=5 and num_devs < 7:\n",
    "        block_1 = cons_data.iloc[0:3]\n",
    "        block_2 = cons_data.iloc[3:]\n",
    "    \n",
    "    else:\n",
    "        block_1 = cons_data.iloc[0:4]\n",
    "        block_2 = cons_data.iloc[4:]\n",
    "    \n",
    "    len_1 = block_1.shape[0]\n",
    "    \n",
    "    try:\n",
    "        len_2 = block_2.shape[0]\n",
    "    except:\n",
    "        len_2 = 0\n",
    "    \n",
    "    headers = {1:r\"\\begin{tabular}{m{1.5in} m{2in}\"+'\\n',\n",
    "              2:r\"\\begin{tabular}{m{1.25in} m{2in} m{.1in} m{1.25in} m{2in}}\"+'\\n',\n",
    "              3:r\"\\begin{tabular}{m{1.25in} m{1.5in} m{.2in} m{1.25in} m{1.5in} m{.2in} m{1.25in} m{1.5in}}\"+'\\n',\n",
    "              4:r\"\\begin{tabular}{m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in} m{.2in} m{1.25in} m{1.25in}}\"+'\\n'}\n",
    "         \n",
    "    lines = {1:r'''\\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            2:r'''\\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=2in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            3:r'''\\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}'''+'\\n'+r'\\end{tabular}',\n",
    "            4:r'''\\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s} & & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}& & \\textbf{%s:} {%s} & \\includegraphics[height=1.5in]{%s}'''+'\\n'+r'\\end{tabular}'}\n",
    "    \n",
    "    \n",
    "    data_1 = []\n",
    "    data_2 = []\n",
    "    \n",
    "    for row in block_1.itertuples():\n",
    "        data_1.append(str(row.DEV_NAME).title())\n",
    "        data_1.append(str(row.TYP_NAME).replace('&', '\\&'))\n",
    "        data_1.append(row.IMAGE_PATH)\n",
    "    \n",
    "    if len_2 > 0:\n",
    "        for row in block_2.itertuples():\n",
    "            data_2.append(str(row.DEV_NAME.title()))\n",
    "            data_2.append(str(row.TYP_NAME).replace('&', '\\&'))\n",
    "            data_2.append(row.IMAGE_PATH)\n",
    "    \n",
    "    # Assembling Nested Tables\n",
    "    latex_block = ''\n",
    "    latex_block += r'''\\begin{table}[H]\n",
    "    \\resizebox{\\textwidth}{!}{\n",
    "    \\begin{tabular}{c}\n",
    "    '''\n",
    "    \n",
    "    latex_block += headers[len_1]\n",
    "    latex_block += lines[len_1] % tuple(data_1)\n",
    "    \n",
    "    if len_2 > 0:\n",
    "        latex_block += r'''\\\\\n",
    "        '''\n",
    "        latex_block += headers[len_2]\n",
    "        latex_block += lines[len_2] % tuple(data_2)\n",
    "    \n",
    "    latex_block += r'''\\end{tabular}}\n",
    "    \\end{table}'''\n",
    "    \n",
    "    with open(f'TABLES/typology_table/{cons_tds}_typology.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 raised an exception.\n",
      "091 raised an exception.\n",
      "128 raised an exception.\n"
     ]
    }
   ],
   "source": [
    "typology = load_typology_data()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_typology_table_block(tds, typology)\n",
    "    except:\n",
    "        print(f'{tds} raised an exception.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_cons_tds = {'091':'Baisley Park. Isolate important developments.',\n",
    "                   '359':'Skip for now.'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waste Services and Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wsa_data():\n",
    "    wsa_data = pd.read_csv('DATA/WASTE_SERVICES_ASSETS.csv')\n",
    "    wsa_data['TDS'] = wsa_data['DEV_TDS'].apply(lambda x: str(x).zfill(3))\n",
    "    wsa_data['INT_COMP_DATE'] = pd.to_datetime(wsa_data['INT_COMP_INSTALL_DATE'], errors='ignore')\n",
    "\n",
    "    return wsa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_waste_services_table(cons_tds, wsa_data, counts_dict=counts):\n",
    "    dev_list = counts_dict[cons_tds]['developments']\n",
    "    cons_data = wsa_data.query(f\"TDS in {dev_list}\")\n",
    "    num_devs = counts_dict[cons_tds]['count']\n",
    "    \n",
    "    def make_waste_services_block(num_cols, block_data):\n",
    "    \n",
    "        col_format = r'X|'\n",
    "        header = r'\\begin{tabularx}{\\textwidth}{V{1.5in}|'+col_format*(num_cols)+r'''}\n",
    "    \\cline{2-4}\n",
    "                                                                                       '''+r'& \\cellcolor{ccorange}{\\color[HTML]{FFFFFF} %s}'*num_cols+r' \\\\ \\hline'+'\\n'\n",
    "        hh_waste_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}Household Waste (DSNY)}               '+r'& %s'*num_cols+r'\\\\ \\hline'+'\\n'\n",
    "        bulk_waste_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}Bulk Waste}                  '+r'& %s'*num_cols+r' \\\\ \\hline'+'\\n'\n",
    "        norm_recycling_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                   '+r'& DSNY Curb Setout'*num_cols + r'\\\\ \\hline'+'\\n'\n",
    "        special_recycling_line = r'\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                   '+r'& %s'*num_cols +r'\\\\ \\hline' + '\\n'\n",
    "        \n",
    "        latex_block = r''''''\n",
    "        latex_block += header % tuple(block_data['DEV_NAME'].apply(lambda x: str(x).title()).tolist())\n",
    "        \n",
    "        \n",
    "        hh_waste_data = []\n",
    "        bulk_waste_data = []\n",
    "        ewaste_data = []\n",
    "        textiles_data = []\n",
    "        \n",
    "        for dev in block_data.itertuples():\n",
    "            if dev.CURBSIDE == 1:\n",
    "                hh_waste_data.append('Curbside Pickup')\n",
    "            elif dev.SHARE == 1:\n",
    "                hh_waste_data.append(f'Transfer to {str(dev.SHARE_SITE).title()}')\n",
    "            else:\n",
    "                if (dev.EXT_COMP_BE == 1) and (dev.COMPACTOR_YARDS == 1):\n",
    "                    hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactor in {int(dev.COMPACTOR_YARDS)} waste yard')\n",
    "                elif (dev.COMPACTOR_YARDS == 1):\n",
    "                    hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yard')\n",
    "                else:\n",
    "                    hh_waste_data.append(f'{int(dev.EXT_COMP_BE)} exterior compactors in {int(dev.COMPACTOR_YARDS)} waste yards')\n",
    "                \n",
    "            if pd.isna(dev.BULK_HAULER):\n",
    "                if int(dev.BULK_SITES) == 0:\n",
    "                    bulk_waste_data.append(\"Transferred for Pickup\")\n",
    "                elif int(dev.BULK_SITES) == 1:\n",
    "                    bulk_waste_data.append(\"One Bulk Drop Site; Transferred for Pickup\")\n",
    "                else:\n",
    "                    bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Drop Sites; Transferred for Pickup\")\n",
    "            else:\n",
    "                if int(dev.BULK_SITES) == 1:\n",
    "                    bulk_waste_data.append(f\"One Bulk Drop Site; Picked up by {dev.BULK_HAULER}\")\n",
    "                elif int(dev.BULK_SITES) > 1:\n",
    "                    bulk_waste_data.append(f\"{dev.BULK_SITES} Bulk Drop Sites; Picked up by {dev.BULK_HAULER}\")\n",
    "                else:\n",
    "                    bulk_waste_data.append(f\"Picked up by {dev.BULK_HAULER}\")\n",
    "            \n",
    "            if dev.ECYCLE == 1:\n",
    "                ewaste_data.append('Previously available through ECycle')\n",
    "            else:\n",
    "                ewaste_data.append('N/A')\n",
    "            \n",
    "            if dev.REFASHION == 1:\n",
    "                textiles_data.append('Previously available through Refashion')\n",
    "            else:\n",
    "                textiles_data.append('N/A')\n",
    "        \n",
    "        latex_block += hh_waste_line % tuple(hh_waste_data)\n",
    "        latex_block += bulk_waste_line % tuple(bulk_waste_data)\n",
    "        latex_block += norm_recycling_line % 'Recycling: Paper and Cardboard'\n",
    "        latex_block += norm_recycling_line % 'Recycling: Metal, Glass, and Plastic'\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: Mattresses']+['N/A' for i in range(0, num_cols)])\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: E-Waste']+ewaste_data)\n",
    "        latex_block += special_recycling_line % tuple(['Recycling: Textiles']+textiles_data)\n",
    "        latex_block += r'\\end{tabularx}'\n",
    "        \n",
    "        return latex_block\n",
    "    \n",
    "    if num_devs <= 4:\n",
    "        num_cols = num_devs\n",
    "        block_data = cons_data\n",
    "        \n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols, block_data))\n",
    "        \n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)\n",
    "        block_data_1 = cons_data.iloc[0:num_cols_1]\n",
    "        block_data_2 = cons_data.iloc[num_cols_1:]\n",
    "        \n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services_1.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols_1, block_data_1))\n",
    "            \n",
    "        with open(f'TABLES/waste_services/{cons_tds}_waste_services_2.tex', 'w') as file_handle:\n",
    "            file_handle.write(make_waste_services_block(num_cols_2, block_data_2))\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa_data = load_wsa_data()\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    make_waste_services_table(tds, wsa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_waste_assets_table(cons_tds, wsa_data, counts_dict=counts):\n",
    "    dev_list = counts_dict[cons_tds]['developments']\n",
    "    cons_data = wsa_data.query(f\"TDS in {dev_list}\")\n",
    "    num_devs = counts_dict[cons_tds]['count']\n",
    "    \n",
    "    header = r'''\n",
    "    \\begin{tabular}{V{.25\\columnwidth}|V{.15\\columnwidth}|V{.15\\columnwidth}|V{.25\\columnwidth}|V{.15\\columnwidth}|}\n",
    "\\cline{2-5}\n",
    "                                                                                              & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Internal Compactors} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} External Compactors} & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Other External Assets}   & \\cellcolor{ccorangelight}{\\color[HTML]{000000} Recycling Bins\\tnote{1}} \\\\ \\hline'''+'\\n'\n",
    "    line_format = r'\\multicolumn{1}{|V{.25\\columnwidth}|}{\\cellcolor{ccorange}{\\color[HTML]{FFFFFF} %s}}        & %s                                                & %s                                                                  & %s & %s                                                            \\\\ \\hline'+'\\n'\n",
    "    \n",
    "    latex_block = r''''''\n",
    "    latex_block += header\n",
    "    \n",
    "    for dev in cons_data.itertuples():\n",
    "        line_data = []\n",
    "        line_data.append(str(dev.DEV_NAME).title())\n",
    "        \n",
    "        if (dev.INT_COMP == 0):\n",
    "            int_comp_string = '0'\n",
    "        elif pd.isna(dev.INT_COMP_DATE):\n",
    "            int_comp_string = str(int(dev.INT_COMP))\n",
    "        else:\n",
    "            int_comp_string = f'{str(int(dev.INT_COMP))}; last replaced {str(dev.INT_COMP_DATE.year)}'\n",
    "        \n",
    "        line_data.append(int_comp_string)\n",
    "        line_data.append(str(int(dev.EXT_COMP_BE)))\n",
    "        \n",
    "        #if (dev.BULK_CRUSHERS == 0) and (dev.BALERS == 0)... REDO THIS ONCE DATA ARE COMPLETE\n",
    "        line_data.append('PLACEHOLDER UNTIL DATA ARE COMPLETE')\n",
    "        line_data.append(str(int(dev.RECYCLING_BINS)))\n",
    "        \n",
    "        latex_block += line_format % tuple(line_data)\n",
    "    \n",
    "    latex_block += r'\\end{tabular}'\n",
    "    \n",
    "    with open(f'TABLES/waste_assets/{cons_tds}_waste_assets.tex', 'w') as file_handle:\n",
    "        file_handle.write(latex_block)\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsa_data = load_wsa_data()\n",
    "for tds in consolidations.keys():\n",
    "    make_waste_assets_table(tds, wsa_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidation Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MAKE</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>SERIAL #</th>\n",
       "      <th>LICENSE #</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>PARKING LOC</th>\n",
       "      <th>WORK LOCATION</th>\n",
       "      <th>PURCHASE PRICE</th>\n",
       "      <th>ASSIGNEE</th>\n",
       "      <th>FUEL TYPE</th>\n",
       "      <th>DEV</th>\n",
       "      <th>DEV_MATCH</th>\n",
       "      <th>TDS</th>\n",
       "      <th>CONS_TDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>CHEVROLET</td>\n",
       "      <td>EXPRESS</td>\n",
       "      <td>1GCWGFCA3E1192366</td>\n",
       "      <td>AA2521</td>\n",
       "      <td>CHEVY-EXPRESS VAN-2014</td>\n",
       "      <td>728 E. NY Ave., Bklyn, NY 11203</td>\n",
       "      <td>NYCHA-Reid Apartments</td>\n",
       "      <td>20362</td>\n",
       "      <td>Casco,Jorge</td>\n",
       "      <td>G</td>\n",
       "      <td>Reid Apartments</td>\n",
       "      <td>REID APARTMENTS</td>\n",
       "      <td>167</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>CHEVROLET</td>\n",
       "      <td>EXPRESS</td>\n",
       "      <td>1GCWGFCA1E1192818</td>\n",
       "      <td>AA2533</td>\n",
       "      <td>CHEVY-EXPRESS VAN-2014</td>\n",
       "      <td>816 Ashford St., Bklyn, NY 11207</td>\n",
       "      <td>NYCHA-Brooklyn Borough Management</td>\n",
       "      <td>20362</td>\n",
       "      <td>London,James</td>\n",
       "      <td>G</td>\n",
       "      <td>Brooklyn Borough Management</td>\n",
       "      <td>!!!NOT FOUND</td>\n",
       "      <td>!!!NOT FOUND</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>DODGE</td>\n",
       "      <td>GRANDCARAVAN</td>\n",
       "      <td>2D4RN4DG0BR657206</td>\n",
       "      <td>AA5440</td>\n",
       "      <td>2011,DODGE,PASS-VAN</td>\n",
       "      <td>30-20 Beach Channel Drive, Queens, NY 11691</td>\n",
       "      <td>NYCHA-Beach 41st Street-Beach Channel Dr</td>\n",
       "      <td>23997</td>\n",
       "      <td>Diaz,Hector</td>\n",
       "      <td>G</td>\n",
       "      <td>Beach 41st Street-Beach Channel Dr</td>\n",
       "      <td>!!!NOT FOUND</td>\n",
       "      <td>!!!NOT FOUND</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>FORD</td>\n",
       "      <td>F250</td>\n",
       "      <td>1FT7X2B63DEB14123</td>\n",
       "      <td>AA6378</td>\n",
       "      <td>FORD-F250-2013</td>\n",
       "      <td>1145 East 229th St., Bronx, NY 10466</td>\n",
       "      <td>NYCHA-Edenwald</td>\n",
       "      <td>29799</td>\n",
       "      <td>Young,Carolyn</td>\n",
       "      <td>G</td>\n",
       "      <td>Edenwald</td>\n",
       "      <td>EDENWALD</td>\n",
       "      <td>057</td>\n",
       "      <td>057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>FORD</td>\n",
       "      <td>F250</td>\n",
       "      <td>1FT7X2B61DEB14136</td>\n",
       "      <td>AA6379</td>\n",
       "      <td>2013,FORD,PICK-UP</td>\n",
       "      <td>210 W.142nd St., NY, NY 10030</td>\n",
       "      <td>NYCHA-Drew-Hamilton Houses</td>\n",
       "      <td>29799</td>\n",
       "      <td>Romero,Carols</td>\n",
       "      <td>G</td>\n",
       "      <td>Drew-Hamilton Houses</td>\n",
       "      <td>!!!NOT FOUND</td>\n",
       "      <td>!!!NOT FOUND</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     YEAR       MAKE         MODEL           SERIAL # LICENSE #  \\\n",
       "0  2014.0  CHEVROLET       EXPRESS  1GCWGFCA3E1192366    AA2521   \n",
       "1  2014.0  CHEVROLET       EXPRESS  1GCWGFCA1E1192818    AA2533   \n",
       "2  2011.0      DODGE  GRANDCARAVAN  2D4RN4DG0BR657206    AA5440   \n",
       "3  2013.0       FORD          F250  1FT7X2B63DEB14123    AA6378   \n",
       "4  2013.0       FORD          F250  1FT7X2B61DEB14136    AA6379   \n",
       "\n",
       "              DESCRIPTION                                  PARKING LOC  \\\n",
       "0  CHEVY-EXPRESS VAN-2014              728 E. NY Ave., Bklyn, NY 11203   \n",
       "1  CHEVY-EXPRESS VAN-2014             816 Ashford St., Bklyn, NY 11207   \n",
       "2     2011,DODGE,PASS-VAN  30-20 Beach Channel Drive, Queens, NY 11691   \n",
       "3          FORD-F250-2013         1145 East 229th St., Bronx, NY 10466   \n",
       "4       2013,FORD,PICK-UP                210 W.142nd St., NY, NY 10030   \n",
       "\n",
       "                              WORK LOCATION PURCHASE PRICE       ASSIGNEE  \\\n",
       "0                     NYCHA-Reid Apartments          20362    Casco,Jorge   \n",
       "1         NYCHA-Brooklyn Borough Management          20362   London,James   \n",
       "2  NYCHA-Beach 41st Street-Beach Channel Dr          23997    Diaz,Hector   \n",
       "3                            NYCHA-Edenwald          29799  Young,Carolyn   \n",
       "4                NYCHA-Drew-Hamilton Houses          29799  Romero,Carols   \n",
       "\n",
       "  FUEL TYPE                                 DEV        DEV_MATCH  \\\n",
       "0         G                     Reid Apartments  REID APARTMENTS   \n",
       "1         G         Brooklyn Borough Management     !!!NOT FOUND   \n",
       "2         G  Beach 41st Street-Beach Channel Dr     !!!NOT FOUND   \n",
       "3         G                            Edenwald         EDENWALD   \n",
       "4         G                Drew-Hamilton Houses     !!!NOT FOUND   \n",
       "\n",
       "            TDS CONS_TDS  \n",
       "0           167      167  \n",
       "1  !!!NOT FOUND      NaN  \n",
       "2  !!!NOT FOUND      NaN  \n",
       "3           057      057  \n",
       "4  !!!NOT FOUND      NaN  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IN DEVELOPMENT\n",
    "\n",
    "#def make_consolidation_assets(cons_tds):\n",
    "vehicle_data = pd.read_excel('DATA/vehicle_inventory.xlsx')\n",
    "vehicle_data['DEV'] = vehicle_data['WORK LOCATION'].apply(lambda x: str(x).replace('NYCHA-',''))\n",
    "\n",
    "def get_dev_name(x):\n",
    "    for key, value in developments.items():\n",
    "        if (x.upper() == value['name'].upper()) or (x.upper() in value['name_alternates']):\n",
    "            return value['name']\n",
    "    \n",
    "    return '!!!NOT FOUND'\n",
    "\n",
    "def get_tds_from_name(x):\n",
    "    for key, value in developments.items():\n",
    "        if value['name'] == x:\n",
    "            return key\n",
    "    \n",
    "    return '!!!NOT FOUND'\n",
    "\n",
    "vehicle_data['DEV_MATCH'] = vehicle_data['DEV'].apply(lambda x: get_dev_name(str(x)))\n",
    "vehicle_data['TDS'] = vehicle_data['DEV_MATCH'].apply(lambda x: get_tds_from_name(str(x)))\n",
    "vehicle_data['CONS_TDS'] = vehicle_data['TDS'].apply(lambda x: developments[x]['cons_tds'] if x in developments.keys() else np.NaN)\n",
    "        \n",
    "vehicle_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waste Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_waste_cols(overview_data):\n",
    "    conversion_factors = {'units_to_tons_day': 0.0025,\n",
    "                         'cy_per_ton': {'trash': 21.05,\n",
    "                                        'trash_actual': 0,\n",
    "                                       'MGP': 18.02,\n",
    "                                       'cardboard': 26.67,\n",
    "                                       'paper': 6.19,\n",
    "                                       'organics': 4.32,\n",
    "                                       'ewaste': 5.65,\n",
    "                                       'textiles': 13.33},\n",
    "                         'gallons_per_cy': 201.974,\n",
    "                         'gallons_per_64gal': 64,\n",
    "                         'gallons_per_40lb_bag': 44,\n",
    "                         'cy_per_44gal_bag':0.174,\n",
    "                         'cy_per_cardboard_bale':0.193}\n",
    "\n",
    "    waste_percentages = {'trash': .26,\n",
    "                         'trash_actual':.894,\n",
    "                        'MGP': .19,\n",
    "                        'cardboard': .07,\n",
    "                        'paper': .07,\n",
    "                        'organics':.32,\n",
    "                        'ewaste': .01,\n",
    "                        'textiles': .08}\n",
    "\n",
    "    capture_rates = {'trash_primary': .75,\n",
    "                    'trash_secondary': .25,\n",
    "                    'mgp': .30,\n",
    "                    'cardboard': .50,\n",
    "                    'paper': .20}\n",
    "\n",
    "    overview_data['WASTE_TONS_DAY'] = overview_data['CURRENT_APTS'].apply(lambda x: x * conversion_factors['units_to_tons_day'])\n",
    "\n",
    "    for key, value in waste_percentages.items():\n",
    "        overview_data[f'{key.upper()}_CY'] = overview_data['WASTE_TONS_DAY'].apply(lambda x: x * value * conversion_factors['cy_per_ton'][key])\n",
    "        overview_data[f'{key.upper()}_TONS'] = overview_data['WASTE_TONS_DAY'].apply(lambda x: x * value)\n",
    "    \n",
    "    overview_data['TRASH_ACTUAL_CY'] = (overview_data['TRASH_CY']+\n",
    "                                           overview_data['MGP_CY']+\n",
    "                                           overview_data['CARDBOARD_CY']+\n",
    "                                           overview_data['PAPER_CY']+\n",
    "                                           overview_data['ORGANICS_CY']+\n",
    "                                           overview_data['EWASTE_CY']+\n",
    "                                           overview_data['TEXTILES_CY'])-(overview_data['MGP_CY']*capture_rates['mgp']+\n",
    "                                                                         overview_data['CARDBOARD_CY']*capture_rates['cardboard']+\n",
    "                                                                         overview_data['PAPER_CY']*capture_rates['paper'])\n",
    "\n",
    "    overview_data['TRASH_CHUTE_CY'] = overview_data['TRASH_ACTUAL_CY']*capture_rates['trash_primary']\n",
    "    overview_data['TRASH_CHUTE_TONS'] = overview_data['TRASH_ACTUAL_TONS']*capture_rates['trash_primary']\n",
    "    overview_data['TRASH_CHUTE_SAUSAGE'] = ((overview_data['TRASH_CHUTE_CY'])/conversion_factors['cy_per_ton']['trash'])*(2000/40)\n",
    "    overview_data['TRASH_DROP_CY'] = overview_data['TRASH_ACTUAL_CY']*capture_rates['trash_secondary']\n",
    "    overview_data['TRASH_DROP_TONS'] = overview_data['TRASH_ACTUAL_TONS']*capture_rates['trash_secondary']\n",
    "    overview_data['TRASH_DROP_BINS'] = overview_data['TRASH_DROP_CY']*conversion_factors['gallons_per_cy']/64\n",
    "    overview_data['CAPTURED_MGP_TONS_WEEK'] = overview_data['MGP_TONS']*capture_rates['mgp']*7\n",
    "    overview_data['CAPTURED_CARDBOARD_TONS_WEEK'] = overview_data['CARDBOARD_TONS']*capture_rates['cardboard']*7\n",
    "    overview_data['CAPTURED_PAPER_TONS_WEEK'] = overview_data['PAPER_TONS']*capture_rates['paper']*7\n",
    "    overview_data['MGP_BAGS_WEEK'] = overview_data['MGP_CY']*capture_rates['mgp']*7/conversion_factors['cy_per_44gal_bag']\n",
    "    overview_data['PAPER_BAGS_WEEK'] = overview_data['PAPER_CY']*capture_rates['paper']*7/conversion_factors['cy_per_44gal_bag']\n",
    "    overview_data['CARDBOARD_BALES_WEEK'] = overview_data['CARDBOARD_CY']*capture_rates['cardboard']*7/conversion_factors['cy_per_cardboard_bale']\n",
    "    \n",
    "    return overview_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEO_BORO</th>\n",
       "      <th>NYCHA_BORO</th>\n",
       "      <th>CONS_TDS</th>\n",
       "      <th>CONS_NAME</th>\n",
       "      <th>DEV_NAME</th>\n",
       "      <th>TDS</th>\n",
       "      <th>AMP</th>\n",
       "      <th>TOTAL_HH</th>\n",
       "      <th>TOTAL_POP</th>\n",
       "      <th>AVG_FAMILY_SIZE</th>\n",
       "      <th>AVG_TENURE</th>\n",
       "      <th>CURRENT_APTS</th>\n",
       "      <th>TOTAL_APTS</th>\n",
       "      <th>STAIRHALLS</th>\n",
       "      <th>WASTE_TONS_DAY</th>\n",
       "      <th>TRASH_CY</th>\n",
       "      <th>TRASH_TONS</th>\n",
       "      <th>TRASH_ACTUAL_CY</th>\n",
       "      <th>TRASH_ACTUAL_TONS</th>\n",
       "      <th>MGP_CY</th>\n",
       "      <th>MGP_TONS</th>\n",
       "      <th>CARDBOARD_CY</th>\n",
       "      <th>CARDBOARD_TONS</th>\n",
       "      <th>PAPER_CY</th>\n",
       "      <th>PAPER_TONS</th>\n",
       "      <th>ORGANICS_CY</th>\n",
       "      <th>ORGANICS_TONS</th>\n",
       "      <th>EWASTE_CY</th>\n",
       "      <th>EWASTE_TONS</th>\n",
       "      <th>TEXTILES_CY</th>\n",
       "      <th>TEXTILES_TONS</th>\n",
       "      <th>TRASH_CHUTE_CY</th>\n",
       "      <th>TRASH_CHUTE_TONS</th>\n",
       "      <th>TRASH_CHUTE_SAUSAGE</th>\n",
       "      <th>TRASH_DROP_CY</th>\n",
       "      <th>TRASH_DROP_TONS</th>\n",
       "      <th>TRASH_DROP_BINS</th>\n",
       "      <th>CAPTURED_MGP_TONS_WEEK</th>\n",
       "      <th>CAPTURED_CARDBOARD_TONS_WEEK</th>\n",
       "      <th>CAPTURED_PAPER_TONS_WEEK</th>\n",
       "      <th>MGP_BAGS_WEEK</th>\n",
       "      <th>PAPER_BAGS_WEEK</th>\n",
       "      <th>CARDBOARD_BALES_WEEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>180</td>\n",
       "      <td>1010 EAST 178TH STREET</td>\n",
       "      <td>1010 EAST 178TH STREET</td>\n",
       "      <td>180</td>\n",
       "      <td>NY005011330</td>\n",
       "      <td>214</td>\n",
       "      <td>448</td>\n",
       "      <td>2.1</td>\n",
       "      <td>21.3</td>\n",
       "      <td>218</td>\n",
       "      <td>220</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5450</td>\n",
       "      <td>2.982785</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>6.352002</td>\n",
       "      <td>0.487230</td>\n",
       "      <td>1.865971</td>\n",
       "      <td>0.103550</td>\n",
       "      <td>1.017461</td>\n",
       "      <td>0.038150</td>\n",
       "      <td>0.236149</td>\n",
       "      <td>0.038150</td>\n",
       "      <td>0.753408</td>\n",
       "      <td>0.1744</td>\n",
       "      <td>0.030793</td>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.581188</td>\n",
       "      <td>0.0436</td>\n",
       "      <td>4.764002</td>\n",
       "      <td>0.365423</td>\n",
       "      <td>11.315918</td>\n",
       "      <td>1.588001</td>\n",
       "      <td>0.121808</td>\n",
       "      <td>5.011482</td>\n",
       "      <td>0.217455</td>\n",
       "      <td>0.133525</td>\n",
       "      <td>0.053410</td>\n",
       "      <td>22.520340</td>\n",
       "      <td>1.900045</td>\n",
       "      <td>18.451356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>308</td>\n",
       "      <td>Claremont Consolidated</td>\n",
       "      <td>1162-1176 WASHINGTON AVENUE</td>\n",
       "      <td>233</td>\n",
       "      <td>NY005013080</td>\n",
       "      <td>64</td>\n",
       "      <td>167</td>\n",
       "      <td>2.6</td>\n",
       "      <td>14.7</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.889363</td>\n",
       "      <td>0.04225</td>\n",
       "      <td>1.893946</td>\n",
       "      <td>0.145275</td>\n",
       "      <td>0.556368</td>\n",
       "      <td>0.030875</td>\n",
       "      <td>0.303371</td>\n",
       "      <td>0.011375</td>\n",
       "      <td>0.070411</td>\n",
       "      <td>0.011375</td>\n",
       "      <td>0.224640</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.173290</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>1.420459</td>\n",
       "      <td>0.108956</td>\n",
       "      <td>3.374012</td>\n",
       "      <td>0.473486</td>\n",
       "      <td>0.036319</td>\n",
       "      <td>1.494249</td>\n",
       "      <td>0.064837</td>\n",
       "      <td>0.039813</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>6.714780</td>\n",
       "      <td>0.566527</td>\n",
       "      <td>5.501551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>067</td>\n",
       "      <td>Sotomayor</td>\n",
       "      <td>1471 WATSON AVENUE</td>\n",
       "      <td>214</td>\n",
       "      <td>NY005010670</td>\n",
       "      <td>96</td>\n",
       "      <td>158</td>\n",
       "      <td>1.6</td>\n",
       "      <td>23.2</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>1.313520</td>\n",
       "      <td>0.06240</td>\n",
       "      <td>2.797212</td>\n",
       "      <td>0.214560</td>\n",
       "      <td>0.821712</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.448056</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.103992</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.331776</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.013560</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.255936</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>2.097909</td>\n",
       "      <td>0.160920</td>\n",
       "      <td>4.983157</td>\n",
       "      <td>0.699303</td>\n",
       "      <td>0.053640</td>\n",
       "      <td>2.206891</td>\n",
       "      <td>0.095760</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>0.023520</td>\n",
       "      <td>9.917214</td>\n",
       "      <td>0.836717</td>\n",
       "      <td>8.125368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>118</td>\n",
       "      <td>Adams</td>\n",
       "      <td>ADAMS</td>\n",
       "      <td>118</td>\n",
       "      <td>NY005001180</td>\n",
       "      <td>919</td>\n",
       "      <td>2,253</td>\n",
       "      <td>2.5</td>\n",
       "      <td>22.5</td>\n",
       "      <td>924</td>\n",
       "      <td>925</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.3100</td>\n",
       "      <td>12.642630</td>\n",
       "      <td>0.60060</td>\n",
       "      <td>26.923166</td>\n",
       "      <td>2.065140</td>\n",
       "      <td>7.908978</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>4.312539</td>\n",
       "      <td>0.161700</td>\n",
       "      <td>1.000923</td>\n",
       "      <td>0.161700</td>\n",
       "      <td>3.193344</td>\n",
       "      <td>0.7392</td>\n",
       "      <td>0.130515</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>2.463384</td>\n",
       "      <td>0.1848</td>\n",
       "      <td>20.192374</td>\n",
       "      <td>1.548855</td>\n",
       "      <td>47.962884</td>\n",
       "      <td>6.730791</td>\n",
       "      <td>0.516285</td>\n",
       "      <td>21.241326</td>\n",
       "      <td>0.921690</td>\n",
       "      <td>0.565950</td>\n",
       "      <td>0.226380</td>\n",
       "      <td>95.453183</td>\n",
       "      <td>8.053403</td>\n",
       "      <td>78.206666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bronx</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>197</td>\n",
       "      <td>Fort Independence</td>\n",
       "      <td>BAILEY AVENUE-WEST 193RD STREET</td>\n",
       "      <td>202</td>\n",
       "      <td>NY005012020</td>\n",
       "      <td>233</td>\n",
       "      <td>459</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>232</td>\n",
       "      <td>233</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>3.174340</td>\n",
       "      <td>0.15080</td>\n",
       "      <td>6.759929</td>\n",
       "      <td>0.518520</td>\n",
       "      <td>1.985804</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>1.082802</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.251314</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.801792</td>\n",
       "      <td>0.1856</td>\n",
       "      <td>0.032770</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.618512</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>5.069947</td>\n",
       "      <td>0.388890</td>\n",
       "      <td>12.042629</td>\n",
       "      <td>1.689982</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>5.333320</td>\n",
       "      <td>0.231420</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.056840</td>\n",
       "      <td>23.966600</td>\n",
       "      <td>2.022067</td>\n",
       "      <td>19.636306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  GEO_BORO NYCHA_BORO CONS_TDS               CONS_NAME  \\\n",
       "0    Bronx      Bronx      180  1010 EAST 178TH STREET   \n",
       "1    Bronx      Bronx      308  Claremont Consolidated   \n",
       "2    Bronx      Bronx      067               Sotomayor   \n",
       "3    Bronx      Bronx      118                   Adams   \n",
       "4    Bronx      Bronx      197       Fort Independence   \n",
       "\n",
       "                          DEV_NAME  TDS          AMP TOTAL_HH TOTAL_POP  \\\n",
       "0           1010 EAST 178TH STREET  180  NY005011330      214       448   \n",
       "1      1162-1176 WASHINGTON AVENUE  233  NY005013080       64       167   \n",
       "2               1471 WATSON AVENUE  214  NY005010670       96       158   \n",
       "3                            ADAMS  118  NY005001180      919     2,253   \n",
       "4  BAILEY AVENUE-WEST 193RD STREET  202  NY005012020      233       459   \n",
       "\n",
       "   AVG_FAMILY_SIZE  AVG_TENURE  CURRENT_APTS  TOTAL_APTS  STAIRHALLS  \\\n",
       "0              2.1        21.3           218         220         1.0   \n",
       "1              2.6        14.7            65          66         1.0   \n",
       "2              1.6        23.2            96          96         1.0   \n",
       "3              2.5        22.5           924         925         7.0   \n",
       "4              2.0        20.0           232         233         1.0   \n",
       "\n",
       "   WASTE_TONS_DAY   TRASH_CY  TRASH_TONS  TRASH_ACTUAL_CY  TRASH_ACTUAL_TONS  \\\n",
       "0          0.5450   2.982785     0.14170         6.352002           0.487230   \n",
       "1          0.1625   0.889363     0.04225         1.893946           0.145275   \n",
       "2          0.2400   1.313520     0.06240         2.797212           0.214560   \n",
       "3          2.3100  12.642630     0.60060        26.923166           2.065140   \n",
       "4          0.5800   3.174340     0.15080         6.759929           0.518520   \n",
       "\n",
       "     MGP_CY  MGP_TONS  CARDBOARD_CY  CARDBOARD_TONS  PAPER_CY  PAPER_TONS  \\\n",
       "0  1.865971  0.103550      1.017461        0.038150  0.236149    0.038150   \n",
       "1  0.556368  0.030875      0.303371        0.011375  0.070411    0.011375   \n",
       "2  0.821712  0.045600      0.448056        0.016800  0.103992    0.016800   \n",
       "3  7.908978  0.438900      4.312539        0.161700  1.000923    0.161700   \n",
       "4  1.985804  0.110200      1.082802        0.040600  0.251314    0.040600   \n",
       "\n",
       "   ORGANICS_CY  ORGANICS_TONS  EWASTE_CY  EWASTE_TONS  TEXTILES_CY  \\\n",
       "0     0.753408         0.1744   0.030793     0.005450     0.581188   \n",
       "1     0.224640         0.0520   0.009181     0.001625     0.173290   \n",
       "2     0.331776         0.0768   0.013560     0.002400     0.255936   \n",
       "3     3.193344         0.7392   0.130515     0.023100     2.463384   \n",
       "4     0.801792         0.1856   0.032770     0.005800     0.618512   \n",
       "\n",
       "   TEXTILES_TONS  TRASH_CHUTE_CY  TRASH_CHUTE_TONS  TRASH_CHUTE_SAUSAGE  \\\n",
       "0         0.0436        4.764002          0.365423            11.315918   \n",
       "1         0.0130        1.420459          0.108956             3.374012   \n",
       "2         0.0192        2.097909          0.160920             4.983157   \n",
       "3         0.1848       20.192374          1.548855            47.962884   \n",
       "4         0.0464        5.069947          0.388890            12.042629   \n",
       "\n",
       "   TRASH_DROP_CY  TRASH_DROP_TONS  TRASH_DROP_BINS  CAPTURED_MGP_TONS_WEEK  \\\n",
       "0       1.588001         0.121808         5.011482                0.217455   \n",
       "1       0.473486         0.036319         1.494249                0.064837   \n",
       "2       0.699303         0.053640         2.206891                0.095760   \n",
       "3       6.730791         0.516285        21.241326                0.921690   \n",
       "4       1.689982         0.129630         5.333320                0.231420   \n",
       "\n",
       "   CAPTURED_CARDBOARD_TONS_WEEK  CAPTURED_PAPER_TONS_WEEK  MGP_BAGS_WEEK  \\\n",
       "0                      0.133525                  0.053410      22.520340   \n",
       "1                      0.039813                  0.015925       6.714780   \n",
       "2                      0.058800                  0.023520       9.917214   \n",
       "3                      0.565950                  0.226380      95.453183   \n",
       "4                      0.142100                  0.056840      23.966600   \n",
       "\n",
       "   PAPER_BAGS_WEEK  CARDBOARD_BALES_WEEK  \n",
       "0         1.900045             18.451356  \n",
       "1         0.566527              5.501551  \n",
       "2         0.836717              8.125368  \n",
       "3         8.053403             78.206666  \n",
       "4         2.022067             19.636306  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_waste_cols(overview_data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_waste_distribution_table(cons_tds, overview_data):\n",
    "    cons_data = overview_data[overview_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "    \n",
    "    if num_devs == 1:\n",
    "        num_cols = num_devs\n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)+1\n",
    "    else:\n",
    "        num_cols = num_devs+1\n",
    "    \n",
    "    if num_devs != 1:\n",
    "        cons_data.loc['Total']= cons_data.sum(numeric_only=True, axis=0)\n",
    "        cons_data.loc['Total','DEV_NAME'] = 'Total'\n",
    "    \n",
    "    def make_waste_distribution_table_block(cons_data, num_cols):\n",
    "        dev_col_format = r'X|'\n",
    "\n",
    "        opening = r'''\n",
    "        \\begin{tabularx}{\\textwidth}{V{1.5in}|%s}\n",
    "        \\cline{2-%s}\n",
    "        ''' % (dev_col_format*num_cols, (num_cols+1))\n",
    "\n",
    "        top_row = r'''\n",
    "                                                                       '''+(r\"& \\multicolumn{1}{V{1.5in}|}{\\cellcolor{ccorange}%s}\"*(num_cols))+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        standard_row = r\"\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                 \"+(r\"& %s                                    \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        captured_row = r\"\\multicolumn{1}{|Y{1.5in}|}{\\cellcolor{ccorangelight}Captured / Week (tons)\\tnote{4}}                        \"+(r\"& %s                                    \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        chute_row = r\"\\multicolumn{1}{|Y{1.5in}|}{\\cellcolor{ccorangelight}Trash Chutes\\tnote{2}}                 \"+(r\"& %s tons or (%s) 40 lbs. sausage bags      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        dropsite_row = r\"\\multicolumn{1}{|Y{1.5in}|}{\\cellcolor{ccorangelight}Drop Sites\\tnote{3}}                 \"+(r\"& %s tons or (%s) 64-gallon bins      \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        OET_row = r\"\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s / Day (CY)\\tnote{5}}              \"+(r\"& %s                                    \"*num_cols)+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        recycling_row = r\"\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                 \"+(r\"& %s tons or (%s) 44-gallon bags                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "        cardboard_row = r\"\\multicolumn{1}{|V{1.5in}|}{\\cellcolor{ccorangelight}%s}                 \"+(r\"& %s tons or (%s) bales                                   \")*num_cols+r\"\\tnhl\"+'\\n'\n",
    "\n",
    "\n",
    "        def make_trash_text(row, text_var, cy_col, other_col):\n",
    "            text_var.append(round(row[cy_col],2))\n",
    "            text_var.append(round(row[other_col], 2))\n",
    "            pass\n",
    "\n",
    "        latex_block = opening\n",
    "        latex_block += top_row % tuple(cons_data['DEV_NAME'].apply(lambda x: str(x).title()).tolist())\n",
    "        latex_block += standard_row % tuple([r\"Waste Generated / Day (Tons)\\tnote{1}\"]+[round(item, 2) for item in cons_data['WASTE_TONS_DAY'].tolist()])\n",
    "        latex_block += standard_row % tuple([r\"Trash / Day (tons)\\tnote{2}\"]+cons_data['TRASH_ACTUAL_TONS'].apply(lambda x: str(round(x,2))).tolist())\n",
    "\n",
    "        trash_chute_text = []\n",
    "        dropsite_text = []\n",
    "\n",
    "        cons_data.apply(lambda row: make_trash_text(row, trash_chute_text, 'TRASH_CHUTE_TONS', 'TRASH_CHUTE_SAUSAGE'), axis=1)\n",
    "        cons_data.apply(lambda row: make_trash_text(row, dropsite_text, 'TRASH_DROP_TONS', 'TRASH_DROP_BINS'), axis=1)\n",
    "\n",
    "        latex_block += chute_row % tuple(trash_chute_text)\n",
    "        latex_block += dropsite_row % tuple(dropsite_text)\n",
    "\n",
    "        latex_block += r\"\\end{tabularx}\\bigskip\"\n",
    "\n",
    "        latex_block += opening\n",
    "        latex_block += top_row % tuple(cons_data['DEV_NAME'].apply(lambda x: str(x).title()).tolist())\n",
    "\n",
    "        mgp_text = []\n",
    "        cardboard_text= []\n",
    "        paper_text = []\n",
    "\n",
    "        cons_data.apply(lambda row: make_trash_text(row, mgp_text, 'CAPTURED_MGP_TONS_WEEK', 'MGP_BAGS_WEEK'), axis=1)\n",
    "\n",
    "        cons_data.apply(lambda row: make_trash_text(row, cardboard_text, 'CAPTURED_CARDBOARD_TONS_WEEK', 'CARDBOARD_BALES_WEEK'), axis=1)\n",
    "\n",
    "        cons_data.apply(lambda row: make_trash_text(row, paper_text, 'CAPTURED_PAPER_TONS_WEEK', 'PAPER_BAGS_WEEK'), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        latex_block += recycling_row % tuple([r\"Metal, Glass, Plastic Captured / Week (tons)\"]+mgp_text)\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_MGP_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        latex_block += cardboard_row % tuple([r\"Cardboard Captured / Week (tons)\"]+cardboard_text)\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_CARDBOARD_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        latex_block += recycling_row % tuple([r\"Paper Captured / Week (tons)\"]+paper_text)\n",
    "        #latex_block += captured_row % tuple(cons_data['CAPTURED_PAPER_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "\n",
    "        #latex_block += OET_row % tuple(['Organics']+cons_data['ORGANICS_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        #latex_block += OET_row % tuple(['E-Waste']+cons_data['EWASTE_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "        #latex_block += OET_row % tuple(['Textiles']+cons_data['TEXTILES_CY'].apply(lambda x: str(round(x,2))).tolist())\n",
    "\n",
    "        latex_block += r\"\\end{tabularx}\"\n",
    "\n",
    "        return latex_block\n",
    "    \n",
    "    if num_devs<= 4:\n",
    "        latex_block = make_waste_distribution_table_block(cons_data, num_cols)\n",
    "\n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block)\n",
    "    \n",
    "    else:\n",
    "        latex_block_1 = make_waste_distribution_table_block(cons_data.iloc[0:num_cols_1], num_cols_1)\n",
    "        latex_block_2 = make_waste_distribution_table_block(cons_data.iloc[num_cols_1:], num_cols_2)\n",
    "        \n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table_1.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block_1)\n",
    "        with open(f'TABLES/waste_distribution_table/{cons_tds}_wd_table_2.tex', 'w') as file_handle:\n",
    "            file_handle.write(latex_block_2)\n",
    "\n",
    "    text_block = r''''''\n",
    "\n",
    "    text_line_multi = r\"{%s}: This development has %s apartment units and %s stairhalls.\\\\\"\n",
    "\n",
    "    text_line_singular = r\"{%s}: This development has %s apartment units and one stairhall.\\\\\"\n",
    "\n",
    "    for row in cons_data.itertuples():\n",
    "\n",
    "        if int(row.STAIRHALLS) == 1:\n",
    "            text_block += text_line_singular % (row.DEV_NAME.title(), int(row.CURRENT_APTS))\n",
    "        else:\n",
    "            text_block += text_line_multi % (row.DEV_NAME.title(), int(row.CURRENT_APTS), int(row.STAIRHALLS))\n",
    "\n",
    "\n",
    "    with open(f'TEXT/waste_distribution_bottom/{cons_tds}_wd_bottom.tex', 'w') as file_handle:\n",
    "        file_handle.write(text_block)\n",
    "        \n",
    "    top_block_template = r'''\n",
    "    By understanding how much waste is generated at each consolidation, planners and managers\n",
    "    can better determine how well current assets and services serve current needs, and what additional \n",
    "    elements are necessary in order for each consolidation to operate as efficiently as possible. \n",
    "\n",
    "    %s has (%s) 30-CY external compactors, each with a footprint of 192 square feet. Given the rate at which waste is produced at NYCHA properties, these containers will fill\n",
    "    up in about (%s) days.'''\n",
    "    \n",
    "    wsa_data = load_wsa_data()\n",
    "    wsa_data = wsa_data.query(f\"TDS in {counts[cons_tds]['developments']}\")\n",
    "    extcomp_total = int(wsa_data['EXT_COMP_BE'].sum())\n",
    "    #print(cons_data['TRASH_ACTUAL_CY'])\n",
    "    #print(cons_data['TRASH_ACTUAL_CY'].sum())\n",
    "    days_to_fill = extcomp_total*(cons_data['TRASH_ACTUAL_CY'].sum())/(30)\n",
    "    \n",
    "    top_block_data = []\n",
    "    top_block_data.append(str(consolidations[cons_tds]['name']).title())\n",
    "    top_block_data.append(str(extcomp_total))\n",
    "    top_block_data.append(str(round(days_to_fill, 1)))\n",
    "    \n",
    "    with open(f'TEXT/waste_distribution_top/{cons_tds}_wd_top.tex', 'w') as file_handle:\n",
    "        file_handle.write(top_block_template % tuple(top_block_data))\n",
    "    \n",
    "    #print(top_block_data)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised by 091\n"
     ]
    }
   ],
   "source": [
    "overview_data = add_waste_cols(load_overview_data())\n",
    "for tds in consolidations.keys():\n",
    "    try:\n",
    "        make_waste_distribution_table(tds, overview_data)\n",
    "    except:\n",
    "        print(f'Exception raised by {tds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Capital Improvements Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asset_data():\n",
    "    asset_data = {'fwd': ['In-Sink Food Grinders', pd.read_csv('DATA/capital_fwd.csv')],\n",
    "                  'ehd': ['Enlarged Hopper Doors', pd.read_csv('DATA/capital_ehd.csv')],\n",
    "                  'int_compactor':['Interior Compactor Replacement', pd.read_csv('DATA/capital_intcom.csv')],\n",
    "                  'wasteyard':['Waste Yard Redesign', pd.read_csv('DATA/capital_wasteyard.csv')]}\n",
    "\n",
    "    for value in asset_data.values():\n",
    "        value[1].columns = [item.strip() for item in value[1].columns]\n",
    "\n",
    "    asset_data['wasteyard'][1]['ESTIMATE'] = asset_data['wasteyard'][1]['TOT_EST']\n",
    "    asset_data['wasteyard'][1]['COST'] = np.nan\n",
    "    \n",
    "    def year_to_string(year):\n",
    "        if pd.isna(year):\n",
    "            return 'N/A'\n",
    "        else:\n",
    "            if int(year) <= 2022:\n",
    "                return str(int(year))\n",
    "            elif (int(year) > 2022) & (int(year) <= 2025):\n",
    "                return '2023-2025'\n",
    "            elif (int(year)>2025) and (int(year)<=2030):\n",
    "                return '2026-2030'\n",
    "            else:\n",
    "                return 'After 2030'\n",
    "    \n",
    "    asset_data['fwd'][1]['_YEAR'] = asset_data['fwd'][1]['EST_YEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['ehd'][1]['_YEAR'] = asset_data['fwd'][1]['CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['int_compactor'][1]['_YEAR'] = asset_data['int_compactor'][1]['CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    asset_data['wasteyard'][1]['_YEAR'] = asset_data['wasteyard'][1]['CONS_CYEAR'].apply(lambda x: year_to_string(x))\n",
    "    return asset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_capital_table(cons_tds, asset_data, overview_data=overview_data):\n",
    "    cons_data = overview_data[overview_data['CONS_TDS'] == cons_tds]\n",
    "    num_devs = cons_data.shape[0]\n",
    "\n",
    "    def make_capital_table_block(block_data, num_devs):\n",
    "        dev_col_format = r'X|'\n",
    "        header = r'''\n",
    "        \\begin{tabularx}{\\textwidth}{r|%s}\n",
    "        \\cline{2-%s}\n",
    "        ''' % ((dev_col_format*num_devs), num_devs)\n",
    "\n",
    "        top_row = r\"\\multicolumn{1}{l|}{}                                                        \"+r\"& \\cellcolor{ccorange}{\\color[HTML]{FFFFFF}%s} \"*num_devs+r\"\\\\ \\hline\"+\"\\n\"\n",
    "\n",
    "        project_block = r\"\\multicolumn{1}{|V{.2\\columnwidth}|}{\\cellcolor{ccorangelight}%s}          \"+(r\"&                                                                  \"*num_devs)+r\"\\\\\"+r'''\n",
    "        \\multicolumn{1}{|r|}{\\cellcolor{ccorangelight}\\textit{Status}}                '''+(r\"& %s                                                         \"*num_devs)+r'''\\\\\n",
    "        \\multicolumn{1}{|r|}{\\cellcolor{ccorangelight}\\textit{%s}}                  '''+(\"& %s                                                     \"*num_devs)+r\"\\\\ \\hline\"+\"\\n\"\n",
    "\n",
    "        devs = block_data['DEV_NAME'].apply(lambda x: str(x).upper()).tolist()\n",
    "        devs_title = block_data['DEV_NAME'].apply(lambda x: str(x).title()).tolist()\n",
    "        latex_block = ''\n",
    "        latex_block += header\n",
    "        latex_block += top_row % tuple(block_data['DEV_NAME'].apply(lambda x: str(x).title()).tolist())\n",
    "\n",
    "        for asset in asset_data.keys():\n",
    "            asset_df = asset_data[asset][1]\n",
    "            #print(asset_data[asset][0])\n",
    "            #print(devs)\n",
    "            #print(asset_df['DEVELOPMENT'].tolist())\n",
    "            if any((dev in asset_df['DEVELOPMENT'].tolist()) for dev in devs):\n",
    "                status_list = []\n",
    "                year_list = []\n",
    "\n",
    "                for dev in devs:\n",
    "                    if dev in asset_df['DEVELOPMENT'].tolist():\n",
    "                        #print(dev)\n",
    "                        if pd.isna(asset_df.loc[asset_df['DEVELOPMENT']== dev,'STATUS'].iloc[0]):\n",
    "                            status_list.append('Not Yet Scheduled')\n",
    "                        else:\n",
    "                            status_list.append(str(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'STATUS'].iloc[0]).title())\n",
    "\n",
    "                        #print(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'STATUS'])\n",
    "                        #print(asset_df.loc[asset_df['DEVELOPMENT']== dev, 'COST'])\n",
    "                        #try:\n",
    "                        year_list.append(str(asset_df.loc[asset_df['DEVELOPMENT']== dev,'_YEAR'].iloc[0]))\n",
    "                    #except:\n",
    "                            #year_list.append('TBD')\n",
    "\n",
    "                    else:\n",
    "                        status_list.append('N/A')\n",
    "                        year_list.append(' ')\n",
    "\n",
    "                asset_block = project_block % tuple([asset_data[asset][0]]+status_list+['Year Planned']+year_list)\n",
    "\n",
    "                latex_block += asset_block\n",
    "\n",
    "        latex_block += r\"\\end{tabularx}\"\n",
    "\n",
    "        return latex_block\n",
    "    \n",
    "    \n",
    "    if num_devs <= 4:\n",
    "        num_cols = num_devs\n",
    "        block_data = cons_data\n",
    "        \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data, num_cols))\n",
    "        \n",
    "    elif num_devs > 4:\n",
    "        num_cols_1 = math.ceil(num_devs/2)\n",
    "        num_cols_2 = (num_devs-num_cols_1)\n",
    "        block_data_1 = cons_data.iloc[0:num_cols_1]\n",
    "        block_data_2 = cons_data.iloc[num_cols_1:]\n",
    "        \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects_1.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data_1, num_cols_1))\n",
    "            \n",
    "        with open(f\"TABLES/capital_projects_table/{cons_tds}_capital_projects_2.tex\", 'w') as file_handle:\n",
    "            file_handle.write(make_capital_table_block(block_data_1, num_cols_1))\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_data= load_asset_data()\n",
    "overview_data = load_overview_data()\n",
    "for tds in consolidations.keys(): \n",
    "    make_capital_table(tds, asset_data, overview_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Staff Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staff_data(name_dict):\n",
    "    #Read budgeted staff and formula allocation\n",
    "    dev_staff = pd.read_csv('DATA/staff_for_table.csv')\n",
    "    dev_staff.fillna(0,inplace=True)\n",
    "    \n",
    "    def find_cons_tds(name, name_dict):\n",
    "        for key, value in name_dict.items():\n",
    "            if (name == value['name']) | (name in value['alternates']):\n",
    "                return key\n",
    "            \n",
    "    dev_staff['CONS_TDS'] = dev_staff['Consolidation'].apply(lambda x: find_cons_tds(x, name_dict))\n",
    "    dev_staff['CONS_NAME'] = dev_staff['CONS_TDS'].apply(lambda x: name_dict[x]['name'] if x is not None else 'NO NAME FOUND')\n",
    "    #Note: Staff list missing for Armstrong, Ft. Washington, and Williams Plaza, as well as scatter-site third-party-managed consolidations\n",
    " \n",
    "    #Read budgeted staff and actuals\n",
    "    actuals_data = pd.read_csv('DATA/Staffing_Analysis/DEVHC.csv')\n",
    "    actuals_data.fillna(0, inplace=True)\n",
    "    actuals_data = actuals_data[actuals_data['RC Name'].apply(lambda x: \"total\" not in str(x).lower()) & actuals_data['Department'].apply(lambda x: \"total\" not in str(x).lower())]\n",
    "    actuals_data['CONS_TDS'] = actuals_data['RC Name'].apply(lambda x: find_cons_tds(x, name_dict))\n",
    "    actuals_data['CONS_NAME'] = actuals_data['CONS_TDS'].apply(lambda x: name_dict[x]['name'] if x is not None else 'NO NAME FOUND')\n",
    "\n",
    "    def convert_neg(x):\n",
    "        try:\n",
    "            return int(x)\n",
    "        except:\n",
    "            return int('-'+str(x).replace('(','').replace(')',''))\n",
    "\n",
    "    actuals_data['VARIANCE'] = actuals_data['Unnamed: 5'].apply(lambda x: convert_neg(x))\n",
    "    actuals_data['ACT'] = actuals_data['13']\n",
    "    \n",
    "    table_frame = pd.read_csv('DATA/Table_Keys.csv')\n",
    "    actuals_keys = pd.read_csv('DATA/Staffing_Analysis/DEVHC_CODES.csv')\n",
    "    \n",
    "    actuals_data = actuals_data.merge(actuals_keys, how='left', left_on='CST_NAME', right_on='TITLE_NAME')\n",
    "    for column in ['Current Modified', 'ACT', 'VARIANCE']:\n",
    "        actuals_data[column] = actuals_data[column].astype(int)\n",
    "        \n",
    "\n",
    "    \n",
    "    return (dev_staff, actuals_data, table_frame, actuals_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_staff_table(cons_tds, dev_staff, actuals_data, table_frame, actuals_keys):\n",
    "    #Fetching staff data for consolidation\n",
    "    cons_data = dev_staff.loc[dev_staff['CONS_TDS'] == cons_tds]\n",
    "    if cons_data.shape[0] == 0:\n",
    "        return(f'Consolidation {cons_tds} not found in staffing data.')\n",
    "    #print(cons_tds)\n",
    "    #print(dev_staff)\n",
    "    \n",
    "    # Isolate and process actuals data for consolidation\n",
    "    try:\n",
    "        cons_actuals = actuals_data[actuals_data['CONS_TDS'] == cons_tds]\n",
    "    except:\n",
    "        print(f'{cons_tds} not found in actuals.')\n",
    "        return np.NaN\n",
    "    \n",
    "    cons_actuals = cons_actuals[['CONS_NAME', 'CONS_TDS', 'Current Modified', 'ACT', \n",
    "                                 'CODE_KEY', 'CODE_NAME']].groupby(by='CODE_KEY', as_index=False).agg({'CONS_NAME': 'first',\n",
    "                                                                                                       'CONS_TDS': 'first',\n",
    "                                                                                                     'Current Modified':sum,\n",
    "                                                                                                     'ACT':sum,\n",
    "                                                                                                     'CODE_NAME':'first'})\n",
    "    cons_actuals\n",
    "    cons_actuals.loc['Total']= cons_actuals.sum(numeric_only=True, axis=0)\n",
    "    cons_actuals.loc['Total','CODE_KEY'] = 11\n",
    "    cons_actuals.loc['Total','CODE_NAME'] = 'TOT'\n",
    "    \n",
    "    for row in cons_actuals.itertuples():\n",
    "        cons_data[f'{row.CODE_NAME}_ACT'] = row.ACT\n",
    "    #print(cons_data)\n",
    "    #Setting up table and transposing data\n",
    "    cons_table_frame = table_frame\n",
    "    cons_table_frame['Formula'] = cons_table_frame['FORMULA_KEY'].iloc[:-1].apply(lambda key: cons_data[key].iloc[0])\n",
    "    cons_table_frame['Budgeted'] = cons_table_frame['BUDG_KEY'].apply(lambda key: cons_data[key].iloc[0])\n",
    "    cons_table_frame['Actual'] = cons_table_frame['ACTUALS_KEY'].iloc[:-2].apply(lambda key: cons_data[key].iloc[0] if key in cons_data.columns else 0)\n",
    "\n",
    "    \n",
    "    #Simplifying table\n",
    "    cons_table = cons_table_frame[['CHART_LINE', 'Formula', 'Budgeted', 'Actual']]\n",
    "    #print(cons_table)\n",
    "    \n",
    "    #Defining LaTeX table format\n",
    "    \n",
    "    def make_staff_table_block(staff_data):\n",
    "    \n",
    "        table_template = r'''\n",
    "        \\begin{tabular}{l|c|c|c|}\n",
    "        \\cline{2-4}\n",
    "                                                                                     & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Formula Allocation} & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Budgeted} & \\cellcolor{ccfuschia}{\\color[HTML]{FFFFFF} Actual} \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Employees}                      & %s                                                      & %s                                                                & %s                                                        \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Property Manager}               & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Asst. Property Manager}         & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Secretaries}                    & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Housing Assistants}             & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Superintendent}                 & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Assistant Superintendent}       & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Supervisor of Caretakers (SOC)} & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Supervisor of Grounds (SOG)}    & %s                                                      & %s                                                                & %s                                                      \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Maintenance Workers}            & %s                                                      & %s                                                                & %s                                                       \\\\ \\hline\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers X}                   & %s                                                      & %s                                                                &                                                       \\\\ \\cline{1-3}\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers J\\tnote{1}}                   &                                                       & %s                                                                &                                                         \\\\ \\cline{1-1} \\cline{3-3}\n",
    "        \\multicolumn{1}{|l|}{\\cellcolor{ccfuschialight}Caretakers G}                   & \\multirow{-2}{*}{%s}                                                      & %s                                     & \\multirow{-3}{*}{%s}                           \\\\ \\hline\n",
    "        \\end{tabular}\n",
    "        \n",
    "        '''\n",
    "\n",
    "        values = []\n",
    "\n",
    "        def extract_data_through_mw(row):\n",
    "            [values.append(item) for item in [str(int(row['Formula'])), \n",
    "                                              str(int(row['Budgeted'])), \n",
    "                                              str(int(row['Actual']))]]\n",
    "            pass\n",
    "\n",
    "        #Processing through Maintenance Worker\n",
    "        staff_data.iloc[0:-3].apply(lambda row: extract_data_through_mw(row), axis=1)\n",
    "\n",
    "        #Processing Caretakers\n",
    "        values.append(str(int(staff_data.iloc[-3, 1])))\n",
    "        values.append(str(int(staff_data.iloc[-3, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-2, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-2, 1])))\n",
    "        values.append(str(int(staff_data.iloc[-1, 2])))\n",
    "        values.append(str(int(staff_data.iloc[-3, 3])))\n",
    "\n",
    "        return table_template % tuple(values)\n",
    "    \n",
    "    #Make and export LaTeX code\n",
    "    with open(f'TABLES/staff_table/{cons_tds}_staff_table.tex', 'w') as file_handle:\n",
    "        file_handle.write(make_staff_table_block(cons_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyleslugg/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "staff_data = load_staff_data(consolidations)\n",
    "\n",
    "for tds in consolidations.keys():\n",
    "    make_staff_table(tds, *staff_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making and Compiling LaTeX Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_latex_file(tds, counts=counts):\n",
    "    #SET UTILITY PATHS HERE\n",
    "    pdflatex_path = '/usr/local/texlive/2018/bin/x86_64-darwin/pdflatex'\n",
    "    ghostscript_path = '/usr/local/bin/gs'\n",
    "    \n",
    "    if counts[tds]['count'] <= 4:\n",
    "        with open('REPORT_TEMPLATE/report.tex', 'r') as file_handle:\n",
    "            text = file_handle.read()\n",
    "            \n",
    "        new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "        with open(f'REPORTS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "            outfile.write(new_text)\n",
    "    \n",
    "    else:\n",
    "        with open('REPORT_TEMPLATE/report_long.tex', 'r') as file_handle:\n",
    "            text = file_handle.read()\n",
    "            \n",
    "        new_text = text.replace('$tds_number$', str(tds))\n",
    "\n",
    "        with open(f'REPORTS/LaTeX/{tds}_report.tex', 'w') as outfile:\n",
    "            outfile.write(new_text)\n",
    "\n",
    "    subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/LaTeX', f'REPORTS/LaTeX/{tds}_report.tex'])\n",
    "    subprocess.check_call([pdflatex_path, '-output-directory', 'REPORTS/LaTeX', f'REPORTS/LaTeX/{tds}_report.tex'])\n",
    "    \n",
    "    #Be sure to install ghostscript (to compress pdfs), or comment out next line. Available via homebrew.\n",
    "    subprocess.check_call([ghostscript_path, '-sDEVICE=pdfwrite', '-dCompatibilityLevel=1.5', '-dNOPAUSE', '-dQUIET', '-dBATCH', f'-sOutputFile=REPORTS/{tds}_report.pdf', f'REPORTS/LaTeX/{tds}_report.pdf'])\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('REPORTS'):\n",
    "    os.makedirs('REPORTS')\n",
    "\n",
    "if not os.path.exists('REPORTS/LaTeX'):\n",
    "    os.makedirs('REPORTS/LaTeX')\n",
    "\n",
    "os.system(\"cp REPORT_TEMPLATE/content.tex REPORTS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/preface.tex REPORTS/LaTeX\")\n",
    "os.system(\"cp REPORT_TEMPLATE/content_long.tex REPORTS/LaTeX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consolidation_list = consolidations.keys()\n",
    "consolidation_list = ['073', '003']\n",
    "for tds in consolidation_list:\n",
    "    try:\n",
    "        compile_latex_file(tds)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "filelist = [f for f in os.listdir('REPORTS/LaTeX') if not f.endswith(\".tex\")]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join('REPORTS/LaTeX', f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
